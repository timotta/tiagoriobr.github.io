<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
    <title>tiago.rio.br</title>
    
    <link rel="alternate" href="/" />
    <link rel="self" href="/feed.xml" type="application/atom+xml" />
    <id>/</id>
    <updated>2025-12-31T14:00:57Z</updated>

    <author>
        <name>Tiago Albineli Motta</name>
        
        <uri>/</uri>
    </author>

    
        <entry>
            <title>Queco: Bolo cookie sem glútem</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/queco-bolo-cookie-sem-gluten/" type="text/html" />
            <id>/pessoal/receitas/refeies/queco-bolo-cookie-sem-gluten/</id>
            <updated>2025-12-30T14:00:00Z</updated>

            <summary type="html">A idéia inicial dessa receita era fazer um cookie sem gluten e sem açúcar. Mas acabei botando de tabela um pouco de açucar por causa das gotas de chocolate ao leite que possuem açucar na sua composição. Eu fiz muitas variações desse &amp;ldquo;bolo&amp;rdquo; até que ele ficasse bom.</summary>
            <content type="html">&lt;p&gt;A idéia inicial dessa receita era fazer um cookie sem gluten e sem açúcar. Mas acabei botando de tabela um pouco de açucar por causa das gotas de chocolate ao leite que possuem açucar na sua composição. Eu fiz muitas variações desse &amp;ldquo;bolo&amp;rdquo; até que ele ficasse bom.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;150g de farinha de aveia&lt;/li&gt;
&lt;li&gt;100g de manteiga com sal&lt;/li&gt;
&lt;li&gt;1 colher de sopa de extrato de baunilha&lt;/li&gt;
&lt;li&gt;1 colher de chá de bicarbonato de sódio&lt;/li&gt;
&lt;li&gt;60g de eritritol&lt;/li&gt;
&lt;li&gt;100g de gotas de chocolate ao leite&lt;/li&gt;
&lt;li&gt;&amp;frac14; de chícara de leite&lt;/li&gt;
&lt;li&gt;1 ovo&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Derreta a manteiga, deixe esfriar e reserve&lt;/li&gt;
&lt;li&gt;Bata o ovo com a mão mesmo&lt;/li&gt;
&lt;li&gt;Misture o leite e bata novamente&lt;/li&gt;
&lt;li&gt;Misture o eritritol, o bicarbonato de sódio e o extrato de baunilha e bata novamente&lt;/li&gt;
&lt;li&gt;Misture a manteiga derretida e misture novamente até ficar homogeneo&lt;/li&gt;
&lt;li&gt;Misture a farinha de aveia até ficar homogeneo&lt;/li&gt;
&lt;li&gt;Adicione as gotas de chocolate e misture tentando espalhar bem&lt;/li&gt;
&lt;li&gt;Coloque a massa em uma forma de silicone e leve ao forno por uns 15 minutos&lt;/li&gt;
&lt;/ol&gt;
</content>
        </entry>
    
        <entry>
            <title>Creme de couve flor e espinafre que virou lasanha</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/creme-couve-flor-espinafre-lasanha/" type="text/html" />
            <id>/pessoal/receitas/refeies/creme-couve-flor-espinafre-lasanha/</id>
            <updated>2025-12-30T13:59:00Z</updated>

            <summary type="html">A receita original era só creme de couve-flor e espinafre, mas assim que fiz logo percebi que ficaria bem melhor esse creme em uma lasanha. Então é isso: Duas receitas em uma.</summary>
            <content type="html">&lt;p&gt;A receita original era só creme de couve-flor e espinafre, mas assim que fiz logo percebi que ficaria bem melhor esse creme em uma lasanha. Então é isso: Duas receitas em uma.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Uma couve flor&lt;/li&gt;
&lt;li&gt;1 maço de espinafre&lt;/li&gt;
&lt;li&gt;250ml de leite&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de queijo minas&lt;/li&gt;
&lt;li&gt;Queijo meia cura (bastante para o queijo da lasanha)&lt;/li&gt;
&lt;li&gt;Massa pronta de lasanha (quanto mais fininha melhor)&lt;/li&gt;
&lt;li&gt;Pimenta branca&lt;/li&gt;
&lt;li&gt;2 dentes de alho&lt;/li&gt;
&lt;li&gt;Sal a gosto&lt;/li&gt;
&lt;li&gt;Manteiga&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo do creme&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Cozinhe no vapor a couve flor toda desmembrada já até ficar fácil de espetar com o garfo&lt;/li&gt;
&lt;li&gt;Cozinhe as folhas do espinafre na água do cozimento da couve flor por 2 minutos e depois as jogue numa bacia com gelo para parar de cozinhar&lt;/li&gt;
&lt;li&gt;Bata a couve flor com o leite e os temperos no liquidificador&lt;/li&gt;
&lt;li&gt;Derreta um pouco de manteiga em uma panela&lt;/li&gt;
&lt;li&gt;Coloque o creme na panela&lt;/li&gt;
&lt;li&gt;Adicione o espinafre rusticamente cortado no creme e misture&lt;/li&gt;
&lt;li&gt;Cozinhe um pouquinho e reserve&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Preparo da lasanha&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Em uma travessa coloque camadas de massa, creme e queijo meia cura, nessa ordem. Mas a primeira camada deve ser bem pouquinho creme.&lt;/li&gt;
&lt;li&gt;Coloque ao forno, e a ultima camada deve ser um pouquinho de creme e queijo pra gratinar.&lt;/li&gt;
&lt;li&gt;Fique observando até notar que o queijo derreteu e no topo gratinou.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DSGxUEzjj3Z/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Tofu crispy petisco</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/tofu-crispy-petisco/" type="text/html" />
            <id>/pessoal/receitas/refeies/tofu-crispy-petisco/</id>
            <updated>2025-12-30T13:36:00Z</updated>

            
            <content type="html">&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;400g de tofu firme&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de extrato de tomate&lt;/li&gt;
&lt;li&gt;4 colheres de sopa de shoyo&lt;/li&gt;
&lt;li&gt;2 colheres de chá de açucar mascavo&lt;/li&gt;
&lt;li&gt;Suco de 1 limão&lt;/li&gt;
&lt;li&gt;2 dentes de alho ralado&lt;/li&gt;
&lt;li&gt;1cm de gengibre fresco ralado&lt;/li&gt;
&lt;li&gt;Pimenta à gosto&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Seque bem o tofu e quebre em pedacinhos pequenos irregulares&lt;/li&gt;
&lt;li&gt;Em outra vasilha misture todos os igredientes até ficar homogeneo&lt;/li&gt;
&lt;li&gt;Derramete esses igredientes misturados nos pedacinhos de tofu e espalhe bem&lt;/li&gt;
&lt;li&gt;Coloque os pedacinhos de tofu em uma trafessa bem espalhados (de preferência com um tapete de silicone embaixo para não grudar)&lt;/li&gt;
&lt;li&gt;Spray de azeite em cima&lt;/li&gt;
&lt;li&gt;Coloque no forno por 12 minutos&lt;/li&gt;
&lt;/ol&gt;
</content>
        </entry>
    
        <entry>
            <title>Unexpected Side Effects of Let’s Add More jobs</title>
            <link rel="alternate" href="/work/willbank/account/unexpected-side-effects-lets-add-more-jobs/" type="text/html" />
            <id>/work/willbank/account/unexpected-side-effects-lets-add-more-jobs/</id>
            <updated>2025-11-25T17:32:00Z</updated>

            <summary type="html">In another iteration to improve our &lt;a href=&#34;../approval-model-finally-approved/&#34;&gt;no-credit account approval model&lt;/a&gt;, we built a new version with a new target. During local testing, it showed a response time significantly higher than the previous model, almost double. Honestly, this wasn’t surprising: the feature count had grown, the model was more complex, and we all know complexity never comes for free.</summary>
            <content type="html">&lt;p&gt;In another iteration to improve our &lt;a href=&#34;../approval-model-finally-approved/&#34;&gt;no-credit account approval model&lt;/a&gt;, we built a new version with a new target. During local testing, it showed a response time significantly higher than the previous model, almost double. Honestly, this wasn’t surprising: the feature count had grown, the model was more complex, and we all know complexity never comes for free.&lt;/p&gt;

&lt;p&gt;But once we deployed it to the development environment, the service API simply refused to start. Looking at our memory metrics, it was obvious that consumption was through the roof. Even after doubling the memory requested in Kubernetes, the service still wouldn’t come up, apparently having developed an expensive taste and asking for even more memory.&lt;/p&gt;

&lt;p&gt;Digging into the issue locally, I noticed that when the service started and loaded the model pickle, &lt;strong&gt;14 forked processes&lt;/strong&gt; magically appeared, as if I had accidentally summoned them.&lt;/p&gt;

&lt;pre&gt;
ps aux | grep myproject
.../python3.11 -m joblib.externals.loky.backend.popen_loky_posix --process-name LokyProcess-1:2 --pipe 29
...
&lt;/pre&gt;

&lt;p&gt;The number &lt;strong&gt;14&lt;/strong&gt; instantly rang a bell: it was the exact value I had used for &lt;code&gt;n_jobs&lt;/code&gt; when training the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/14jobs.png&#34; alt=&#34;/assets/images/14jobs.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;I had never had issues using &lt;code&gt;n_jobs&lt;/code&gt; when training XGBoost. XGBoost normally keeps that parallelism contained to the fit step. But in this new model, I had also set &lt;code&gt;n_jobs&lt;/code&gt; on the &lt;code&gt;ColumnTransformer&lt;/code&gt; inside the Pipeline. Looking back, a bold decision. Not necessarily a wise one.&lt;/p&gt;

&lt;p&gt;To give you an idea, the pipeline looked something like this:&lt;/p&gt;

&lt;pre&gt;
preprocessor = ColumnTransformer([
    (&#39;num&#39;, num_pipeline, NUM_FEATURES),
    (&#39;cat&#39;, cat_pipeline, CAT_FEATURES),
], n_jobs=14)

pipeline = Pipeline([
    (&#39;preprocessor&#39;, preprocessor),
    (&#39;xgboost&#39;, XGBClassifier(..., n_jobs=14))
])
&lt;/pre&gt;

&lt;p&gt;And that’s where the mischief was hiding. &lt;code&gt;n_jobs&lt;/code&gt;, wonderfully useful to speed up training, turned into a tiny chaos agent when loading the model for &lt;code&gt;predict&lt;/code&gt;, spawning more processes than my dev environment cared to handle.&lt;/p&gt;

&lt;p&gt;To fix the problem, I wrote a small script to &amp;ldquo;edit&amp;rdquo; the pickle (yes, it always feels slightly illegal) and took the opportunity to migrate to &lt;code&gt;cloudpickle&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;
with open(f&#34;{base_path}/problematic.pkl&#34;, &#34;rb&#34;) as f1:
    model = pickle.load(f1)

dict(model.steps)[&#34;preprocessor&#34;].n_jobs = None

with open(f&#34;{base_path}/fixed.pkl&#34;, &#34;wb&#34;) as f2:
    f2.write(cloudpickle.dumps(model))
&lt;/pre&gt;

&lt;p&gt;That was enough to bring the response time back to the level of the previous model, even with the added complexity. And the deployment to the dev environment finally behaved like a civilized service.&lt;/p&gt;

&lt;p&gt;Of course, editing a pickle always carries the risk of altering the model&amp;rsquo;s behavior, but in this case it was safe. Running the model on a sample of 10,000 examples returned the exact same scores.&lt;/p&gt;

&lt;p&gt;I can’t help but think how difficult it would have been to diagnose the memory issues if I hadn’t recalled that &lt;em&gt;I had explicitly set&lt;/em&gt; &lt;code&gt;n_jobs=14&lt;/code&gt;. It really felt like past me had constructed the perfect trap for future me, and future me walked right into it.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>An Approval Model That Finally Got Approved</title>
            <link rel="alternate" href="/work/willbank/account/approval-model-finally-approved/" type="text/html" />
            <id>/work/willbank/account/approval-model-finally-approved/</id>
            <updated>2025-11-08T14:18:00Z</updated>

            <summary type="html">Until last year, Will Bank was mostly a digital bank with one obsession: credit cards. Every campaign, every banner, every catchy line revolved around them. But then came the big question: what if we also cared about people who don’t have credit? That’s when we started exploring how to grow our customer base among users without access to credit, focusing on digital account usage only. There was just one tiny problem: the experience for this group wasn’t great. We had to be careful not to approve users dreaming of credit and hand them&amp;hellip; a digital account. Nobody likes that kind of plot twist.</summary>
            <content type="html">&lt;p&gt;Until last year, Will Bank was mostly a digital bank with one obsession: credit cards. Every campaign, every banner, every catchy line revolved around them. But then came the big question: what if we also cared about people who don’t have credit? That’s when we started exploring how to grow our customer base among users without access to credit, focusing on digital account usage only. There was just one tiny problem: the experience for this group wasn’t great. We had to be careful not to approve users dreaming of credit and hand them&amp;hellip; a digital account. Nobody likes that kind of plot twist.&lt;/p&gt;

&lt;p&gt;When our team was created to tackle this, a few experiments were already floating around. One of them aimed to find, among credit-denied customers, those who still had potential to become great digital account users. A noble quest, but easier said than done.&lt;/p&gt;

&lt;p&gt;The first challenge was defining what a &amp;ldquo;good customer&amp;rdquo; even meant. The data science team and business folks were in deep philosophical discussions over what &amp;ldquo;engagement&amp;rdquo; actually looked like. The metrics were solid, repeated transactions, specific frequencies, consistent usages, but not exactly actionable. One example definition was: &amp;ldquo;The customer performed a cash-out at least once per month for three months.&amp;rdquo; That sounds reasonable, right? Except it came with two major problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We needed three months of data before even training.&lt;/li&gt;
&lt;li&gt;Then another three months to check results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make things worse, we barely had any non-credit customers in the data. We planned to open the funnel randomly for two months to collect more samples. But with a three-month target, that meant at least five months before we could train a single model.
We wanted to do data science, not archaeology.&lt;/p&gt;

&lt;p&gt;So our first big move was to redefine success. We needed something we could actually measure this century. After some digging, we found a beautiful correlation: customers who made any transaction in the first week were much more likely to stay engaged in the long term. Perfect. Fast feedback, clear signal, no time travel required.&lt;/p&gt;

&lt;p&gt;With the new target set, and after a month of random sampling, I jumped into the team’s notebooks. They were running SageMaker, which came with ancient versions of pandas, scikit-learn, and XGBoost, and, tragically, no Optuna, my favorite hyperparameter-tuning companion. Given how tiny our dataset was, using SageMaker felt like renting a stadium to play table tennis. I moved everything locally, upgraded the stack, and life instantly got easier. We have accidentally created a Notebook template that we started using in many other following models for recommendation, churn detection and fraud prevention among others.&lt;/p&gt;

&lt;p&gt;Before the data science discipline was internalized, communication had been another headache. The data science presentations were full of bins, score ranges, and KS values. Impressive stuff, unless you weren’t a data scientist. To business stakeholders, it looked like hieroglyphics. They thought the model was a failure when it really wasn’t; it was just badly translated from &amp;ldquo;data&amp;rdquo; to &amp;ldquo;business.&amp;rdquo; So we changed the narrative. Instead of showing Kolmogorov-Smirnov, we started showing activation rates, complaint ratios, and the famous &amp;ldquo;what happens if we approve these people&amp;rdquo; chart. Suddenly, everyone was speaking the same language.&lt;/p&gt;

&lt;p&gt;I added a threshold analysis to the jupyter template to find the optimal score cutoff for activation and built guardrail metrics to show the trade-offs. The model was simple, put together in just two days, but the results were great. I made a deck with easy visuals, and for the first time, the business team didn’t just nod politely; they were genuinely excited. The project had direction again. Sometimes, the real model improvement is just better storytelling.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/approval.png&#34; alt=&#34;/assets/images/approval.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The model still needed tuning, but since data science was already part of the team, &lt;a href=&#34;https://www.linkedin.com/in/gustavo-alexis-sabill%C3%B3n-lee-3584b474/&#34;&gt;Gustavo Lee&lt;/a&gt;, who had been working on the model long before I joined, and &lt;a href=&#34;https://www.linkedin.com/in/jadson-marcelino-ele-he-86678329/&#34;&gt;Jadson Marcelino&lt;/a&gt;, whom we had recently hired and brought many ideas for new features and target setups, continued improving it.&lt;/p&gt;

&lt;p&gt;Meanwhile, I was trying to figure out how to actually get it live, since I was still the new kid at the company.&lt;/p&gt;

&lt;p&gt;Lucky for us, in parallel we were redesigning the onboarding flow to ensure customers approved without credit wouldn’t get confused or disappointed. That also helped us understand which features were actually available to the model at runtime, which is always a fun game of “what can we really use without months of engineering work?” Because the team was multidisciplinary, we could quickly adjust which features to include in training as we discovered new constraints. As we like to say, “A nearly perfect model in production is better than a perfect one stuck in a Jupyter notebook.” (We all know a few of those.)&lt;/p&gt;

&lt;p&gt;Deploying the model turned out to be refreshingly simple. Will Bank already had an internal platform to spin up new apps in the Kubernetes cluster. We just built a small FastAPI service with the model pickle, deployed it, and plugged into our observability and event tracking system. We could now watch the model’s scores and feature drift in real time, basically, data science with popcorn.&lt;/p&gt;

&lt;p&gt;Then came the fun part: results. Once live, the model delivered a 27% increase in activation in an A/B test, and the improvement held steady for months after rollout. We also kept a random control group untouched by the model, so we always had a healthy benchmark and new data for retraining. The curve went up, the charts looked great, and for once, nobody asked, &amp;ldquo;but is it statistically significant?&amp;rdquo; (It was.)&lt;/p&gt;

&lt;p&gt;That choice paid off in the long run. Over the following months, we went through five model iterations, refining targets, adding new features, and slowly reintroducing attributes we had initially left out for the development speed. &lt;a href=&#34;/work/willbank/account/patching-scikit-learn-improve-api-performance/&#34;&gt;I have writen a post about one of those iteraction here&lt;/a&gt;. Each version got smarter, and the team got more and more knowledge about clients without credit.&lt;/p&gt;

&lt;p&gt;Meanwhile, our team kept improving the non-credit customer experience to the point that we no longer needed to block users with low engagement potential. The model evolved too. It became less about who to approve and more about reducing friction in the onboarding flow, deciding whether to even ask customers if they still wanted to open an account after being denied credit. That small change had a big impact: more customers transacting, more staying active, and far fewer confused faces.&lt;/p&gt;

&lt;p&gt;It was a great reminder that, in data science, the smartest thing you can do isn’t always building a new algorithm: It’s helping people make slightly better decisions at the right moment. After all, the Agile Manifesto wasn’t written for data scientists, but maybe it should’ve been: working insights over perfect models, collaboration over documentation, and delivering value over chasing accuracy.&lt;/p&gt;

&lt;p&gt;Although I describe &lt;a href=&#34;https://www.linkedin.com/in/timotta/&#34;&gt;myself&lt;/a&gt; as the protagonist in this story, it has many other layers and perspectives, and none of it would have happened without the collective work of this amazing group of professionals: &lt;a href=&#34;https://www.linkedin.com/in/gustavo-alexis-sabill%C3%B3n-lee-3584b474/&#34;&gt;Gustavo Lee&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jadson-marcelino-ele-he-86678329/&#34;&gt;Jadson Marcelino&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/osvaldo-berg-jr/&#34;&gt;Osvaldo Berg&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/flaviaveloso/&#34;&gt;Flávia Veloso&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/pamelaps/&#34;&gt;Pamela Souza&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/rodrigomendesnunes/&#34;&gt;Rodrigo Nunes&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/camilamsilva/&#34;&gt;Camila de Melo&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/lurishirosaki/&#34;&gt;Luri Shirosaki&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ahelensouza/&#34;&gt;Helen Souza&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/anacrislemos/&#34;&gt;Ana Lemos&lt;/a&gt; e &lt;a href=&#34;https://www.linkedin.com/in/rodrigo-orofino/&#34;&gt;Rodrigo Orofino&lt;/a&gt;.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Tofu ou frango teriaki na airfryer</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/tofu-frango-teriaki-airfryer/" type="text/html" />
            <id>/pessoal/receitas/refeies/tofu-frango-teriaki-airfryer/</id>
            <updated>2025-10-19T13:15:00Z</updated>

            <summary type="html">A receita original aqui descrita é vegana e com tofu. Aqui vou descrever como fazer tanto com tofu como com frango. Outra diferença é que a receita do molho eu dobrei, dessa forma fica mais simples de mergulhar o empanado inteiro nele ao final.</summary>
            <content type="html">&lt;p&gt;A receita original aqui descrita é vegana e com tofu. Aqui vou descrever como fazer tanto com tofu como com frango. Outra diferença é que a receita do molho eu dobrei, dessa forma fica mais simples de mergulhar o empanado inteiro nele ao final.&lt;/p&gt;

&lt;h2&gt;Igredientes para o empanado&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;1 tofu firme cortado em 5 fatias, ou 5 fatias de peito de frango sasami&lt;/li&gt;
&lt;li&gt;1 xícara de farinha de trigo&lt;/li&gt;
&lt;li&gt;1 xícara de leite (Se quiser vegano use qualquer leite vegetal)&lt;/li&gt;
&lt;li&gt;Suco de 1 limão&lt;/li&gt;
&lt;li&gt;Alho em pó&lt;/li&gt;
&lt;li&gt;Paprica defumada&lt;/li&gt;
&lt;li&gt;Pimenta branca&lt;/li&gt;
&lt;li&gt;Shoyo&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Igredientes para o molho&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;8 colheres de sopa de açucar mascavo&lt;/li&gt;
&lt;li&gt;1 xícara de água&lt;/li&gt;
&lt;li&gt;6 colheres de sobremesa de alho picado&lt;/li&gt;
&lt;li&gt;Um pedaço pequeno de gengibre fatiado&lt;/li&gt;
&lt;li&gt;Cebolinha picada&lt;/li&gt;
&lt;li&gt;6 colheres de sopa de shoyo&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de vinagre&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Igredientes decoração&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Gergelim&lt;/li&gt;
&lt;li&gt;Cebolinha picada&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo do empanado&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Passe shoyo em volta das fatias de tofu ou de frango e reserve&lt;/li&gt;
&lt;li&gt;Misture a farinha com alho em pó, paprica defumada, pimenta e sal&lt;/li&gt;
&lt;li&gt;Em outro prato misture o leite, o suco de limão e uma colher de sopa da mistura de farinha&lt;/li&gt;
&lt;li&gt;Passe as fatias uma a uma no leite, depois na farinha&lt;/li&gt;
&lt;li&gt;Coloque as fatias na airfryer por dando uma sprayzada de azeite em cima&lt;/li&gt;
&lt;li&gt;Deixe fritando na airfryer por 5 minutos, e vai virando de lado, de 5 em 5 minutos até completar 20 minutos&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Preparo do molho&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Cozinhe o açucar com a água, o alho, o gengibre e a cebolinha até reduzir&lt;/li&gt;
&lt;li&gt;Coe, tirando os elementos sólidos&lt;/li&gt;
&lt;li&gt;Misture o shoyo e o vinagre e cozinhe mais até reduzir&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Preparo do final&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Mergulhe o empanado no molho reduzido até tudo ficar amarronzado&lt;/li&gt;
&lt;li&gt;Decore com gergelim e cebolinha&lt;/li&gt;
&lt;/ol&gt;
</content>
        </entry>
    
        <entry>
            <title>Palha italiana de pistache e nozes</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/palha-italiana-simples-pistache-nozes/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/palha-italiana-simples-pistache-nozes/</id>
            <updated>2025-10-18T01:35:00Z</updated>

            
            <content type="html">&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;2 latas/caixas de leite condensado (total de 790g)&lt;/li&gt;
&lt;li&gt;30g de manteiga com sal&lt;/li&gt;
&lt;li&gt;2 colheres de sobremesa de pasta de pistache&lt;/li&gt;
&lt;li&gt;100g de nozes picada&lt;/li&gt;
&lt;li&gt;1 pacote de biscoito maizena (175g)&lt;/li&gt;
&lt;li&gt;Leite em pó para decorar&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Em uma panela coloque o leite condensado a manteiga&lt;/li&gt;
&lt;li&gt;Cozinhe em fogo baixo mexendo sem parar pra não deixar ferver&lt;/li&gt;
&lt;li&gt;Quando estiver em ponto de brigadeiro desligue o fogo&lt;/li&gt;
&lt;li&gt;Misture toda a nozes&lt;/li&gt;
&lt;li&gt;Misture a pasta de pistache até ficar homogeneamente verde&lt;/li&gt;
&lt;li&gt;Misture todo o bicoito maizena quebrado anteriormente cada um em 3 partes&lt;/li&gt;
&lt;li&gt;Misture bem pra ficar o mais uniforme possível a distribuição dos biscoitos&lt;/li&gt;
&lt;li&gt;Derrame a mistura em uma forma de silicone ou em uma forrada com plástico filme&lt;/li&gt;
&lt;li&gt;Tampe com plástico filme e coloque na geladeira até firmar, eu botei até um pouco no freezer para acelerar o processo&lt;/li&gt;
&lt;li&gt;Quando estiver firme, tire da forma e polvilhe com leite em pó em todas os lados&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Essa é uma variaçã da receita de &lt;a href=&#34;../palha-italiana-simples&#34;&gt;palha italiana simples&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Kafta vegana: Explosão de sabor</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/kafta-vegana-explosao-sabor/" type="text/html" />
            <id>/pessoal/receitas/refeies/kafta-vegana-explosao-sabor/</id>
            <updated>2025-09-22T01:30:00Z</updated>

            
            <content type="html">&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;1 xícara de proteína de soja desidratada&lt;/li&gt;
&lt;li&gt;1 cebola média&lt;/li&gt;
&lt;li&gt;4 dentes de alho&lt;/li&gt;
&lt;li&gt;Azeite&lt;/li&gt;
&lt;li&gt;3 colheres de sopa de shoyu&lt;/li&gt;
&lt;li&gt;Suco de 1 e &amp;frac12; limão&lt;/li&gt;
&lt;li&gt;1 punhado de cheiro verde&lt;/li&gt;
&lt;li&gt;Páprica defumada&lt;/li&gt;
&lt;li&gt;1 colher de sopa de molho barbecue&lt;/li&gt;
&lt;li&gt;Mais ou menos 100g de Farinha de linhaça&lt;/li&gt;
&lt;li&gt;Sal e pimenta a gosto&lt;/li&gt;
&lt;li&gt;Meia colher de chá de bicarbonato de sódio&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Refogue a cebola e o alho no azeite com bicarbonato de sódio até dourar e reserve&lt;/li&gt;
&lt;li&gt;Após ferver uma panela de água, coloque a proteína de soja mais suco de &amp;frac12; limão e deixe cozinhando por 10 minutos&lt;/li&gt;
&lt;li&gt;Escorra a carne de soja bem, lave e exprema bem ela.&lt;/li&gt;
&lt;li&gt;Misture todos os igredientes: A soja, o refogado, o cheiro verde picado, a páprica, o shoyo, o sal e a pimenta. Experimente para dosar o sal, a páprica e a pimenta.&lt;/li&gt;
&lt;li&gt;Vá adicionando a farinha de linhaça aos poucos até que seja possível fazer bolinhos.&lt;/li&gt;
&lt;li&gt;Faça bolinhos e coloque na airfryer jogando azeide com spray ou pincelando por cima&lt;/li&gt;
&lt;li&gt;7 minutos na airfryer de um lado, gire os bolinhos e coloque mais 7 minutos.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DNlObROtAKH/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Chipa Guazú: Torta salgada</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/chipa-guazu-torta-salgado/" type="text/html" />
            <id>/pessoal/receitas/refeies/chipa-guazu-torta-salgado/</id>
            <updated>2025-09-21T13:59:00Z</updated>

            
            <content type="html">&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;1 lata de milho&lt;/li&gt;
&lt;li&gt;&amp;frac12; lata de leite&lt;/li&gt;
&lt;li&gt;2 ovos&lt;/li&gt;
&lt;li&gt;1 cebola picada e refogada no azeite&lt;/li&gt;
&lt;li&gt;Sal e pimenta a gosto&lt;/li&gt;
&lt;li&gt;1 colher de chá de fermento em pó&lt;/li&gt;
&lt;li&gt;100g de queijo meia cura&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Bata todos os igredientes menos o queijo no liquidificador&lt;/li&gt;
&lt;li&gt;Rale o queijo e misture a maior parte dele&lt;/li&gt;
&lt;li&gt;Coloque a massa em uma travessa e jogue o restante do queijo ralado em cima&lt;/li&gt;
&lt;li&gt;Leve ao forno pré-aquecido a 200 graus durante 30 a 40 minutos até dourar&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DJkS5QYqkm3/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Palha italiana simples</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/palha-italiana-simples/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/palha-italiana-simples/</id>
            <updated>2025-09-20T13:26:00Z</updated>

            <summary type="html">Por várias vezes eu fiz essa receita com chocolate em pó 100% alcalino. Tudo que eu estava fazendo com esse chocolate estava ficando com um gosto de especiarias, canela, noz moscada. Troquei para chocolate sem ser alcalino, e o sabor de especiaria desapareceu, mas ficou muito mais chocolatudo, então reduzi o total de gramas de chocolate em pó da receita (ainda não testei essa mudança).</summary>
            <content type="html">&lt;p&gt;Por várias vezes eu fiz essa receita com chocolate em pó 100% alcalino. Tudo que eu estava fazendo com esse chocolate estava ficando com um gosto de especiarias, canela, noz moscada. Troquei para chocolate sem ser alcalino, e o sabor de especiaria desapareceu, mas ficou muito mais chocolatudo, então reduzi o total de gramas de chocolate em pó da receita (ainda não testei essa mudança).&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;2 latas/caixas de leite condensado (total de 790g)&lt;/li&gt;
&lt;li&gt;30g de manteiga com sal&lt;/li&gt;
&lt;li&gt;40g de chocolate em pó 100% cacau alcalino (ou 30g de chocolate em pó 100%)&lt;/li&gt;
&lt;li&gt;1 pacote de biscoito maizena (175g)&lt;/li&gt;
&lt;li&gt;Leite em pó para decorar&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Em uma panela coloque o leite condensado a manteiga e o chocolate em pó&lt;/li&gt;
&lt;li&gt;Cozinhe em fogo baixo mexendo sem parar pra não deixar ferver&lt;/li&gt;
&lt;li&gt;Quando estiver em ponto de brigadeiro desligue o fogo&lt;/li&gt;
&lt;li&gt;Misture todo o bicoito maizena quebrado anteriormente cada um em 3 partes&lt;/li&gt;
&lt;li&gt;Misture bem pra ficar o mais uniforme possível a distribuição dos biscoitos&lt;/li&gt;
&lt;li&gt;Derrame a mistura em uma forma de silicone ou em uma forrada com plástico filme&lt;/li&gt;
&lt;li&gt;Tampe com plástico filme e coloque na geladeira até firmar, eu botei até um pouco no freezer para acelerar o processo&lt;/li&gt;
&lt;li&gt;Quando estiver firme, tire da forma e polvilhe com leite em pó em todas os lados&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DKcf8GZKx6X/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>How I got the unfair reputation of disliking semantics</title>
            <link rel="alternate" href="/work/globocom/farofus/how-got-unfair-reputation-disliking-semantics/" type="text/html" />
            <id>/work/globocom/farofus/how-got-unfair-reputation-disliking-semantics/</id>
            <updated>2025-09-18T23:37:00Z</updated>

            <summary type="html">When people today talk about semantic search, they usually mean vector embeddings, RAG pipelines, and all that fancy LLM jazz. But back in 2011, at Globo.com, we were into a very different kind of “semantics”: the Semantic Web.</summary>
            <content type="html">&lt;p&gt;When people today talk about semantic search, they usually mean vector embeddings, RAG pipelines, and all that fancy LLM jazz. But back in 2011, at Globo.com, we were into a very different kind of “semantics”: the Semantic Web.&lt;/p&gt;

&lt;p&gt;And believe it or not, despite my supposed reputation as a semantics hater, my first encounter with SPARQL was love at first sight.&lt;/p&gt;

&lt;h2&gt;Falling in Love with SPARQL&lt;/h2&gt;

&lt;p&gt;Before I even joined &lt;a href=&#34;https://www.techtudo.com.br/&#34;&gt;TechTudo&lt;/a&gt;, I had already been introduced to &lt;a href=&#34;https://jena.apache.org/tutorials/sparql_pt.html&#34;&gt;SPARQL&lt;/a&gt; at Globo.com. At the time, many of Globo’s products were experimenting with organizing their entities in ontologies stored in &lt;a href=&#34;https://vos.openlinksw.com/owiki/wiki/VOS&#34;&gt;Virtuoso&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the uninitiated, SPARQL is like SQL’s eccentric cousin who reads philosophy books and talks about relationships. Data isn’t rows and columns. It’s triples: subject, predicate, object. And you can chain these triples into hierarchies that go up, down, sideways. It’s like the family tree of knowledge.&lt;/p&gt;

&lt;p&gt;When I learned it, I thought: “Wow, this is beautiful. This is the future. I’m going to tell my grandchildren about this.”&lt;/p&gt;

&lt;p&gt;But as in many relationships, the problems weren’t with SPARQL itself&amp;hellip;&lt;/p&gt;

&lt;h2&gt;The Big Semantic Web Dream&lt;/h2&gt;

&lt;p&gt;The whole Virtuoso/SPARQL setup wasn’t there because TechTudo really needed it. No. It was part of Globo.com’s grand plan: connect all products, G1, GE, GShow, GloboVideos, into one giant semantic brain. Content would flow magically between them, like neurons firing in the collective consciousness of Globo.&lt;/p&gt;

&lt;p&gt;In practice?&lt;/p&gt;

&lt;p&gt;Well… politics happened. Business didn’t really want to share traffic. Everyone wanted to receive visitors, but no one wanted to give it away. So, in the end, the “semantic web of everything” was centralized, but the promised cross-domain benefits, sharing traffic between products, were never really used.&lt;/p&gt;

&lt;p&gt;Meanwhile, we, humble TechTudo folks, were left dealing with the side effects of this dream.&lt;/p&gt;

&lt;h2&gt;The Return of the Big Oracle&lt;/h2&gt;

&lt;p&gt;Here’s the irony: just before Virtuoso, Globo.com had invested a ton of effort breaking down its big, scary Oracle monolith into smaller databases. This gave teams autonomy, agility, and sanity.&lt;/p&gt;

&lt;p&gt;Then came Virtuoso. To be fair, the folks working on it, from the ontology design to the deployments, were incredibly talented and did an amazing job within the constraints. But no matter how good the people were, the centralized strategy itself was the problem: schema changes required massive coordination, overnight deployments, and left us with less autonomy.&lt;/p&gt;

&lt;p&gt;I still have an email from that era coined with the phrase “the return of the big Oracle”. That was exactly what it felt like. Déjà vu, but with more RDF.&lt;/p&gt;

&lt;p&gt;And since TechTudo was considered “the little sibling” compared to G1, GE, or GShow, guess whose schema requests always ended up at the bottom of the priority list? Yep. Ours.&lt;/p&gt;

&lt;h2&gt;Endless Debates About Printers and Android&lt;/h2&gt;

&lt;p&gt;Now, you might ask: “But did you really need to change the schema that often?”
Oh yes.&lt;/p&gt;

&lt;p&gt;At TechTudo we worked side by side with editors, and they constantly saw opportunities to improve navigation and SEO by tweaking hierarchies.&lt;/p&gt;

&lt;p&gt;Examples:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Printers and Scanners were categories. But what about multifunction printers? Should they have their own specific category, or should entities like these belong to more than one category at the same time? Existential crisis in hardware form.&lt;/li&gt;
&lt;li&gt;Android and iOS were operating systems. But many readers also thought Android was a type of phone. Technically wrong, but SEO doesn’t care about your feelings.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We had dozens of situations like this.&lt;/p&gt;

&lt;p&gt;The problem? Every change required weeks of discussion because opportunities to actually implement changes were rare. It was like playing chess, but if you touched a piece, you had to wait two weeks to check the result.&lt;/p&gt;

&lt;h2&gt;The MySQL Rebellion&lt;/h2&gt;

&lt;p&gt;At some point, we thought: “Enough philosophy. Let’s get practical.”&lt;/p&gt;

&lt;p&gt;So we prototyped a “diet version” of semantics using MySQL:&lt;/p&gt;

&lt;p&gt;Hierarchies modeled with n-to-n tables.
 Entities and content synced asynchronously into other tables with all ancestors (basically, pre-computed tags).&lt;/p&gt;

&lt;p&gt;Did we lose SPARQL’s expressive power? Absolutely. Did we care? Nope.
Because in practice, all we really needed was:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Give me all content for this entity&lt;/li&gt;
&lt;li&gt;Give me the related entities of this entity&lt;/li&gt;
&lt;li&gt;Give some contents from the related entities&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With MySQL, we could do this quickly, and without begging another team for permission.&lt;/p&gt;

&lt;p&gt;And the best part: editors could experiment. Try a hierarchy change, see the results, undo if necessary. No more weeks of debates about printers’ identities. Just: test, learn, move on.&lt;/p&gt;

&lt;p&gt;The vibe changed instantly. People got bolder, faster, happier. Autonomy is a hell of a drug.&lt;/p&gt;

&lt;h2&gt;The Evolution&lt;/h2&gt;

&lt;p&gt;Eventually, the Semantic Web team evolved their platform and moved part of it towards ElasticSearch. With multi-value keyword fields, they could implemented better some of what we hacked in MySQL.&lt;/p&gt;

&lt;p&gt;Editing the schema was still a no-go, but for Globo’s major products, that wasn’t really a big issue. They didn’t need constant tweaks to their hierarchies the way TechTudo did. And hey, at least queries got faster. Baby steps.&lt;/p&gt;

&lt;h2&gt;Rumors of My “Hatred” for Semantics&lt;/h2&gt;

&lt;p&gt;Of course, the rumor mill started. “He doesn’t like semantics,” they said.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/semantic.png&#34; alt=&#34;/assets/images/semantic.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Totally unfair!&lt;/p&gt;

&lt;p&gt;I loved SPARQL so much I even built a &lt;a href=&#34;https://github.com/timotta/active-semantic&#34;&gt;Ruby prototype (ActiveRecord-style)&lt;/a&gt; for it, and later a full proprietary Python library. I was a fanboy!&lt;/p&gt;

&lt;p&gt;What I didn’t like was the centralized bottleneck, it was an autonomy problem.&lt;/p&gt;

&lt;p&gt;SPARQL will always have a special place in my heart. But in TechTudo, the real win wasn’t about elegant queries. It was about giving editors and the team the courage to try, fail, and learn fast.&lt;/p&gt;

&lt;p&gt;And yes… we did finally figure out what to do with printers.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Pasta verde refrescante e proteíca</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/pasta-verde-refrescante-proteica/" type="text/html" />
            <id>/pessoal/receitas/refeies/pasta-verde-refrescante-proteica/</id>
            <updated>2025-09-06T23:46:00Z</updated>

            <summary type="html">Essa receita ficou maravilhosa para comer com doritos. Também ficou muito boa com torradinhas, mas com doritos ficou incrivel. Eu pretendo repetir no futuro reduzindo a quantidade de hortelã.</summary>
            <content type="html">&lt;p&gt;Essa receita ficou maravilhosa para comer com doritos. Também ficou muito boa com torradinhas, mas com doritos ficou incrivel. Eu pretendo repetir no futuro reduzindo a quantidade de hortelã.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;250g de ervilha congelada&lt;/li&gt;
&lt;li&gt;250g de semente de edamame congelada&lt;/li&gt;
&lt;li&gt;1 ramo de cebolinha&lt;/li&gt;
&lt;li&gt;10g de hortelã fresca&lt;/li&gt;
&lt;li&gt;15g de coentro fresco&lt;/li&gt;
&lt;li&gt;1 dente de alho picado&lt;/li&gt;
&lt;li&gt;Suco de 1 limão&lt;/li&gt;
&lt;li&gt;1 colher de sopa de pasta de misô&lt;/li&gt;
&lt;li&gt;2 colher de sopa de shoyu&lt;/li&gt;
&lt;li&gt;2 colher de sopa de mel&lt;/li&gt;
&lt;li&gt;Pimenta branca a gosto&lt;/li&gt;
&lt;li&gt;Semente de gergelim de cor mista para decorar&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Pincele a cebolinha inteira com azeite e sal e leve ao forno por uns 5 a 10 minutos até escurecer.&lt;/li&gt;
&lt;li&gt;Ferva água, e derrame sobre a ervilha e o edamame. Espere esfriar, e repita o processo (na receita original ele faz isso apenas uma vez).&lt;/li&gt;
&lt;li&gt;Assim que esfriar novamente coloque a ervilha e o edamame em uma trafessa com água e gelo para esfriar bem e manter a cor.&lt;/li&gt;
&lt;li&gt;Reserve uma parte pequena da ervilha e do edamame para decorar ao final.&lt;/li&gt;
&lt;li&gt;Bata no processador todos os igredientes: Ervilha, edamame, a cebolinha assada, as folhas de hortelã e coentro, o alho, o suco de limão, a pasta de misô, shoyu, mel e pimenta.&lt;/li&gt;
&lt;li&gt;Quando estiver bem homogêneo vá provando se está a seu gosto e vá ajustando com shoyo, mel e pimenta. Cuidado com o shoyo para não escurecer a pasta. Se estiver escurecendo e sentir que ainda está sem sal, use sal mesmo para manter o verdão lindo da pasta.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Como servir&lt;/h2&gt;

&lt;p&gt;Coloque a pasta em cima de torradinhas ou de doritos, bote algumas sementes de ervilha e edamame em cima e depois &amp;ldquo;Pouvilhe&amp;rdquo; com gergelim misto para decorar.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DKcf8GZKx6X/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Capybara, pareto principle and bureaucracy</title>
            <link rel="alternate" href="/work/personal/capybara/capybara-pareto-principle-bureaucracy/" type="text/html" />
            <id>/work/personal/capybara/capybara-pareto-principle-bureaucracy/</id>
            <updated>2025-09-01T00:32:00Z</updated>

            <summary type="html">In December 2017, I proudly released &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.timotta.games.monsterarrow&#34;&gt;Capybara and the Goo&lt;/a&gt; game into the wild.</summary>
            <content type="html">&lt;p&gt;In December 2017, I proudly released &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.timotta.games.monsterarrow&#34;&gt;Capybara and the Goo&lt;/a&gt; game into the wild.&lt;/p&gt;

&lt;p&gt;It had taken me two years to develop, though—confession time—about 80% of the actual work was crammed into just three months. The rest of the time? Endless tweaking of layouts, designs, and silky-smooth transitions. Basically, a Pareto principle masterclass: 20% inspiration, 80% fiddling.&lt;/p&gt;

&lt;p&gt;The original idea was to make something like the classic game Lemmings. But as so often happens in game dev, the project evolved… and mutated… until I had something completely different. The capybara protagonist was born out of pure local inspiration: my hometown is crawling with capybaras. They’re fluffy, they’re cute, and honestly, who wouldn’t want one as their game’s mascot?&lt;/p&gt;

&lt;p&gt;At launch, the game had a mini boom—over a thousand installs! Not bad for a gooey rodent experiment. The only catch was that I hadn’t built in any sticky “keep playing forever” mechanics like collectibles or social hooks. The only long-term engagement feature was a ranking system using Google Play Games. My plan was to add new content slowly over time… but then life (and shinier projects) got in the way. The game slipped into the shadows.&lt;/p&gt;

&lt;p&gt;Years passed. Google Play eventually yanked it from the store, disabled my developer account, and buried me under ignored emails about SDK updates and new forms. Bureaucracy: 1, Tiago: 0.&lt;/p&gt;

&lt;p&gt;Then, out of nowhere, my Panamanian niece asked her dad (my brother-in-law) if she could play “the Capybara game.” Panic! I had no way to give her access—the game was gone, and I wasn’t even sure I could build it again after all this time.&lt;/p&gt;

&lt;p&gt;Spoiler: I couldn’t. The game was built in &lt;a href=&#34;https://love2d.org/&#34;&gt;Love2D&lt;/a&gt;—a &lt;a href=&#34;https://www.lua.org/&#34;&gt;Lua&lt;/a&gt;-based game engine with an Android wrapper—and everything was tied to an ancient version of that setup. Updating it was anything but simple. Since the engine isn’t exactly mainstream, it felt like every bug was a one-of-a-kind nightmare just for me. What I thought would take two days stretched into two painful weeks.&lt;/p&gt;

&lt;p&gt;Past Tiago had at least left some gifts: a bunch of handy visual debug tools I’d learned to build back in my Globo Videos (now &lt;a href=&#34;https://globoplay.globo.com/&#34;&gt;Globo Play&lt;/a&gt;) days, back when we were migrating from the good old Windows Media Player to the brave new world of Macromedia Flash.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/capybara.png&#34; alt=&#34;/assets/images/capybara.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;But Past Tiago had also left traps: cryptic magic numbers with names so indecipherable that even Future Tiago couldn’t guess what they meant. Thanks, buddy.&lt;/p&gt;

&lt;p&gt;Still, the hardest boss fight wasn’t the code—it was Google Play. Endless forms, mysterious “review” failures, especially for my bank statement. No explanation, just “Nope. Try again.” After nearly a week of trial and error, I finally broke through.&lt;/p&gt;

&lt;p&gt;And so, &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.timotta.games.monsterarrow&#34;&gt;Capybara and the Goo&lt;/a&gt; is back! My family can play, my niece is happy, and maybe—just maybe—I’ll add new levels, collectibles, or social features this time. That is, if Google Play doesn’t drain all my life energy first. They’ve already set a deadline: update again by November 1st, or else. Bureaucracy never sleeps.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Cuca beludo de leite condensado</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/cuca-beludo-leite-condensado/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/cuca-beludo-leite-condensado/</id>
            <updated>2025-08-09T01:39:00Z</updated>

            <summary type="html">A receita original não leva suco de limão. Eu adicionei em uma tentativa de fazer uma cuca sabor limão. Não deu o sabor de limão mas ela ficou melhor do que da primeira vez que fiz sem limão. Outra coisa que fiz melhor nesta versão que a receita original é peneirar a gema de ovo. Na próxima vez que eu fizer esta receita, tentarei sem o amido de milho.</summary>
            <content type="html">&lt;p&gt;A receita original não leva suco de limão. Eu adicionei em uma tentativa de fazer uma cuca sabor limão. Não deu o sabor de limão mas ela ficou melhor do que da primeira vez que fiz sem limão. Outra coisa que fiz melhor nesta versão que a receita original é peneirar a gema de ovo. Na próxima vez que eu fizer esta receita, tentarei sem o amido de milho.&lt;/p&gt;

&lt;h2&gt;Igredientes do creme&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;395g de leite condensado (1 lata)&lt;/li&gt;
&lt;li&gt;200ml de creme de leite&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de amido de milho&lt;/li&gt;
&lt;li&gt;2 gemas&lt;/li&gt;
&lt;li&gt;300 ml de leite&lt;/li&gt;
&lt;li&gt;1 colher de sopa de essência de baunilha&lt;/li&gt;
&lt;li&gt;Suco de meio limão&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Igredientes da farofa&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;2 xícaras e &amp;frac12; de farinha de trigo&lt;/li&gt;
&lt;li&gt;1 xícara de açucar cristal&lt;/li&gt;
&lt;li&gt;1 ovo&lt;/li&gt;
&lt;li&gt;100g de manteiga gelada&lt;/li&gt;
&lt;li&gt;1 colher de sopa de fermento em pó&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo do creme&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Misture bem o leite condensado com o amido de milho para que não fique nenhuma pelota&lt;/li&gt;
&lt;li&gt;Agora misture a gema, peneirando ela para que as parte mais solidas não sejam misturadas&lt;/li&gt;
&lt;li&gt;Misture o restante dos igredientes&lt;/li&gt;
&lt;li&gt;Cozinhe em fogo baixo ou médio sempre mexendo até que fique com a consistência de um mingau&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Preparo da farofa&lt;/h2&gt;

&lt;p&gt;Misture todos os igredientes com uma espátula inicialmente e depois com as mãos misture até tentando apertar todas as pelotinhas de manteiga até que vire um farofa.&lt;/p&gt;

&lt;h2&gt;Preparo final&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Em um refratário untado com manteiga, coloque uma camada com metade da farofa na parte de baixo, o creme no meio e outra camada com a outra metade da farofa na parte de cima.&lt;/li&gt;
&lt;li&gt;Coloque no forno até começar a dourar&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DJouOP8teYW/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Mantecol: Doce argentino de natal</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/mantecol-doce-argentino-natal/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/mantecol-doce-argentino-natal/</id>
            <updated>2025-08-04T13:29:00Z</updated>

            <summary type="html">Nas ultimoas duas vezes que fiz essa receita, ele ficou muito mole não firmando o torrão que deveria se tornar. Eu imagino que a causa seja eu ter esquentado o açucar menos do que deveria. Outra possibilidade foi que na segunda vez eu não utilizei um recipiente firme com papel anti aderente ao final, usei uma forma de silicone. Tentarei novamente depois.</summary>
            <content type="html">&lt;p&gt;Nas ultimoas duas vezes que fiz essa receita, ele ficou muito mole não firmando o torrão que deveria se tornar. Eu imagino que a causa seja eu ter esquentado o açucar menos do que deveria. Outra possibilidade foi que na segunda vez eu não utilizei um recipiente firme com papel anti aderente ao final, usei uma forma de silicone. Tentarei novamente depois.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;400g de amendoim torrado&lt;/li&gt;
&lt;li&gt;3 colheres de sopa de óleo&lt;/li&gt;
&lt;li&gt;300g de açucar&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de mel&lt;/li&gt;
&lt;li&gt;100ml de água&lt;/li&gt;
&lt;li&gt;Três claras de ovo&lt;/li&gt;
&lt;li&gt;Papel manteiga ou anti aderente&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Coloque no processador o amendoim e bata com as 3 colheres de óleo até virar o amendoim virar uma pasta&lt;/li&gt;
&lt;li&gt;Em uma panela coloque o açucar, o mel e a água e deixe no fogo até alcançar 125 graus ou formar uma bolhinha se colcado na água fria&lt;/li&gt;
&lt;li&gt;Enquanto o açucar esquenta, bata a clara em uma batedeira até virar neve.&lt;/li&gt;
&lt;li&gt;Ainda batendo, vá misturando o açucar aos poucos na batedeira e bata por mais uns 5 minutos&lt;/li&gt;
&lt;li&gt;Depois de parar de bater, misture a pasta de amendoim feita anteriormente nesta clara em neve adocicada até ficar homogêneo&lt;/li&gt;
&lt;li&gt;Coloque essa massa em um recipiente forrado com papel anti aderente e deixe esfriar na geladeira.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DDuvSBBvzui/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Bolo japonês abrasileirado sem glútem</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/bolo-japones-abrasileirado-sem-gluten/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/bolo-japones-abrasileirado-sem-gluten/</id>
            <updated>2025-07-29T23:04:00Z</updated>

            <summary type="html">Com a receita original desse bolo, achei muito sem sabor e não consegui firmar bem, ficando quase um mingau, então modifiquei algumas coisas para abrasileirar e ficou bem melhor e mais firme, embora ainda um bolo bem suave.</summary>
            <content type="html">&lt;p&gt;Com a receita original desse bolo, achei muito sem sabor e não consegui firmar bem, ficando quase um mingau, então modifiquei algumas coisas para abrasileirar e ficou bem melhor e mais firme, embora ainda um bolo bem suave.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;3 gemas de ovo&lt;/li&gt;
&lt;li&gt;50g de farinha de arroz&lt;/li&gt;
&lt;li&gt;50g de açucar refinado&lt;/li&gt;
&lt;li&gt;500ml de leite&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Misture a farinha, o açucar e o leite&lt;/li&gt;
&lt;li&gt;Coe as gemas de ovo para tirar as partes meio durinhas e misture&lt;/li&gt;
&lt;li&gt;Leve a fogo baixo e mexendo constantemente até formar um creme homogêneo&lt;/li&gt;
&lt;li&gt;Leve a geladeira por 2 a 3 horas.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;No futuro vou testar trocar o açucar por stevia para fazer uma versão além de sem glutem, também light.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DEk5Z0GyTWy/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Cenoura deliciosa sem gosto de cenoura</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/cenoura-deliciosa-sem-gosto-cenoura/" type="text/html" />
            <id>/pessoal/receitas/refeies/cenoura-deliciosa-sem-gosto-cenoura/</id>
            <updated>2025-07-29T23:02:00Z</updated>

            <summary type="html">Eu gosto de cenoura, mas essa receita é também pra quem não gosta de cenoura. O sabor é totalmente diferente de cenoura e fica delicioso.</summary>
            <content type="html">&lt;p&gt;Eu gosto de cenoura, mas essa receita é também pra quem não gosta de cenoura. O sabor é totalmente diferente de cenoura e fica delicioso.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;3 cenoura médias (Eu usei 1 cenoura e meia grande)&lt;/li&gt;
&lt;li&gt;2 colher de chá de manteiga&lt;/li&gt;
&lt;li&gt;1 colher de sopa de raspas de casca de laranja&lt;/li&gt;
&lt;li&gt;1 colher de sopa de gengibre ralado&lt;/li&gt;
&lt;li&gt;1 colher de sopa de alho picado&lt;/li&gt;
&lt;li&gt;200ml suco de laranja&lt;/li&gt;
&lt;li&gt;Uma pitada de páprica picante&lt;/li&gt;
&lt;li&gt;Sal&lt;/li&gt;
&lt;li&gt;Uma pitada de pimenta calabresa&lt;/li&gt;
&lt;li&gt;1 colher de chá de missô&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de mel&lt;/li&gt;
&lt;li&gt;Gergelim&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Emulsione a manteiga com um pouco de água&lt;/li&gt;
&lt;li&gt;Salteie as cenouras nessa emulsão com páprica até elas começarem a dourar&lt;/li&gt;
&lt;li&gt;Adicione o alho, gengibre e as raspas da casca de limão e doure por mais um minuto&lt;/li&gt;
&lt;li&gt;Coloque o suco de laranja, o missô, o mel, a pimenta calabresa e o sal&lt;/li&gt;
&lt;li&gt;Quando as cenouras estiverem macias, mas não moles, retire-as do liquido&lt;/li&gt;
&lt;li&gt;Coe o liquido com uma peneira&lt;/li&gt;
&lt;li&gt;Leve o liquido coado a fogo baixo para reduzir e se tornar um molho&lt;/li&gt;
&lt;li&gt;Sirva as cenouras com um pouco desse molho por cima e gergelim para decorar&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DLigHGYychx/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Cookies rápidos com gotas de chocolate, ou não</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/cookies-rapidos-chocolate-ou-nao/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/cookies-rapidos-chocolate-ou-nao/</id>
            <updated>2025-07-29T22:54:00Z</updated>

            <summary type="html">Essa receita eu fiz com gota de chocolate e sem. Eu preferi sem gota de chocolate, para sentir o sabor apenas do biscoito. Mas de ambas formas fica boa.</summary>
            <content type="html">&lt;p&gt;Essa receita eu fiz com gota de chocolate e sem. Eu preferi sem gota de chocolate, para sentir o sabor apenas do biscoito. Mas de ambas formas fica boa.&lt;/p&gt;

&lt;p&gt;Você vai precisar de um tapetinho de silicone anti aderente. Sem isso é bem possível que seu biscoito fique grudado na travessa.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;110g de manteiga com sal (se for sem sal, como na receita original, você precisará botar sal no final)&lt;/li&gt;
&lt;li&gt;150g de açucar mascavo&lt;/li&gt;
&lt;li&gt;115g de açucar cristal&lt;/li&gt;
&lt;li&gt;1 ovo grande&lt;/li&gt;
&lt;li&gt;1 colher de sopa de essência de baunilha&lt;/li&gt;
&lt;li&gt;200g de farinha de trigo&lt;/li&gt;
&lt;li&gt;&amp;frac12; colher de sopa de bicarbonato de sódio&lt;/li&gt;
&lt;li&gt;200g de gotas de chocolate (opcional)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Derreta a manteiga e deixe esfriar um pouco&lt;/li&gt;
&lt;li&gt;Junte os açucares com a manteiga e misture até ficar homogêneo&lt;/li&gt;
&lt;li&gt;Coloque o ovo e a essência de baunilha e misture novamente até ficar homogêneo&lt;/li&gt;
&lt;li&gt;Adicione a farinha e o bicarbonato de sódio e misture até ficar homogêneo&lt;/li&gt;
&lt;li&gt;Opcionalmente adicione as gotas de chocolate e misture até ficar bem espalhado&lt;/li&gt;
&lt;li&gt;Forme bolinhas pequenas e coloque espalhado sobre o tapete de silicone&lt;/li&gt;
&lt;li&gt;Coloque no forno médio pré aquecido por 10 minutos&lt;/li&gt;
&lt;li&gt;Tire e espere esfriar um pouco para tirar do tapete&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Aqui é importante notar que eu testei algumas minutagens. Na receita original fala 5 minutos. Mas ficou muito mole, mesmo depois de esfriado. Fui aumentando um pouco até chegar na minutagem de 10 minutos, que deu uma consistência crocante mas macio por dentro. Contudo vai depender muito de cada forno.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DLh4evNIE2f/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Salada mexicana clássica de milho</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/salada-mexicana-classica-milho/" type="text/html" />
            <id>/pessoal/receitas/refeies/salada-mexicana-classica-milho/</id>
            <updated>2025-07-05T22:41:00Z</updated>

            <summary type="html">Essa aqui eu já fiz com salsinha e com coentro. Com coentro ficou mil vezes melhor. Mas na falta, vai de salsinha mesmo. Ainda não experimentei com jalapeño.</summary>
            <content type="html">&lt;p&gt;Essa aqui eu já fiz com salsinha e com coentro. Com coentro ficou mil vezes melhor. Mas na falta, vai de salsinha mesmo. Ainda não experimentei com jalapeño.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;2 xícaras de milho verde cozido&lt;/li&gt;
&lt;li&gt;1 colher de sopa de manteiga&lt;/li&gt;
&lt;li&gt;1 xícara de parmesão ralado fino&lt;/li&gt;
&lt;li&gt;2 dentes de alho picado&lt;/li&gt;
&lt;li&gt;&amp;frac12; xícara de iogurte natural&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de maionese&lt;/li&gt;
&lt;li&gt;&amp;frac12; cebola roxa picada&lt;/li&gt;
&lt;li&gt;&amp;frac12; pimentão vermelho picado (ou jalapeño)&lt;/li&gt;
&lt;li&gt;1 xícara de coentro picado (ou salsinha)&lt;/li&gt;
&lt;li&gt;Sal&lt;/li&gt;
&lt;li&gt;Pimenta do reino&lt;/li&gt;
&lt;li&gt;&amp;frac12; limão&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Passe a manteiga numa frigideira&lt;/li&gt;
&lt;li&gt;Adicione o alho picado e dá uma mexidinha&lt;/li&gt;
&lt;li&gt;Adiciona o milho cozido&lt;/li&gt;
&lt;li&gt;Tempera com sal e pimenta do reino a gosto&lt;/li&gt;
&lt;li&gt;Refoga bem até ficar dourado&lt;/li&gt;
&lt;li&gt;Despeja em uma tigela grande&lt;/li&gt;
&lt;li&gt;Coloca o iogurte grego, a maionese e o queijo parmesão e mistura&lt;/li&gt;
&lt;li&gt;Adiciona a cebola roxa picada, o pimentão vermelho, o coentro e mistura&lt;/li&gt;
&lt;li&gt;Por fim, coloque o limão expremido e misture mais um pouco&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DI69LzsyzKZ/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Bean-fu da Birmânia de lentilha ou ervilha</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/bean-fu-lentilha-ervilha/" type="text/html" />
            <id>/pessoal/receitas/refeies/bean-fu-lentilha-ervilha/</id>
            <updated>2025-06-30T17:25:00Z</updated>

            <summary type="html">Bean-fu é o tofu feito com outras leguminosas. Ainda não testei com feijão fradinho ou branco, e nem com milho. Mas testarei.</summary>
            <content type="html">&lt;p&gt;Bean-fu é o tofu feito com outras leguminosas. Ainda não testei com feijão fradinho ou branco, e nem com milho. Mas testarei.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;200g de lentilha, lentilha rosa ou ervilha&lt;/li&gt;
&lt;li&gt;200ml de água&lt;/li&gt;
&lt;li&gt;3 colheres de chá de sal&lt;/li&gt;
&lt;li&gt;Azeite de oliva&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo inicial&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Deixe de molho por 12 horas a leguminosa&lt;/li&gt;
&lt;li&gt;Escorra a água do molho lavando as leguminosas&lt;/li&gt;
&lt;li&gt;Bata no liquidificador a legminosa com 200ml de água e o sal até ficar bem homogêneo&lt;/li&gt;
&lt;li&gt;Derrame a mistura em uma panela&lt;/li&gt;
&lt;li&gt;Em fogo baixo, cozinhe essa mistura mexendo o tempo todo tentando fazer não grudar na panela. Se grudar também não tem problema.&lt;/li&gt;
&lt;li&gt;Quando estiver quase firme, derrame em um recipiente de forma que fique com um dedo de altura&lt;/li&gt;
&lt;li&gt;Coloque na geladeira&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Preparo final&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;O bean-fu já estará firme e pode ser retirado do recipiente como um blocão&lt;/li&gt;
&lt;li&gt;Corte em cubinhos, pincele com azeite de oliva&lt;/li&gt;
&lt;li&gt;Coloque na airfryer por mais ou menos 10 minutos&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DExcXTjJShy/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Lasanha de de banana da terra com carne de soja</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/lasanha-banana-terra-carne-soja/" type="text/html" />
            <id>/pessoal/receitas/refeies/lasanha-banana-terra-carne-soja/</id>
            <updated>2025-06-30T11:29:00Z</updated>

            <summary type="html">Essa receita rendeu tanta lasanha que tive que congelar, na próxima vez tentarei fazer em menor quantidade e anotar o quanto de cada igrediente.</summary>
            <content type="html">&lt;p&gt;Essa receita rendeu tanta lasanha que tive que congelar, na próxima vez tentarei fazer em menor quantidade e anotar o quanto de cada igrediente.&lt;/p&gt;

&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Carne de soja (Não lembro a quantidade, mas foi bastante)&lt;/li&gt;
&lt;li&gt;2 colheres de sopa de vinagre&lt;/li&gt;
&lt;li&gt;3 pimentões, um de cada cor&lt;/li&gt;
&lt;li&gt;1 cacho de banana da terra&lt;/li&gt;
&lt;li&gt;Azeite de oliva&lt;/li&gt;
&lt;li&gt;Sal&lt;/li&gt;
&lt;li&gt;Pimenta do reino&lt;/li&gt;
&lt;li&gt;Extrato de tomante&lt;/li&gt;
&lt;li&gt;Passata de tomate&lt;/li&gt;
&lt;li&gt;Queijo mussarela&lt;/li&gt;
&lt;li&gt;Queijo parmesão&lt;/li&gt;
&lt;li&gt;2 dentes de alho&lt;/li&gt;
&lt;li&gt;1 cebola&lt;/li&gt;
&lt;li&gt;&amp;frac12; colher de chá de bicarbonato de sódio&lt;/li&gt;
&lt;li&gt;Sazon de bacon (ele é vegano)&lt;/li&gt;
&lt;li&gt;Páprica defumada&lt;/li&gt;
&lt;li&gt;Uma colher de sopa de manteiga&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Carne moída&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Cozinhe a carne de soja com água e o vinagre&lt;/li&gt;
&lt;li&gt;Depois de alguns minutos fervido, escorra a água, lave o carne com água corrente apertando pra tirar o excesso dela&lt;/li&gt;
&lt;li&gt;Em outra panela refogue o alho levemente&lt;/li&gt;
&lt;li&gt;Coloque os pimentões picados bem pequenininhos em cubinho e refogue mais um pouco&lt;/li&gt;
&lt;li&gt;Coloque a cebola picada também em cubinhos, e refogue mais um pouco&lt;/li&gt;
&lt;li&gt;Coloque o bicarbonato de sódio, misture e coloque um pouquinho de água, deixe dourar&lt;/li&gt;
&lt;li&gt;Coloque a carne de soja e misture bem com o refogado&lt;/li&gt;
&lt;li&gt;Coloque o sal, a pimenta e o sazon de bacon e a páprica defumada&lt;/li&gt;
&lt;li&gt;Vá misturando e provando para acertar o sal e o cozimento, normal grudar um pouco na panela&lt;/li&gt;
&lt;li&gt;Em outra panela derreta um pouco de manteiga emulsionando com um pouco de água&lt;/li&gt;
&lt;li&gt;Misture essa manteiga com na carne, com o extrato de tomate e a passata de tomate&lt;/li&gt;
&lt;li&gt;Misture bem e prove se essa bolonhesa está bem potente de sabor&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Bananas&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Unte cada banana com azeite e coloque no forno por 15 minutos&lt;/li&gt;
&lt;li&gt;Vire elas e deixe mais 15 minutos&lt;/li&gt;
&lt;li&gt;Retire do forno&lt;/li&gt;
&lt;li&gt;Com a ajuda de um prato ou rolo, amasse elas bastante para ficar como fatias de massa de lasanha&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Lasanha&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Em um recipiente coloque: Uma camada de carne com queijo mussarela ralado em cima e outra camada de banana da terra&lt;/li&gt;
&lt;li&gt;Faça varias camadas e finalize com uma camada de carne com queijo mussarela&lt;/li&gt;
&lt;li&gt;Pouvilhe queijo parmesão ralado em cima&lt;/li&gt;
&lt;li&gt;Coloque no forno até gratinar&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DJPChWXRhCq/&#34;&gt;Receita original não é vegetariana&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Creme de tomate com capeleti e espinafre</title>
            <link rel="alternate" href="/pessoal/receitas/refeies/creme-tomate-capeleti-espinafre/" type="text/html" />
            <id>/pessoal/receitas/refeies/creme-tomate-capeleti-espinafre/</id>
            <updated>2025-06-29T10:45:00Z</updated>

            
            <content type="html">&lt;h2&gt;Igredientes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;3 tomates&lt;/li&gt;
&lt;li&gt;8 tomates secos&lt;/li&gt;
&lt;li&gt;3 colheres de sopa de extrato de tomate&lt;/li&gt;
&lt;li&gt;creme de leite&lt;/li&gt;
&lt;li&gt;1 dente de alho&lt;/li&gt;
&lt;li&gt;manjericão seco&lt;/li&gt;
&lt;li&gt;manjericão fresco&lt;/li&gt;
&lt;li&gt;folhas de espinafre&lt;/li&gt;
&lt;li&gt;sal&lt;/li&gt;
&lt;li&gt;pimenta preta&lt;/li&gt;
&lt;li&gt;capeleti fresco recheado (Esse eu compro no supermercado Zona Sul, de cogumelo trufado, mas ficou bom também de quatro queijos)&lt;/li&gt;
&lt;li&gt;caldo de legumes (Eu uso sazon mesmo)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Preparo&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Coloque 400ml em uma panela ferva com o sazon de legumes, deixe reservado.&lt;/li&gt;
&lt;li&gt;Unte uma outra panela com azeite de olive.&lt;/li&gt;
&lt;li&gt;Corte os tomates ao meio e coloque-os com a face cortada para baixo, em circulo na panela&lt;/li&gt;
&lt;li&gt;Coloque todos os dentes de alho descascados no centro do circulo&lt;/li&gt;
&lt;li&gt;Jogue sal e pimenta em cima dos tomates (Quanto? Não sei, um dia volto para dizer)&lt;/li&gt;
&lt;li&gt;Cozinhe em fogo médio com tampa fechada até perceber que é possível tirar a pele do tomate com facilidade&lt;/li&gt;
&lt;li&gt;Desligue o fogo e retire toda a pele do tomate&lt;/li&gt;
&lt;li&gt;Coloque o extrato de tomate e o caldo de legumes&lt;/li&gt;
&lt;li&gt;Com um processador bata todo tomate e alho da panela até tudo ficar líquido&lt;/li&gt;
&lt;li&gt;Espere esfriar um pouco e coloque o creme de leite e vá misturando até o vermelho ficar laranja&lt;/li&gt;
&lt;li&gt;Acenda o fogo novamente e fique mexendo, não deixe ferver pra não talhar&lt;/li&gt;
&lt;li&gt;Coloque os capeletis, o tomate seco e o manjericão seco&lt;/li&gt;
&lt;li&gt;Fique mexendo e acertando sal e pimenta, esperimentando pra ver se está saboroso e potente&lt;/li&gt;
&lt;li&gt;Quando os capeletis estiverem cozidos, coloque as folhas de espinafre&lt;/li&gt;
&lt;li&gt;Mexa mais um pouco até os espinafres estarem todos molinhos misturados&lt;/li&gt;
&lt;li&gt;Sirva e decore com manjericão fresco&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DJWwBAGOZeo/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Bolo de chocolate: Melhor receita de 2024</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/bolo-chocolate-melhor-receita-2024/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/bolo-chocolate-melhor-receita-2024/</id>
            <updated>2025-06-28T19:24:00Z</updated>

            <summary type="html">Bolo fácil de fazer, 5 minutos de microondas. Eleita a melhor receita de 2024 pelo NYT!</summary>
            <content type="html">&lt;p&gt;Bolo fácil de fazer, 5 minutos de microondas. Eleita a melhor receita de 2024 pelo NYT!&lt;/p&gt;

&lt;h2&gt;Bolo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;1/3 de xícara de manteiga derretida&lt;/li&gt;
&lt;li&gt;&amp;frac14; de xícara de leite&lt;/li&gt;
&lt;li&gt;1 colher de chá de essência de baunilha&lt;/li&gt;
&lt;li&gt;1 colher de sopa de café passado forte&lt;/li&gt;
&lt;li&gt;&amp;frac34; de xícara de açucar mascavo&lt;/li&gt;
&lt;li&gt;1 xícara de farinha&lt;/li&gt;
&lt;li&gt;&amp;frac12; xícara de cacau em pó 50%&lt;/li&gt;
&lt;li&gt;&amp;frac12; colher de chá de sal&lt;/li&gt;
&lt;li&gt;&amp;frac12; colher de sopa de fermento químico em pó&lt;/li&gt;
&lt;li&gt;&amp;frac14; de colher de sopa de bicarbonato&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Calda&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&amp;frac34; de xícara de água quente&lt;/li&gt;
&lt;li&gt;&amp;frac12; xícara de açucar mascavo&lt;/li&gt;
&lt;li&gt;&amp;frac14; de xícara de cacau em pó 50%&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pra fazer o bolo&lt;/h2&gt;

&lt;p&gt;Mistura tudo com uma colher até ficar uma massa homogênea, espalhe sobre um recipiente que possa ir pro microondas mas não coloque o no microondas ainda.&lt;/p&gt;

&lt;h2&gt;Pra fazer a calda&lt;/h2&gt;

&lt;p&gt;Misture o cacau em pó e o açucar mascavo até ficar homogeneo, e cubra o bolo com essa &amp;ldquo;farinha&amp;rdquo; e depois derrame com cuidado a água quente sobre esse o bolo.&lt;/p&gt;

&lt;h2&gt;Finalizando&lt;/h2&gt;

&lt;p&gt;Coloque no microondas por 5 minutos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DFqojLzyV6z/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The Journey to Hybrid Search at OLX Brasil</title>
            <link rel="alternate" href="/work/olxbrasil/search/jouney-hybrid-search/" type="text/html" />
            <id>/work/olxbrasil/search/jouney-hybrid-search/</id>
            <updated>2025-06-07T23:36:00Z</updated>

            <summary type="html">This is the story of how we improved the search experience at OLX Brasil in early 2024 by introducing semantic search.</summary>
            <content type="html">&lt;p&gt;This is the story of how we improved the search experience at OLX Brasil in early 2024 by introducing semantic search.&lt;/p&gt;

&lt;p&gt;We’ve always known that search is at the core of the user experience. When someone types something like “European football team shirt,” they’re not just entering keywords — they’re expressing intent. Our traditional lexical search engine, &lt;a href=&#34;/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/&#34;&gt;Podium&lt;/a&gt;, handled direct matches well, but often failed when the query was more conceptual or abstract.&lt;/p&gt;

&lt;p&gt;That’s where it all started — with a hackday.&lt;/p&gt;

&lt;h2&gt;From Hackday to Proof of Value&lt;/h2&gt;

&lt;p&gt;During an internal experiment started by &lt;a href=&#34;https://www.linkedin.com/in/daniel-correa-araujo/&#34;&gt;Daniel Araújo&lt;/a&gt;, he tested the use of the OpenAI embeddings API to improve our ability to understand and respond to semantic queries. The idea was simple: what if search could understand concepts, not just strings?&lt;/p&gt;

&lt;p&gt;The first tests showed something powerful: it was able to return relevant results for many examples of queries like “European football team shirt” — even when listing titles didn’t contain those exact words. This revealed real potential in semantic search.&lt;/p&gt;

&lt;p&gt;But there was skepticism.&lt;/p&gt;

&lt;h2&gt;A PM’s Bet (and a Little Faith)&lt;/h2&gt;

&lt;p&gt;Our initial hypothesis was conservative: this technology wouldn’t increase replies (the number of contacts on listings). At best, we thought it might improve coverage slightly — and at a high operational cost. Still, our Product Manager, &lt;a href=&#34;https://www.linkedin.com/in/claudiabozza/&#34;&gt;Claudia Bozza&lt;/a&gt; believed in it and pushed forward, even when we flagged potential cost-benefit concerns.&lt;/p&gt;

&lt;p&gt;And she was right.&lt;/p&gt;

&lt;h2&gt;First Version: Fast Build, Fast Results&lt;/h2&gt;

&lt;p&gt;We built a quick solution in just two weeks: if a lexical search returned no results, we would trigger a fallback semantic search. Even with this simple approach, the results were impressive. That gave us the confidence to invest in building a robust hybrid search system.&lt;/p&gt;

&lt;h2&gt;A Strategic Shift: Moving Away from OpenAI&lt;/h2&gt;

&lt;p&gt;As the solution evolved, we decided to move away from OpenAI’s API. Response times were too high for real-time search. We also tested Google’s Gemini, and faced similar issues.&lt;/p&gt;

&lt;p&gt;Instead, we adopted local embedding generation using &lt;a href=&#34;https://sbert.net/&#34;&gt;sentence-transformers&lt;/a&gt;, giving us more control and performance. We started generating vectors in-house, allowing the solution to scale more efficiently.&lt;/p&gt;

&lt;h2&gt;So why hybrid, and not purely semantic?&lt;/h2&gt;

&lt;p&gt;For OLX’s search engine, diversity and recency are critical metrics. We had already proven in several past experiments that both correlate strongly with the number of replies and how quickly items are sold.&lt;/p&gt;

&lt;p&gt;When it comes to diversity, we noticed a major drawback in using a purely semantic approach: if we generated vector embeddings using only the ad title, diversity dropped to near zero for simple one- or two-word queries. This happened because the cosine distance between the word &amp;ldquo;iPhone&amp;rdquo; and an ad titled simply &amp;ldquo;iPhone&amp;rdquo; is effectively zero. Since we have a large volume of listings with minimal or generic titles, this led to highly repetitive results.&lt;/p&gt;

&lt;p&gt;On the other hand, the more ad fields we added to the embedding (e.g. description, location), the better the diversity became — but at the cost of precision.&lt;/p&gt;

&lt;p&gt;As for recency, even when we applied time decay functions in the final score computation, purely semantic search often returned very old listings — especially in queries with lots of potential matches (which is most of them). That’s because in vector search, you have to define how many items each shard should return. Those selected items won’t necessarily include the newest listings. For example, if a user searches for “iPhone” and we have 20,000 iPhone listings per shard, but only retrieve the top 50 per shard, the most recent ads might be buried in the remaining 19,950 items — even if their cosine distance is only slightly worse.&lt;/p&gt;

&lt;p&gt;We could increase the number of items per shard, but that comes at a steep cost in processing time and infrastructure usage.&lt;/p&gt;

&lt;p&gt;After extensive experimentation, we concluded that a hybrid search strategy offered the best of both worlds. The offline results confirmed this: hybrid search provided the strongest balance between ranking precision and overall quality.&lt;/p&gt;

&lt;h2&gt;Engineering the Hybrid Search Algorithm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/hybrid-search.png&#34; alt=&#34;/assets/images/podium-math.gif&#34;&gt;&lt;/p&gt;

&lt;p&gt;Once the concept was validated, we moved into an intensive algorithm analysis phase. We ran over 40 offline experiments, evaluating key relevance metrics such as freshness, diversity and &lt;a href=&#34;https://medium.com/grupoolxtech/uma-nova-m%C3%A9trica-para-calcular-relev%C3%A2ncia-de-busca-65372f154f8f&#34;&gt;R-DCG (ranking-aware precision)&lt;/a&gt;. During this phase, we validated several crucial parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The number of items to retrieve from the vector search&lt;/li&gt;
&lt;li&gt;Which ad fields (title, description, location, etc.) to use for embedding generation&lt;/li&gt;
&lt;li&gt;Which embedding model to use (and how it performed on our dataset)&lt;/li&gt;
&lt;li&gt;How to combine lexical and semantic results effectively&lt;/li&gt;
&lt;li&gt;How to sort results after merging both types of rankings&lt;/li&gt;
&lt;li&gt;Which type of time decay function to apply to maintain result recency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This meticulous tuning allowed us to strike the right balance between semantic understanding and ranking precision. At the end, we could choose the best combination of parameters to get an algorithm that improved:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Freshness: +65%&lt;/li&gt;
&lt;li&gt;Diversity: +9%&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/grupoolxtech/uma-nova-m%C3%A9trica-para-calcular-relev%C3%A2ncia-de-busca-65372f154f8f&#34;&gt;R-DCG&lt;/a&gt;: +0.72%&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Experiments in Production&lt;/h2&gt;

&lt;p&gt;Bringing the solution to production experiments wasn’t straightforward.&lt;/p&gt;

&lt;p&gt;We had a hypothesis: the increased response time of lexical searches — caused by sharing infrastructure with the hybrid search — could negatively impact the control group’s metrics in A/B tests.&lt;/p&gt;

&lt;p&gt;To ensure a fair comparison, we set up a dedicated ElasticSearch infrastructure specifically for the hybrid search experiments. This separation ensured that the computational overhead of vector search wouldn’t degrade the performance of pure lexical queries.&lt;/p&gt;

&lt;p&gt;As a result, all experiments were run in an almost fully isolated infrastructure setup.&lt;/p&gt;

&lt;h2&gt;The Impact: Results that Justify the Investment&lt;/h2&gt;

&lt;p&gt;Despite the increase in response time, the gains in textual searches were undeniable:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;+3.7% in repliers&lt;/li&gt;
&lt;li&gt;+4.3% in replies&lt;/li&gt;
&lt;li&gt;+4.3% in clicks&lt;/li&gt;
&lt;li&gt;And most notably: a 94% reduction in “no result” queries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Encouraged by these results, we decided to roll out the solution to 100% of users. But there was a problem — the cost.&lt;/p&gt;

&lt;h2&gt;Performance: The Millisecond Challenge&lt;/h2&gt;

&lt;p&gt;Initially, the hybrid search response time was a major concern: &lt;strong&gt;P95 &amp;gt; 300ms&lt;/strong&gt;, compared to &lt;strong&gt;~80ms&lt;/strong&gt; for pure lexical search. This made a full rollout unfeasible without quadrupling our infrastructure costs.&lt;/p&gt;

&lt;p&gt;To tackle this, we launched a series of 24 performance experiments, each focused on shaving off latency. And for each experiment, we monitored key business metrics as guardrails to ensure we didn’t lose the gains we had previously achieved. Some of the key optimizations included:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Vector quantization&lt;/li&gt;
&lt;li&gt;Increasing the number of shards&lt;/li&gt;
&lt;li&gt;Upgrading ElasticSearch&lt;/li&gt;
&lt;li&gt;Reducing HNSW candidates&lt;/li&gt;
&lt;li&gt;Merging lexical search fields&lt;/li&gt;
&lt;li&gt;Generating embeddings using GPUs&lt;/li&gt;
&lt;li&gt;And more&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There was no silver bullet, but together these improvements brought &lt;strong&gt;P95 down to ~120ms&lt;/strong&gt; — even faster than other companies in our group that had also implemented hybrid search.&lt;/p&gt;

&lt;h2&gt;Conclusion: Progress, Perception&lt;/h2&gt;

&lt;p&gt;The biggest improvements came in conceptual and indirect searches, where vector models truly understood user intent. But new challenges emerged too. Queries that previously returned nothing started returning irrelevant results, which led to complaints and a perception of low precision.&lt;/p&gt;

&lt;p&gt;This perception didn’t exist in the pure Podium model — if a result didn’t match, it returned nothing. In the hybrid version, we had to deal with the qualitative nuance of returning something that might not feel useful to the user.&lt;/p&gt;

&lt;p&gt;That was the focus of our work during my final months there — work I didn’t get to finish: tuning problematic cases and exploring ways to visually communicate that certain results might be &amp;ldquo;off&amp;rdquo;, while still being the best semantic match available.&lt;/p&gt;

&lt;p&gt;What stood out during this journey was the company’s willingness to invest — not only in research, but also financially — to improve user experience. Even with performance improvements, hybrid search required more computing power: more ElasticSearch resources and ongoing vector processing.&lt;/p&gt;

&lt;h2&gt;The Economics Behind the Rollback&lt;/h2&gt;

&lt;p&gt;Recently, some friends who still work at OLX shared that hybrid search was eventually sunset, and OLX reverted back to &lt;a href=&#34;/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/&#34;&gt;Podium’s&lt;/a&gt; pure lexical model due to infrastructure costs.&lt;/p&gt;

&lt;p&gt;As an active user of the platform, I had already noticed a drop in views and contacts on my OLX listings. I even tested the search myself to understand what was happening — and the absence of semantic understanding was immediately noticeable. &lt;a href=&#34;https://www.linkedin.com/in/raphael-pinheiro-b6530a107/&#34;&gt;Raphael Pinheiro&lt;/a&gt;, who is not only a developer but also sells products made with his 3D printer, also noticed a sharp decline in views on his listings. He even mentioned it to me — without knowing that the hybrid search had been rolled back.&lt;/p&gt;

&lt;p&gt;Of course, these are anecdotal examples, and the overall impact was probably small enough to justify the decision — especially in core categories like real estate, vehicles, appliances, and furniture. But for smaller, more niche categories, the effect may have been significant. A 50% drop in views and replies in those segments might represent less than 1% of the total, but for that specific seller, it’s a huge impact.&lt;/p&gt;

&lt;p&gt;It’s unfortunate, because the progress was substantial and there was still room for optimization. All major players in search are investing in hybrid approaches, because they’re not only proven to deliver better results, but also offers better scalability for product development by avoiding manual tuning of lemmatization, synonyms, and other linguistic rules.&lt;/p&gt;

&lt;p&gt;Hybrid search may be gone from production, but the knowledge, progress, and collaboration built along the way remain one of the most impactful chapters of that journey.&lt;/p&gt;

&lt;h2&gt;The Minds Behind the Magic&lt;/h2&gt;

&lt;p&gt;Just like &lt;a href=&#34;/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/&#34;&gt;Podium&lt;/a&gt;, the hybrid search was build collaborativelly by &lt;a href=&#34;https://www.linkedin.com/in/timotta/&#34;&gt;me&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/daniel-correa-araujo/&#34;&gt;Daniel Araújo&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/raphael-pinheiro-b6530a107/&#34;&gt;Raphael Pinheiro&lt;/a&gt;. The engineering manager at the time was &lt;a href=&#34;https://www.linkedin.com/in/paulohesilva/&#34;&gt;Paulo Silva&lt;/a&gt; and the product manager &lt;a href=&#34;https://www.linkedin.com/in/claudiabozza/&#34;&gt;Claudia Bozza&lt;/a&gt;. A special thanks to &lt;a href=&#34;https://www.linkedin.com/in/josehisse/&#34;&gt;José Hisse&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/denilsonjunior/&#34;&gt;Denilson Limoeiro&lt;/a&gt; who helped us putting the vector generation running under GPU. Please accept my sincere apologies if I inadvertently omitted anyone.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Patching scikit-learn to improve API performance</title>
            <link rel="alternate" href="/work/willbank/account/patching-scikit-learn-improve-api-performance/" type="text/html" />
            <id>/work/willbank/account/patching-scikit-learn-improve-api-performance/</id>
            <updated>2025-02-27T13:49:00Z</updated>

            <summary type="html">This week at Will Bank, we deployed a new machine learning model to compete with the one we introduced into production late last year. Offline evaluations showed promising results, with a 35% improvement in the target metric and a cumulative gain of 60% compared to the solution that preceded last year’s model.</summary>
            <content type="html">&lt;p&gt;This week at Will Bank, we deployed a new machine learning model to compete with the one we introduced into production late last year. Offline evaluations showed promising results, with a 35% improvement in the target metric and a cumulative gain of 60% compared to the solution that preceded last year’s model.&lt;/p&gt;

&lt;p&gt;However, during the initial deployment of this new model—where we logged the results without impacting end users—we observed a significantly higher computational cost compared to the previous model. The API response time at the 99th percentile was 260% higher than that of the previous model.&lt;/p&gt;

&lt;p&gt;Indeed, the new model used more features, and the selected hyperparameters of the &lt;a href=&#34;https://xgboost.readthedocs.io/en/stable/&#34;&gt;XGBoost&lt;/a&gt; classifier indicated the use of more estimators and greater depth in the trees. Furthermore, the size of the model was significantly larger than the previous model. We had moved from a model of approximately 150kb to one of about 4mb in pickle format.&lt;/p&gt;

&lt;p&gt;This larger size made complete sense as we had added some categorical features with a greater diversity of values. Since we used &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html&#34;&gt;TargetEncoder&lt;/a&gt; and &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html&#34;&gt;OneHotEncoder&lt;/a&gt; to handle these features, and these preprocessing steps are part of the persisted &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html&#34;&gt;Pipeline&lt;/a&gt;, all possible categories would be saved in the final pickle, resulting in a much larger file.&lt;/p&gt;

&lt;p&gt;We did not expect this to be the cause of the slowdown, as a category lookup in the TargetEncoder or OneHotEncoder should be O(1), likely using a dictionary to store the valur for each category. As a result, we initially ruled out model size as a contributing factor to the issue.&lt;/p&gt;

&lt;p&gt;We then proceeded to evaluate whether the cause of the slowness was the deeper hyperparameters of XGBoost.&lt;/p&gt;

&lt;p&gt;I then retrained the model with softer parameters to speed up the results with a smaller amount of data. The result of this retraining was a much more performant model. I could conclude here that I had found the cause of the problem: the hyperparameters. But there was one more difference in the experiment setup: the amount of data.&lt;/p&gt;

&lt;p&gt;With this in hands I decided to put it to the test and repeated the hyperparameters of the problematic model, training with a small amount of data, and the model performed just as well as the one with smoother hyperparameters. In other words, the hyperparameters were not the cause of the slowness.&lt;/p&gt;

&lt;p&gt;I changed the approach to this analysis and decided to break down the problematic model by testing the execution time of each step in the Pipeline. Here, we benefit from the flexibility and standardization of the Pipelines and steps in &lt;a href=&#34;https://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt;. As a result, it was possible to observe that it was indeed the TargetEncoder of a single column that was causing 95% of the computational cost of the model. And it was precisely for the feature that had the highest variety of categories.&lt;/p&gt;

&lt;p&gt;This didn&amp;rsquo;t make sense to me; the number of different categories shouldn&amp;rsquo;t be causing performance issues. In my mind, looking up the calculated value would be done in dictionary O(1) time. So, I decided to investigate the code of the TargetEncoder from scikit-learn.&lt;/p&gt;

&lt;p&gt;As soon as I began reading the code, I realized that there was no category-value map as I had assumed. Instead, the implementation used two separate lists—one for categories and another for values. It appears that this &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html&#34;&gt;transformer&lt;/a&gt; is optimized for transforming large datasets, where the number of rows significantly exceeds the number of categories.&lt;/p&gt;

&lt;p&gt;Within the library’s internals, a vector the size of the category set is generated as a mask to determine which category should be applied. This means that each time a transformation is performed, a large mask must be created, applied, and used to select the corresponding value. While this approach might seem inefficient at first, it is highly effective for large datasets. Instead of iterating through every row individually, a much smaller category-based mask is applied across the entire dataset, significantly improving efficiency in such scenarios.&lt;/p&gt;

&lt;p&gt;However, this is not an ideal solution for a web endpoint, where each request processes a single row. In this scenario, the number of categories far exceeds the number of rows transformed per API request, resulting in unnecessary computational overhead.&lt;/p&gt;

&lt;p&gt;To address this issue, I developed the following function to patch the TargetEncoder when used in the API. Notice that I created a category/value cache and used it to define the value of each cell, which would make this algorithm O(NxM). However, since the API processes only one row and one column per request in this TargetEncoder, there is no need to iterate through rows and columns, effectively reducing the algorithm&amp;rsquo;s complexity to O(1).&lt;/p&gt;

&lt;pre&gt;
from sklearn.preprocessing import TargetEncoder
import numpy as np


def patch_target_encoder_improving_performance():
    TargetEncoder.transform_old = TargetEncoder.transform
    TargetEncoder.transform = new_target_encoder_transform


def new_target_encoder_transform(self, *args, **kwargs):
    if &#34;_cache_cat&#34; not in self.__dict__:
        self._cache_cat = {v: i for i, v in enumerate(self.categories_[0])}
    X = args[0]
    X_out = np.empty_like(X, dtype=np.float64)
    for row_i, row in enumerate(X):
        for col_i, category in enumerate(row):
            index = self._cache_cat.get(category)
            if index is None:
                X_out[row_i, col_i] = self.target_mean_
            else:
                X_out[row_i, col_i] = self.encodings_[0][index]
    return X_out
&lt;/pre&gt;

&lt;p&gt;After deploying this improvement, we were able to match the response time of the new model with the previous one as you can see in the graph below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/target-encoder-performance.png&#34; alt=&#34;/assets/images/mab.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;That said, a warning remains: this is not a fully secure solution. A future update to scikit-learn could modify its internal attributes, which this patch relies on, potentially altering its behavior or even causing silent errors. To mitigate this risk, we have implemented tests in the API to verify the expected behavior across various scenarios.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update (2025-09-15)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recently I needed to apply TargetEncoder to multiple columns within the same instance. To handle that, I updated the code:&lt;/p&gt;

&lt;pre&gt;
def new_target_encoder_transform(self, *args, **kwargs):
    if &#34;_cache_cat&#34; not in self.__dict__:
        self._cache_cat = []
        for categories in self.categories_:
            self._cache_cat.append({v: i for i, v in enumerate(categories)})

    X = args[0]
    X_out = np.empty_like(X, dtype=np.float64)

    for row_i, row in X.iterrows():
        for col_i, category in enumerate(row):
            index = self._cache_cat[col_i].get(category)
            if index is None:
                X_out[row_i, col_i] = self.target_mean_
            else:
                X_out[row_i, col_i] = self.encodings_[col_i][index]
    return X_out
&lt;/pre&gt;

&lt;p&gt;It still missing many functionalities of the TargetEncoder like the &amp;ldquo;unfrequent&amp;rdquo; parameter configurations. But works for me.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Resources that help me to learn data science</title>
            <link rel="alternate" href="/work/general/articles/resources-help-learn-data-science/" type="text/html" />
            <id>/work/general/articles/resources-help-learn-data-science/</id>
            <updated>2024-11-30T10:56:00Z</updated>

            <summary type="html">In this article, I list and comment on books and courses that helped me learn the discipline of data science as a developer.</summary>
            <content type="html">&lt;p&gt;In this article, I list and comment on books and courses that helped me learn the discipline of data science as a developer.&lt;/p&gt;

&lt;p&gt;About 10 years ago, I started studying Machine Learning casually and purely out of curiosity. Over time, I began working with data science in various roles across different companies. Most of my learning came through hands-on experience in projects that challenged my limited knowledge, forcing me to study further, ask questions to dozens of professionals, read articles, attend conferences, and, most importantly, experiment and learn from mistakes.&lt;/p&gt;

&lt;p&gt;In this journey, I participated in modeling systems for &lt;a href=&#34;/archives/recommendation/&#34;&gt;recommendation&lt;/a&gt;, churn identification, advertising segmentation, &lt;a href=&#34;/archives/trustandsafety/&#34;&gt;fraud prevention&lt;/a&gt;, &lt;a href=&#34;/archives/search/&#34;&gt;search&lt;/a&gt;, judicial decision prediction, and many other auxiliary classifications. This extensive experience, which demanded continuous learning, shaped me into what is now called a hybrid developer—someone who is both a software engineer and a data scientist.&lt;/p&gt;

&lt;p&gt;This brief introduction about myself is meant to provide a clearer response to the developers who often ask me how to become a hybrid developer like me. The answer is not straightforward, as my experience has been quite erratic. There was no clear learning path. As I’ve explained, most of my learning was unstructured and driven by business needs.&lt;/p&gt;

&lt;p&gt;However, there are indeed some courses and books that helped me along the way to hybridizing my knowledge. Without further ado, here are the main courses that helped me:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-introduction&#34;&gt;Machine Learning&lt;/a&gt; by Stanford: This was the first course I took, and it helped me understand the entire theory behind models by implementing algorithms that are now abstracted by various libraries. Without the intuitive knowledge of these algorithms, I believe using these abstractions can lead to false results.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/recommender-systems&#34;&gt;Introduction to Recommender Systems&lt;/a&gt; by the University of Minnesota: My first hybrid role in development and data science was working on the &lt;a href=&#34;/archives/recommendation/&#34;&gt;recommendation system&lt;/a&gt; at &lt;a href=&#34;/archives/globocom/&#34;&gt;Globo.com&lt;/a&gt;. I studied this course to understand this field, and I consider it a must for anyone working with recommendation systems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/game-theory-1&#34;&gt;Game Theory&lt;/a&gt; by Stanford: I took this course purely out of curiosity and don’t regret it. The mathematical knowledge I gained frequently helps me think probabilistically and even design proper incentives and disincentives for features that go beyond data science.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/projects/statistics-data-science&#34;&gt;Statistics for Data Science&lt;/a&gt; by Project Network: I enrolled in this course to deepen my statistical knowledge, which is essential for better understanding model results, research evaluations, and analyses. I constantly apply what I learned here.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/account/accomplishments/specialization/F8SQK79VWVJ2&#34;&gt;Mathematics for Machine Learning&lt;/a&gt; by Imperial College: I took this course to deepen my understanding of the mathematics behind the models I had been using for some time. It was particularly important for understanding the workings of deep learning models and recent LLMs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Equally important, below is a list of the most significant books in my data science journey:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vwang0/recommender_system/blob/master/Recommender%20Systems%20Handbook.pdf&#34;&gt;Recommender System Handbook&lt;/a&gt;: To delve deeper into the concepts of recommendation systems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/relevant-search&#34;&gt;Relevant Search&lt;/a&gt;: A prerequisite for anyone working with search systems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rj.olx.com.br/rio-de-janeiro-e-regiao/livros-e-revistas/livro-generative-deep-learning-de-david-foster-1018413877?&#34;&gt;Generative Deep Learning&lt;/a&gt;: Essential for understanding the inner workings of new generative algorithm families.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com.br/Small-Data-Indicam-Grandes-Tend%C3%AAncias/dp/8569809719&#34;&gt;Small Data&lt;/a&gt;: Offers an opposing perspective on relying solely on big data for decision-making, showing how anecdotal examples can provide game-changing insights.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com.br/gp/product/B077Q3VZFR/&#34;&gt;Natural Language Processing with TensorFlow&lt;/a&gt;: Although I prefer PyTorch, this book is very interesting as it explains various text processing methods for machine learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope this list of books and courses helps you dive deeper into a new field, whether to work directly in it, as I do, or to simply bridge the communication gap that a lack of knowledge might cause when discussing with specialists.&lt;/p&gt;

&lt;p&gt;But here&amp;rsquo;s a warning: These courses and books were essential, but what truly made me learn was practice and experience. Experiments, mistakes, and exchanges of ideas with the excellent professionals I had the privilege of encountering throughout my life were the real drivers of my knowledge.&lt;/p&gt;

&lt;p&gt;And of course, remember: when starting to study, you may fall prey to the &lt;a href=&#34;https://thedecisionlab.com/biases/dunning-kruger-effect&#34;&gt;Dunning-Kruger Effect&lt;/a&gt;. It happened to me, and it can happen to you too, so be humble.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>No one cared about Multi Armed Bandit’s tech</title>
            <link rel="alternate" href="/work/globocom/recommendation/no-one-cared-about-multi-armed-bandit-tech/" type="text/html" />
            <id>/work/globocom/recommendation/no-one-cared-about-multi-armed-bandit-tech/</id>
            <updated>2024-09-28T10:56:00Z</updated>

            <summary type="html">In mid-2017, inspired by the book &amp;ldquo;Bandit Algorithms for Website Optimization&amp;rdquo; by John Myles White, I decided to implement a service allowing Globo.com products to use Multi Armed Bandit algorithms to optimize their usability.</summary>
            <content type="html">&lt;p&gt;In mid-2017, inspired by the book &amp;ldquo;Bandit Algorithms for Website Optimization&amp;rdquo; by John Myles White, I decided to implement a service allowing Globo.com products to use Multi Armed Bandit algorithms to optimize their usability.&lt;/p&gt;

&lt;p&gt;The algorithms explained in the book are relatively simple. Epsilon-Greedy, Softmax, and UCB are simple statistical calculations that do not require a great processing power or historical storage. So technically it would be possible to aggregate the necessary information in the memory of the service itself, avoiding any IO, enabling the service to have very low latency.&lt;/p&gt;

&lt;p&gt;To keep everything in memory and maintain consistency, a first alternative would be to handle all requests on a single server scaled vertically. We know how dangerous this is in terms of resilience. So, a second alternative would be to have multiple servers behind a load balancer, reducing the consistency of aggregations slightly, and relying on the fact that the site&amp;rsquo;s huge audience will bring the numbers recorded on each server closer together.&lt;/p&gt;

&lt;p&gt;These two possible alternatives would also require that the servers be pre-scaled for the most critical peak moments of the portal. At dawn, for example, when there were very few accesses, we would continue with the same cost as the periods of the most absurd traffic. Something we do not desire.&lt;/p&gt;

&lt;p&gt;The ideal alternative would be to try to use concepts like autoscaling and multiple containers, which are already default decisions in the deployment of new services today. At Globo.com, we already had an internal PaaS called &lt;a href=&#34;https://tsuru.io/&#34;&gt;Tsuru&lt;/a&gt;, which is open source, and I have not seen anything better in any other company to this day.&lt;/p&gt;

&lt;p&gt;However, for a service that is not stateless, the autoscaling process can be painful.&lt;/p&gt;

&lt;p&gt;Imagine the situation where we have 20 pods in the air serving requests, all of them already have their aggregated numbers stabilized and generating the best results for each experiment. Then a peak time starts, new pods are allocated, and we go to 30 pods. At this peak moment, a very important moment for the site, 33% of requests no longer have the most optimized version in terms of results.&lt;/p&gt;

&lt;p&gt;In other words, the regret metric we aim to minimize does not get reduced to the desired extent in the case of a stateful service with autoscaling.&lt;/p&gt;

&lt;p&gt;The solution found was to create a periodic sync of each service instance with a common Redis. So, in addition to storing in the service&amp;rsquo;s memory the total number of events and the total number of successes for each experiment, we also started storing in the service&amp;rsquo;s memory the values found in the common memory represented by Redis. In this sync, the service increment Redis keys with the difference between the current state and what was in the last sync, updating the common memory. After this, the sync gets those keys from Redis updating its internal states. Always using atomic variables.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/mab.png&#34; alt=&#34;/assets/images/mab.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;With this, when a new pod is created due to auto scaling, we start it reading those keys from redis, so it is born with the necessary aggregated data for a more optimal choice that reduces regret for each experiment.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s been over seven years and I know that Multi Armed Bandit is still used at Globo.com because it is possible to observe the service requests when accessing the Portal. However, I believe that the service may have been completely modified by now. It was initially created in single day during a hackday, written using Scala and Finagle, and after that was used for various optimizations at Globo.com, from choosing a photo to selecting the best position to place advertising.&lt;/p&gt;

&lt;p&gt;When I presented it at hackday, I wanted to emphasize this technological solution involving low latency and scalable eventual consistency for its operation as the interesting part of the project. In my mind, the calculations were so simple that they didn&amp;rsquo;t even deserve much attention because it is very simple to replicate them either all in memory, or by spending latency using IO massively.&lt;/p&gt;

&lt;p&gt;However, the technological aspect of this solution did not receive the attention that I thought it deserved. What most sparked interest, obviously, were the expectations of use for improving usability that such a project could bring to users and all its possibilities. In other words, no one cares about the technological solution explicitly, but implicitly, they were crucial for maximizing results, minimizing regret in periods of auto scaling, reducing costs, and reducing latency.&lt;/p&gt;

&lt;p&gt;I like to believe, in the depths of my being, that without this technological solution, the project would not have been used by so many products with as much success as it was. But is that really true?&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Delaying the launch of a model inspired by Turing</title>
            <link rel="alternate" href="/work/olxbrasil/trustandsafety/delaying-launch-model-inspired-by-turing/" type="text/html" />
            <id>/work/olxbrasil/trustandsafety/delaying-launch-model-inspired-by-turing/</id>
            <updated>2024-08-03T10:56:00Z</updated>

            <summary type="html">This is the story of how we built a machine learning model factory, and inspired by the movie &amp;ldquo;The Imitation Game,&amp;rdquo; we decided not to launch it until another change was made. This second change had such a positive outcome that the factory achieved a much lower result than we had originally programmed, but without it, we would not have completed the journey. This case occurred between the first and second quarter of 2023.</summary>
            <content type="html">&lt;p&gt;This is the story of how we built a machine learning model factory, and inspired by the movie &amp;ldquo;The Imitation Game,&amp;rdquo; we decided not to launch it until another change was made. This second change had such a positive outcome that the factory achieved a much lower result than we had originally programmed, but without it, we would not have completed the journey. This case occurred between the first and second quarter of 2023.&lt;/p&gt;

&lt;p&gt;The issue we needed to resolve was the high number of ads for prohibited items being posted on the OLX Brazil platform. Prohibited items are products whose sale is banned, controlled, requires special permission, or are the result of piracy. Examples of prohibited items would be weapons, gas cylinders, wild animals, counterfeit brand clothing, streaming accounts, among others.&lt;/p&gt;

&lt;p&gt;At that time, there was already a structure in place to prevent the entry of such ads through a huge set of regexes that were constantly being incremented. These regexes moved the ads to a manual review queue, and a team of operators analyzed each ad identified by those regexes. The work of these operators was increasing day by day, and we needed to do something to reduce this manual verification.&lt;/p&gt;

&lt;p&gt;The idea was to create a machine learning model to identify a certain percentage of ads with a high degree of certainty to automatically approve or reject it. This idea came from a hackday held by the squad to evaluate if for some group of regexes it would be possible to make a simple model that identified a good percentage with high certainty (&amp;gt;=99% precision).&lt;/p&gt;

&lt;p&gt;A group of regex would be a list of regex that already existed in the old process and that identify a type of prohibited item. We had years of history in the datalake of regexes that matched and whether the ad in question had been approved or rejected by the operators. As the model made on hackday worked, we decided to implement a POC with the evaluated group of regexes.&lt;/p&gt;

&lt;p&gt;In the middle of the implementation, we realized the opportunity, if we left the model code generic enough and with good guardrails for threshold verification to obtain high precision in cross-validation, we could make a &amp;ldquo;factory&amp;rdquo; of models.&lt;/p&gt;

&lt;p&gt;We then created a command line that receive as parameter a group of regexes. The command downloads and prepares the dataset separating it into training and testing, optimizes the model&amp;rsquo;s hyperparameters using cross-validation on the training dataset trying to reach thresholds with high precision &amp;gt;=99% for refusal and for acceptance. Then it is validated on the test dataset if these same thresholds would achieve the same precision. If yes, the model is used, if not, the model is discarded for this group of regexes.&lt;/p&gt;

&lt;p&gt;With this process, we quickly generated 26 models, with an estimate of reducing 5% of the operators&amp;#39; work due to the automatic decision. We put the model into production but without really making the decision, just to observe if the good precision result in offline evaluations would hold up in production, and it was also a success.&lt;/p&gt;

&lt;p&gt;Our drama then began when we realized that if we really let the models make decisions, users would quickly learn how to escape, and the labor savings of operators would not be consummated. This is because the operator&amp;rsquo;s decision tends to be slower, and less deterministic, than that of the model. This prevents the user from testing ways to circumvent. Quick feedback of refusal or approval could cause even more work for the operators.&lt;/p&gt;

&lt;p&gt;In the meetings where we talked about this, with all that pressure to put it into production soon, we cited the movie &amp;ldquo;The Imitation Game,&amp;rdquo; where Turing mentioned a similar strategy. The &amp;ldquo;enemy&amp;rdquo; could not perceive what was happening. A simple idea would be to cause a delay in the model&amp;rsquo;s response. But another idea was hotter and more promising in our minds, something we had been ventilating for a long time: The temporary blocking of the user from posting ads according to the number of &amp;ldquo;fouls&amp;rdquo; he committed, with possible progression.&lt;/p&gt;

&lt;p&gt;The complexity of this change came from the fact that we should create communications explaining what was happening, both for the moment of the event and for the moment of trying to post an ad (or login). Besides the analysis to define the ideal time between one attempt and another, how much ideal blocking time, as well as the natural work of the task, for state storage and expiration of the time window of attempts and interventions in the necessary places to prevent the temporarily blocked user from acting.&lt;/p&gt;

&lt;p&gt;At this moment when we were determined to bet on this solution, inspired by Turing, I was already leaving the squad to a mission in the search squad. I participated only in the debates so that this would be the solution adopted before enabling the prohibited items model, and also the initial architecture discussions. The squad only had top developers, so they quickly put the solution into production and we had an incredible result.&lt;/p&gt;

&lt;p&gt;The solution brought a &lt;strong&gt;48%&lt;/strong&gt; reduction in the index of prohibited items, which greatly reduced the work of the operators for this specific problem, and consequently made the model of prohibited items, later turned on, have a lower result than we initially expected: Still making decisions in &lt;strong&gt;~5%&lt;/strong&gt; of the cases, but from many fewer cases. Interestingly, the precision of the model, one year later, continues at &lt;strong&gt;~99%&lt;/strong&gt;, probably due to the strategy that does not let the user adapt.&lt;/p&gt;

&lt;p&gt;The people envolved in the development of both solutions were: &lt;a href=&#34;https://www.linkedin.com/in/timotta/&#34;&gt;me&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/let%C3%ADciagarcez/&#34;&gt;Leticia Garcez&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/robson-rocha-512a5b16/&#34;&gt;Robson Rocha&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/carvalho-lucas/&#34;&gt;Lucas Carvalho&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/victorseidl/&#34;&gt;Victor Lyra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/leonardopereiracampos/&#34;&gt;Leonardo Campos&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/geusandre/&#34;&gt;Andre de Geus&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nataliakarmierczak&#34;&gt;Natalia Maria Karmierczak&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/daniel-freitas-7487b5136/&#34;&gt;Daniel Freitas&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/debora-algamis-jansen-273187106/&#34;&gt;Debora Jansem&lt;/a&gt; e &lt;a href=&#34;https://www.linkedin.com/in/deborah-cordeiro-568298121/&#34;&gt;Débora Cordeiro&lt;/a&gt;. An amazing group of people.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/turing.png&#34; alt=&#34;/assets/images/turing.png&#34;&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Podium: The algorithm that defeated BM25</title>
            <link rel="alternate" href="/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/" type="text/html" />
            <id>/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/</id>
            <updated>2024-06-08T10:56:00Z</updated>

            <summary type="html">This article explains Podium, a search relevance algorithm created by the OLX Brazil Search team to replace the globally renowned BM25. It resulted in increased clicks, contact intentions, and reduced complaints about search quality, and was launched in April 2021.</summary>
            <content type="html">&lt;p&gt;This article explains Podium, a search relevance algorithm created by the OLX Brazil Search team to replace the globally renowned BM25. It resulted in increased clicks, contact intentions, and reduced complaints about search quality, and was launched in April 2021.&lt;/p&gt;

&lt;p&gt;In previous years, our search team at OLX has tried various approaches to optimize search and bring more accurate results. With each change, there was an observed improvement in short-term AB testing, but in the medium term, advertisers adapted to increase impressions of their ads by exploiting algorithm loopholes, consequently worsening the accuracy of the results.&lt;/p&gt;

&lt;p&gt;To make this clearer, I will describe below two examples of changes that were made and caused harmful reactions from advertisers:&lt;/p&gt;

&lt;p&gt;Example 1: At some point, we began to give more importance to the brand and model attributes configured by the advertiser. Therefore, an advertisement for a cellphone marked with the brand &amp;ldquo;Samsung&amp;rdquo; and the model &amp;ldquo;Galaxy&amp;rdquo; would have a better chance of being found in a search for &amp;ldquo;Samsung Galaxy&amp;rdquo; than one that was not configured this way.&lt;/p&gt;

&lt;p&gt;In AB test, this experiment brought gains, but after the rollout, advertisers noticed, and began advertising smartphones with different brands in the most searched brands. So, his ad started to appear in two different searches. In addition, advertisers of services and jobs started placing their ads in other categories that contained the brand and model attributes to appear in searches for the service title as well as in random searches.&lt;/p&gt;

&lt;p&gt;Example 2: In another period, through a precision analysis using Bayesian optimization, we defined automatic weights for each attribute of the ad. It so happened that the title obviously had a higher score. Some months after the AB test, advertisers realized that if they repeated the most important words in the title, it would have a higher chance of being elevated to the top of the search results, because of the BM25.&lt;/p&gt;

&lt;p&gt;In this way ads like &amp;ldquo;freight freight freight&amp;rdquo; and &amp;ldquo;invicta watch invicta watch invicta watch&amp;rdquo; were always at the top of their respective searches, disturbing the lives of buyers and advertisers less focused on SEO at OLX. As a temporary solution, we penalized ads with duplicate terms and standardized the internal title used in the search. However, there was still a loophole for cases of derived words, which internally in Elastic Search were considered the same word due to Stemming, and the search result become boring with a lot of ad with the same title.&lt;/p&gt;

&lt;p&gt;These were just two examples of several of our attempts to optimize the search of OLX Brazil using BM25.&lt;/p&gt;

&lt;p&gt;However, they demonstrate that we would never achieve a good result with BM25, as the mathematical intuition behind this algorithm does not make sense for the C2C marketplace, due to two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the numerator of BM25 there is the quantity of matches of the searched words.&lt;/li&gt;
&lt;li&gt;In the denominator, one of the components is the size of the document.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That is, BM25 always favors smaller documents with the highest number of matches. In this case, if a search for &amp;ldquo;bicycle&amp;rdquo; is done on a C2C marketplace, the ads with the highest match will be those whose title is simply &amp;ldquo;bicycle&amp;rdquo;. And if the ad description is included in the calculation, the highest matches will be those with the fewest words in that description and with the most repetitions of the word. In other words, the least informative, least creative, and poorest ads in terms of information will have the best score.&lt;/p&gt;

&lt;p&gt;During this period when we were studying a way to improve search, we were in love to an &lt;a href=&#34;https://pages.cs.wisc.edu/%7Eanhai/papers/chimera-vldb14.pdf&#34;&gt;Wallmart paper&lt;/a&gt; about product classification, and based on it we created a system called Guanabara (in honor to a famous supermarket in Rio de Janeiro). One of the ideas from the paper is a hybrid system of regexes, machine learning models, and human in the loop to identify products.&lt;/p&gt;

&lt;p&gt;While we were creating the regexes for this system, we noticed that there was a pattern in the title of the ads that we could observe for portuguese language. Ignoring some irrelevant words at the beginning of each title, such as &amp;ldquo;vendo&amp;rdquo;, &amp;ldquo;novo&amp;rdquo;, &amp;ldquo;lindo&amp;rdquo; among others, the order of the words indicated the real relevance of an ad for a certain term, in portuguese.&lt;/p&gt;

&lt;p&gt;Ads like &amp;ldquo;Vendo Samsung Galaxy&amp;rdquo;, &amp;ldquo;Samsung Galaxy s10&amp;rdquo; and &amp;ldquo;Samsung galaxy novo&amp;rdquo;, should have the same relevance to each other, but a higher relevance than &amp;ldquo;Fone de Samsung galaxy&amp;rdquo; in a search for &amp;ldquo;Samsung galaxy&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;And so the idea arose: What if we gave a higher score to matches based on the position of the term in the ad and the position of the term in the searched text?&lt;/p&gt;

&lt;p&gt;Based on this idea, we created the Podium algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/podium-math.gif&#34; alt=&#34;/assets/images/podium-math.gif&#34;&gt;&lt;/p&gt;

&lt;!-- \sum_{k&lt;3}^{k=0}\sum_{i&lt;3}^{i=0} match(P_i,Q_k) * S/2^^{i+k} --&gt;

&lt;p&gt;Where &lt;strong&gt;P&lt;/strong&gt; is the list of the first three relevant words from the ad, and &lt;strong&gt;Q&lt;/strong&gt; is the list of the first three relevant words from the search made, &lt;strong&gt;S&lt;/strong&gt; is an initial higher score (currently at 200k) and &lt;strong&gt;match&lt;/strong&gt; is a function that returns 1 or 0 if the terms are equal.&lt;/p&gt;

&lt;p&gt;This is a base calculation. The other texts from the ad and the query are also used, but with lower scores and not as important for the intuition behind the algorithm. In addition, the terms in the ad and the query are deduplicated through stemming in advance.&lt;/p&gt;

&lt;p&gt;One of the most important things about this algorithm is that it purposely causes ties in ads where the accuracy between the search terms and the terms present are the same. So in the example given, a search for &amp;ldquo;Samsung Galaxy&amp;rdquo;, the ads &amp;ldquo;Vendo Samsung Galaxy&amp;rdquo;, &amp;ldquo;Samsung Galaxy s10&amp;rdquo; and &amp;ldquo;Samsung Galaxy novo&amp;rdquo; all have the same score, allowing us to reorder them by recency, which is another very important factor in a C2C marketplace.&lt;/p&gt;

&lt;p&gt;Before that, with BM25, to achieve good recency in the results, we would apply a score decay function relative to how old the ad was. This decay often caused a loss of precision, especially in cases of products with low inventory. A classic example we used to demonstrate the problem was searching for &amp;ldquo;torno mecânico&amp;rdquo; in a certain region there were only 2 &amp;ldquo;torno mecânico&amp;rdquo; ads, but they were not recent. When searching for it with BM25 plus the decay function, these ads would appear in the last results.&lt;/p&gt;

&lt;p&gt;With Podium forcing ties in similar matches, we achieved a good balance between recency but only in ads with good accuracy, without needing a decay function.&lt;/p&gt;

&lt;p&gt;With that, our qualitative &lt;a href=&#34;https://medium.com/grupoolxtech/uma-nova-m%C3%A9trica-para-calcular-relev%C3%A2ncia-de-busca-65372f154f8f&#34;&gt;R-DCG, a ranking metric we have explained in a previously post,&lt;/a&gt; increased by &lt;strong&gt;3.8%&lt;/strong&gt;. And this result was accompanied by a &lt;strong&gt;3.6%&lt;/strong&gt; increase in contact intentions and a &lt;strong&gt;2.9%&lt;/strong&gt; increase in clicks on the top 5 positions of textual searches.&lt;/p&gt;

&lt;p&gt;And most importantly, after years, advertisers have not been able to find a relevant loophole in Podium to increase their impressions and disturb OLX&amp;rsquo;s search accuracy. It is still possible to do so through tags in ad descriptions, which increases impressions in case of low inventory in specific locations, but since there was already no inventory, it is not a major concern.&lt;/p&gt;

&lt;p&gt;It is worth noting that after Podium went live, several search improvements were implemented, including the inclusion of synonyms, spellers and much more.&lt;/p&gt;

&lt;p&gt;More recently we lauched a new version of Podium, the Podium-N. In this update, we solved the issue of ads that should have the same relevance but didn&amp;rsquo;t with the original Podium. Example: &amp;ldquo;Vendo samsung galaxy&amp;rdquo; and &amp;ldquo;Vendo galaxy da samsung&amp;rdquo; for the search &amp;ldquo;Galaxy&amp;rdquo;. But that&amp;rsquo;s for another post.&lt;/p&gt;

&lt;p&gt;In addition, we are finalizing the integration of semantic search together with Podiun-N, in a hybrid search, which is already showing great results. But that will also be discussed in a future post.&lt;/p&gt;

&lt;p&gt;Podium was a collaborative solution made by &lt;a href=&#34;https://www.linkedin.com/in/timotta/&#34;&gt;me&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/leonardowajnsztok/&#34;&gt;Leonardo Wajnsztok&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/filipecasal/&#34;&gt;Filipe Casal&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/daniel-correa-araujo/&#34;&gt;Daniel Araújo&lt;/a&gt;. The engineering manager at the time was &lt;a href=&#34;https://www.linkedin.com/in/rogeriofrodrigues/&#34;&gt;Rogério Rodrigues&lt;/a&gt; followed by &lt;a href=&#34;https://www.linkedin.com/in/gumaestri/&#34;&gt;Gustavo Maestri&lt;/a&gt; and the product manager and who gave the name for the child was &lt;a href=&#34;https://www.linkedin.com/in/pedrotangari/&#34;&gt;Pedro Tangari&lt;/a&gt;. Please accept my apologies if I inadvertently leave anyone out. It happened three years ago&amp;hellip;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Random recommendation due to concurrency</title>
            <link rel="alternate" href="/work/globocom/recommendation/random-recommendation-due-to-concurrency/" type="text/html" />
            <id>/work/globocom/recommendation/random-recommendation-due-to-concurrency/</id>
            <updated>2024-03-31T10:56:00Z</updated>

            <summary type="html">This is the story of how a simple mitigation solution led us to discover the root cause of a difficult-to-replicate bug, originating from a lack of knowledge about a technology.</summary>
            <content type="html">&lt;p&gt;This is the story of how a simple mitigation solution led us to discover the root cause of a difficult-to-replicate bug, originating from a lack of knowledge about a technology.&lt;/p&gt;

&lt;p&gt;When I started in the Recommendation System team at Globo.com in 2014, there was already a system in production that was used to recommend other articles on the side of the article screen. It was initially used on only one of Globo.com&amp;rsquo;s sites and was about to be used on another one as well.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t exactly remember which sites they were, but let&amp;rsquo;s say initially it was G1, a news website, and it was about to be adopted on GE, a sports website. It was at this moment of adoption that we started receiving sporadic complaints about GE articles being recommended on G1 and vice versa. This was a huge problem, there couldn&amp;rsquo;t be cross-recommendation between products at that moment, and the fact that this bug was occurring was a risk to the continuity of the recommendation systems project.&lt;/p&gt;

&lt;p&gt;However, we were unable to consistently replicate the issue. Every now and then, someone on the team would be able to witness it, proving that yes, it was not just a hallucination.&lt;/p&gt;

&lt;p&gt;To quickly mitigate the issue, we thought of a simple solution, but before explaining this solution, it is important to understand how the first version of recommendation on Globo.com worked.&lt;/p&gt;

&lt;p&gt;There was an API written in Ruby, which in the request made by the client of the article, searched in a Mongo DB for the IDs of articles recommended to a user for a specific website. This personalized recommendation was generated in a periodic batch using Mahout on Hadoop. These article IDs were sent to a Solr Handler along with the ID of the context article where the recommendation would be presented. In this Solr handler, a More Like This query was executed to find similar articles to the context article, and these two lists of articles were then combined for the recommendation return. The diagram below demonstrates how each system interacts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/recommendation-old.png&#34; alt=&#34;/assets/images/recommendation-old.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;In other words, it was a mixture of personalized recommendation through collaborative filtering calculated by Mahout and contextual recommendation by similarity calculated by Solr. And the customized Solr Handler combined both lists.&lt;/p&gt;

&lt;p&gt;Looking at the Solr Handler code, there didn&amp;rsquo;t seem to be any issue with the More Like This query that could cause a wrong return of articles from another site. Also, in the client we didn&amp;rsquo;t find any sign of a problem with sending the site ID incorrectly. In the Ruby API, also quite simple, there didn&amp;rsquo;t seem to be any issues with querying the Mongo DB with the site ID or passing this ID to the Solr Handler.&lt;/p&gt;

&lt;p&gt;Then we were left with the calculation of the personalized recommendation, whose code was a little more complex to understand, involving several codes in PIG and some Map Reduces for Mahout use. But it didn&amp;rsquo;t make sense for that to be the problem, as the execution of this batch process took about two hours, and when the sporadic case occurred, we would look at the key related to the user/site on the Mongo DB and not find this inconsistency.&lt;/p&gt;

&lt;p&gt;Without any tricks up our sleeve, we decided to implement the simple solution I mentioned: Strengthening the filtering in the Solr Handler so that after combining the two algorithms, no content from another website would be returned. The idea was to stop the bleeding, and then try to better understand the root of the problem.&lt;/p&gt;

&lt;p&gt;The solution was deployed and during the deploy we paid more attention to the Solr logs and found sporadic errors in the use of the SimpleDateFormat class. This class was used in this Solr Handler to format the final result of the articles. Looking at the code, it was possible to notice a possible source for this specific problem and a clue of what could be happening to cause the original problem.&lt;/p&gt;

&lt;p&gt;SimpleDateFormat is not threadsafe, so it is ideal not to use global instances. In this Solr Handler, SimpleDateFormat was not instantiated statically, which is a good sign. However, it was instantiated as an attribute of the class. This means that each instance of the Handler would have its own SimpleDateFormat. If each request instantiated its handler, we would not have the concurrency issue.&lt;/p&gt;

&lt;p&gt;But we were having. Reading the documentation, it became clear that the Solr Handler was not generating an instance per request. So in practice, the SimpleDateFormat was being reused in several requests causing the sporadic errors we observed in the logs. Another attribute that was inadvertently in the same situation was the list of documents to be returned in the request.&lt;/p&gt;

&lt;p&gt;Gotcha! That&amp;rsquo;s when you have finally caught the source of the problem!&lt;/p&gt;

&lt;p&gt;During the request, when the combination of the return of More Like This occurred with the personalized recommendation sent via parameter, if another request occurred at the same time, the recommendations from different people and sites were combined.&lt;/p&gt;

&lt;p&gt;It was possible to replicate the problem locally by adding sleeps to force slowness and then execute simultaneous requests. The final solution was to remove these instance attributes and only use them locally in the methods.&lt;/p&gt;

&lt;p&gt;This demonstrates the importance of understanding the system in which you are working, especially when working with multi-threaded systems.&lt;/p&gt;

&lt;p&gt;This reminded me of a problem in the past also at Globo.com, but in this case caused by me: I used a HashMap as cache in a multithread service, and this caused several locks in the application, for the same reason: HashMap is not thread safe. The problem was only identified with thread dumps and the solution was to use a ConcurrentHashMap instead.&lt;/p&gt;

&lt;p&gt;Finally, the lesson is: There is no doubt that it is easier to learn by doing, but to put it into production, it is important to carefully read the specifications of the libraries, frameworks, and environments we are using. And in case of a difficult analysis problem, it is worth trying simple and potentially ineffective solutions, because that may lead you to a greater understanding of the real problem.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The rise, peak and downfall of Web Democracy</title>
            <link rel="alternate" href="/work/personal/webdemocracy/the-rise-peak-and-downfall-of-web-democracy/" type="text/html" />
            <id>/work/personal/webdemocracy/the-rise-peak-and-downfall-of-web-democracy/</id>
            <updated>2023-12-26T10:56:00Z</updated>

            <summary type="html">This is the story about the rise, peak, and downfall of a politicians&amp;#39; rating app that I created and maintained for years as a pet project, along with an explanation of why it was shut down.</summary>
            <content type="html">&lt;p&gt;This is the story about the rise, peak, and downfall of a politicians&amp;#39; rating app that I created and maintained for years as a pet project, along with an explanation of why it was shut down.&lt;/p&gt;

&lt;p&gt;It all began in 2008, when Orkut was the most used social network in Brazil and it had started to release an Open Social API for application integration. I then had the idea of creating an app where you could rate and discuss Brazilian politicians.&lt;/p&gt;

&lt;p&gt;The app was simple, a list of elected politicians and candidates with information gathered through web crawlers on the internet itself, a positive and negative evaluation, and a comments section. This would generate lists of the best, worst, and most controversial politicians.&lt;/p&gt;

&lt;p&gt;And with the open social API, it would still be possible to find friends who are most politically similar to you. I actually didn&amp;rsquo;t end up implementing this part because, as mentioned, it was a pet project and I didn&amp;rsquo;t dedicate a considerable amount of time to it.&lt;/p&gt;

&lt;p&gt;At a certain point, Orkut began to decline in Brazil with the growth of Facebook. In this other social network, there was already the possibility of creating applications, and I spent some time adapting Web Democracy to work on both, with Open Social and Facebook. However, the Facebook application API was very unstable, it changed a lot, and it did not bring as much return as the application on Orkut.&lt;/p&gt;

&lt;p&gt;So after changing the application about three times to adapt it to Facebook, I decided to extract Web Democracy from the social networks and turn it into a website. It was a good decision. The number of accesses started to grow again due to the good implemented SEO, and it remained like that for a long time until it was shut down.&lt;/p&gt;

&lt;p&gt;During this period, interesting things happened.&lt;/p&gt;

&lt;p&gt;In a radio station in the countryside of Minas Gerais, a politician was questioned about a low rating on Web Democracy, and his response was an accusation that the application had been bought by his opponents.&lt;/p&gt;

&lt;p&gt;Another politician threatened to sue me through a Facebook message if I didn&amp;rsquo;t take down his profile because it would be defamation. The interesting thing is that this politician was highly rated on the app. I explained this to him and removed him from the platform. Then he asked to come back and I refused.&lt;/p&gt;

&lt;p&gt;There was a period when leaders of a Brazilian liberal movement reached out to me to have the website ally with them. I refused because the concept of Web Democracy was to be a neutral platform without ideologies.&lt;/p&gt;

&lt;p&gt;During the time I was on the air, I was starting a recommendation systems course, so I implemented a Content-Based recommendation for the website that had a good result in increased engagement, and I even released the data to the recommendation systems community: &lt;a href=&#34;http://programandosemcafeina.blogspot.com/2015/11/dados-abertos-do-web-democracia.html&#34;&gt;http://programandosemcafeina.blogspot.com/2015/11/dados-abertos-do-web-democracia.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At some point, I tried to create new features for the application, such as the ability to create petitions, promote them, and evaluate them. However, this tool did not gain much success.&lt;/p&gt;

&lt;p&gt;And then you ask me: With so much fun, why did the Web Democracy fall?&lt;/p&gt;

&lt;p&gt;Web Democracy was developed using Ruby on Rails. Since I didn&amp;rsquo;t work regularly on this project, every time I came back to develop something it was a challenge. This was because it was always necessary to update some Ruby library, whose ecosystem at the time ignored the concept of compatibility between versions.&lt;/p&gt;

&lt;p&gt;I had even implemented a very comprehensive test suite with 100% coverage, however, whenever there was an update of rspec, cucumber, rails, or any other dependency of these libraries, even the test code would fail. This caused me an incredible laziness to work on the project.&lt;/p&gt;

&lt;p&gt;Another technical difficulty was the frequent updating of politicians that needed to be done. With every new pre and post election, a new crawler had to be made because the source websites would change. It always required effort to bypass captchas, session cookies, and created bursts for protection.&lt;/p&gt;

&lt;p&gt;Not to mention that, as I wouldn&amp;rsquo;t want to lose the old assessments and comments or duplicate politicians, I needed to create a match between them. Notice that the sources of information didn&amp;rsquo;t always have the CPF (Brazilian taxpayer ID), so the match wasn&amp;rsquo;t that simple: Names changed, nicknames changed, parties changed, and even states changed.&lt;/p&gt;

&lt;p&gt;I even developed a heuristic that gave me 100% precision in the match, but with low recall. To deal with uncertain cases, I created an annotation interface for the possible politicians that would be a match and manually did it myself. It was quite a tedious task.&lt;/p&gt;

&lt;p&gt;In a certain year, which I can&amp;rsquo;t quite remember, laziness got the best of me and I stopped doing these updates, and as a result, the Web Democracy platform became outdated. Additionally, Google&amp;rsquo;s algorithm changed during this period, further deprioritizing repeated content, thus limiting the app&amp;rsquo;s access even more.&lt;/p&gt;

&lt;p&gt;With very few monthly visits, outdated and only burdening me in hosting, I decided to kill the project for good in 2022. Occasionally, I still visit the comments and suggestions page of Web Democracy with a bittersweet nostalgia for a project that brought so much joy and sadness: &lt;a href=&#34;http://programandosemcafeina.blogspot.com/2008/08/web-democracia-criticas-e-sugestoes.html&#34;&gt;http://programandosemcafeina.blogspot.com/2008/08/web-democracia-criticas-e-sugestoes.html&lt;/a&gt;.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>How I invented Ajax despite it already existing</title>
            <link rel="alternate" href="/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/" type="text/html" />
            <id>/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/</id>
            <updated>2023-11-15T10:56:00Z</updated>

            <summary type="html">This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.</summary>
            <content type="html">&lt;p&gt;This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.&lt;/p&gt;

&lt;p&gt;Between 2003 and 2004, I worked for a very small software company. There were only three developers, and each one worked separately on a group of softwares, as if they were three one-person teams. I had little experience and had recently graduated, and I also did not have the habit of reading books and articles to learn. All my learning until then was based on empirical experimentation, solving real problems from personal projects and the companies I was working for.&lt;/p&gt;

&lt;p&gt;At that time, the majority of websites and web applications would reload the entire pages every time a click or form submission occurred. And for me, that was indeed the only way to exchange information between the browser and the server. My knowledge was limited to that.&lt;/p&gt;

&lt;p&gt;However, one of the projects at Indata involved customizing a GIS project from Mexico. In this project, I noticed that information was loaded on the page without having to reload the entire page. I found that amazing, a revolution within my limited knowledge. I needed to learn that!&lt;/p&gt;

&lt;p&gt;Notice, at that time we didn&amp;rsquo;t have Chrome&amp;rsquo;s Web Developer Tools. We used Internet Explorer to debug websites. JavaScript errors were masked, there was no way to inspect anything. Additionally, the JavaScript code on the client side of this software was minified.&lt;/p&gt;

&lt;p&gt;Reading line by line of that obfuscated code I found the so-called XMLHttpRequest object where the magic was happening. But I didn&amp;rsquo;t understand anything. To me, that was the end of the execution line. There was nothing more after it.&lt;/p&gt;

&lt;p&gt;I couldn&amp;rsquo;t notice the callback that would be executed through it. It wasn&amp;rsquo;t due to a total unawareness of the pattern, as at that time I had already developed some games in Java that used listeners in response to user actions, a similar pattern, but due to my lack of practice, I couldn&amp;rsquo;t associate it.&lt;/p&gt;

&lt;p&gt;Nowadays, I try to understand why I couldn&amp;rsquo;t associate one thing with another, and my only hypothesis is that the abstraction of a callback for user action made sense to me at the time, as it is an external interruption to the code&amp;rsquo;s execution flow, while the callback within the execution flow didn&amp;rsquo;t make sense in my limited brain at that time.&lt;/p&gt;

&lt;p&gt;Also, Google was not very popular in Brazil yet, and all search tools (in Portuguese because at the time I didn&amp;rsquo;t have experience in other languages either) were directories and did not search internally within websites and texts. Neither did my colleagues at the company know that technology. So, for some time, I was stuck without understanding how that GIS software could load data from the server without reloading the page.&lt;/p&gt;

&lt;p&gt;So one day I remembered an HTML tag that I rarely used, but had learned in an online HTML and JavaScript course: the iframe. I noticed that with the iframe, I could communicate with the parent using JavaScript and exchange information. This was the solution I needed.&lt;/p&gt;

&lt;p&gt;With iframe, I would be able to retrieve information from the server and populate it in the parent through a function call, sending the data as a parameter. In fact, it was the same callback pattern used by XMLHttpRequest, but in this case, it made sense to me, as it was an interruption coming from another page being loaded.&lt;/p&gt;

&lt;p&gt;At the time, I was developing Angaturama, a receptive tourism company management system. I started using the &amp;ldquo;Ajax&amp;rdquo; technique through iframes, and as a result, the system became very user-friendly. We even created a shortcut key to display hidden iframes that were hidden by CSS containing the results for debugging purposes.&lt;/p&gt;

&lt;p&gt;A few years after I left the company, Google started to become more popular in Brazil. I learned English, developed the habit of reading books, technical articles, and participating in software discussion groups. During this period, I began to notice that there was a trending term called &amp;ldquo;Ajax&amp;rdquo;. Everywhere I looked, people were talking about it, and the infamous XMLHttpRequest. In 2006, jQuery was born and made everything even easier.&lt;/p&gt;

&lt;p&gt;When I discovered all of this, I felt like the boy from the movie &lt;a href=&#34;https://www.imdb.com/title/tt0461804/&#34;&gt;&amp;ldquo;Mongolian ping pong&amp;rdquo;&lt;/a&gt; who is sent to school and finds a court with several other boys playing ping pong, with that ball he spent months trying to figure out what it was.&lt;/p&gt;

&lt;p&gt;In a later meeting of Indata employees, I remember us laughing about this makeshift job done on the Angaturama and how, due to our lack of knowledge at the time, I became famous among them for inventing &amp;ldquo;Ajax&amp;rdquo; despite its existence already.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Collecting data to identify data collect</title>
            <link rel="alternate" href="/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/" type="text/html" />
            <id>/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/</id>
            <updated>2023-08-19T10:56:00Z</updated>

            <summary type="html">This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!</summary>
            <content type="html">&lt;p&gt;This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!&lt;/p&gt;

&lt;p&gt;It happened in mid-2022. At the time, I had already been working for a year and a half in the Trust &amp;amp; Safety area at OLX Brasil, and we had achieved significant success in reducing fraud attempts. Especially in a type of fraud known as &amp;ldquo;False Payment&amp;rdquo;. In fact, another type of fraud known as &amp;ldquo;Data Collect&amp;rdquo; has since become the highest number of reported cases.&lt;/p&gt;

&lt;p&gt;The problem with this other type of fraud is that the behavior of the fraudsters was very similar to that of another type of malicious user: the spammer. A modeling to identify this type of fraudster, done a few months earlier, failed for this reason. We were unable to separate the spammer from a fraudster of this type of fraud in any way.&lt;/p&gt;

&lt;p&gt;Then you may ask me, why not prevent both characters with the same model?&lt;/p&gt;

&lt;p&gt;The approach we had towards spammers was quite different from the approach towards a fraudster. I cannot go into detail about how these approaches because confidentiality is part of security, and making it public can assist both groups in finding ways to circumvent. But trust me, it was very important to distinguish our actions in dealing with these two groups.&lt;/p&gt;

&lt;p&gt;We were at an impasse, our last model was actually doing a good job at accurately distinguishing between legitimate users and these two groups, but we couldn&amp;rsquo;t put it into production for making automatic decisions. We were missing the ability to identify which micro behaviors set one group apart from the other, and there was only one team in the company that could help us with that: the monitoring team.&lt;/p&gt;

&lt;p&gt;This team was a multidisciplinary team of fraud identification experts in the company. They were responsible for manually reviewing samples from automated models and operational team decisions regarding reports that were not handled automatically.&lt;/p&gt;

&lt;p&gt;We spoke with this team in an attempt to find any new criteria that could help us separate the cases. The conversation proved to be a not-so-effective tool, as the developers and monitors didn&amp;rsquo;t seem to speak the same language. We couldn&amp;rsquo;t objectively map out these determining factors through these unstructured conversation.&lt;/p&gt;

&lt;p&gt;To address this, we created a spreadsheet with a large sample of cases that the not so good model had identified as fraud (and within those cases, there were many spams) and added two columns: one to indicate whether it was fraud or spam, and another for free text where the operator could explain the reasons behind their conclusion. This spreadsheet was sent to the monitoring team for completion.&lt;/p&gt;

&lt;p&gt;The result could not be more satisfactory. In the first lines, the free text column started off more &amp;ldquo;poetic&amp;rdquo; and gradually, the repetitive pattern of the groups caused the monitors to write in a more concise and even repetitive manner, generating a clear pattern on new criteria that we could adopt for the machine learning model.&lt;/p&gt;

&lt;p&gt;Our data collection had worked. Our data collection had been successful.&lt;/p&gt;

&lt;p&gt;These pieces of information generated new feature calculations for a new machine learning model training. We sent new samples of the results from this new model for monitoring evaluation, which could assure us of high accuracy. For a few weeks, we kept the artifact in production without taking any action, only logging, just to monitor if the generated result would reflect the results obtained in the offline evaluation. Everything went smoothly.&lt;/p&gt;

&lt;p&gt;In the weeks after enabling the new model to take actions against the fraudsters, we experienced an average decrease of 33% in &amp;ldquo;Data collect&amp;rdquo; frauds per week. Another type of fraud, known as &amp;ldquo;False Sale&amp;rdquo;, also saw a 25% decrease, as it relies on a more complicated social engineering involving the execution of &amp;ldquo;Data collect&amp;rdquo; fraud at first place to be effective.&lt;/p&gt;

&lt;p&gt;So we data collected to avoid data collect successfully.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The fantastic widget factory</title>
            <link rel="alternate" href="/work/globocom/webmedia/the-fantastic-widget-factory/" type="text/html" />
            <id>/work/globocom/webmedia/the-fantastic-widget-factory/</id>
            <updated>2023-04-29T10:56:00Z</updated>

            <summary type="html">This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.</summary>
            <content type="html">&lt;p&gt;This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.&lt;/p&gt;

&lt;p&gt;The case occurred in mid-2006 at Globo.com. At that time, the portal was divided into several independent sites, such as G1, GloboEsporte, Ego, among others. Generally, these sites displayed only text news, and all video assets were concentrated on another site called GMC, Globo Media Center.&lt;/p&gt;

&lt;p&gt;Our initial goal was to break down the silo in relation to video content, allowing developers from other sites to offer GMC registered videos in a simple and standardized way. At the time, an API was already available (known as WebmediaAPI), but it did not standardize the display of these content offerings, which would require additional effort from these teams for implementation.&lt;/p&gt;

&lt;p&gt;The solution we proposed would be to develop standardized visual widgets. These widgets would bring offers of various types such as recent videos, most viewed videos, best-rated videos, with the possibility of applying various filters and layout options. In this way, it would only be necessary to instantiate the widget into backend code page to offer video content in any Globo.com website.&lt;/p&gt;

&lt;p&gt;Here comes our first issue: At that time, Globo.com websites used a platform called Vignette, which made all its content static during the publication of homepages and articles. In other words, the news articles were static HTML generated after each publication, and the homepages were the same. Any changes to the page needed an editor to publish them for them to be displayed.&lt;/p&gt;

&lt;p&gt;This went against the dynamic nature of video content. How to maintain dynamically ordered content such as &amp;ldquo;most viewed&amp;rdquo; on a static HTML page? One way would be to use server-side includes, since the static files were served by Apache.&lt;/p&gt;

&lt;p&gt;Unfortunately, there were many restrictions to make any changes to the infrastructure at globo.com in 2006. To achieve this, we would need a lot of persuasion, meetings, presentations and one-on-one conversations, with no guarantee of success. We did not have the necessary time or social skills required for this goal. But we had a lot of creativity!&lt;/p&gt;

&lt;p&gt;So we decided to make these widgets on the client side. However, cross-domain requests to obtain information from WebmediaApi would only be possible if we could provide the necessary CORS headers. But to do this, changes to the infrastructure would also be necessary, as the application server (Apache) in front of WebmediaApi was also rigid, beyond our control, and exchanged all headers.&lt;/p&gt;

&lt;p&gt;Our solution was to allow WebmediaApi to return javascript call to a callback function instead of the json content. We then had a JavaScript with the widget code, which included another &amp;ldquo;JavaScript&amp;rdquo; that was the WebmediaAPI call with the name of a dynamic function as a parameter. This function was &amp;ldquo;printed&amp;rdquo; in the API response as a function call to finally render the chosen and customized widget.&lt;/p&gt;

&lt;p&gt;There was even a timeout control in case the callback took too long to respond.&lt;/p&gt;

&lt;p&gt;This solved our problem and after a few sprints, we created a true widget factory. Needs and ideas would come, and we would generate the necessary widgets and filters on WebmediaApi. It was a success. So much so, that the standardized layout of the widgets created the need to update the GMC, the aggregator site for all videos, to use the same layouts standards. Thus, Globo Vídeos was born, the successor to GMC.&lt;/p&gt;

&lt;p&gt;But there was a catch&amp;hellip;&lt;/p&gt;

&lt;p&gt;At that time, Google did not index pages rendered via JavaScript. And we needed at least Globo Videos to remain server-side rendered so that we wouldn&amp;rsquo;t lose relevance in organic searches.&lt;/p&gt;

&lt;p&gt;The solution: we started rendering javascript widgets on the server. Since everything in Vignette was java, we had to render using Rhino (if I&amp;rsquo;m not mistaken, before java 1.5 it was only possible to render javascript in the JVM through this lib).&lt;/p&gt;

&lt;p&gt;To maintain the freshness of the data, the Globo Vídeos homepage began to be rendered on the server every X period of time. I don&amp;rsquo;t remember if this automatic publication was done via cron or some Vignette tool, but the point is that static HTML began to be generated frequently, with the HTML of the widgets. However, the video pages continued to display client-side widgets as it would be impractical to render millions of HTML frequently.&lt;/p&gt;

&lt;p&gt;There was an added complexity to this solution as the process responsible for generating static HTML files on Globo.com was running on a WebLogic server. Although our server-side JavaScript rendering solution worked seamlessly in all other environments, including local, staging, and QA, it failed to function when deployed to production. After extensive debugging efforts, we discovered that the WebLogic version used in production differed from the other environments and contained an older Rhino version that superseded our newer one. To ensure proper widget rendering, we needed to execute the code by instantiating another classloader that loaded the correct Rhino jar version, which we successfully accomplished.&lt;/p&gt;

&lt;p&gt;In the end, we were able to develop a client-side widget platform that empowers websites to offer videos independently and consistently, without having to duplicate code. Furthermore, we devised a server-side rendered video product that can be easily indexed by Google. These achievements were made possible by implementing a series of three engineering workarounds.&lt;/p&gt;
</content>
        </entry>
    
</feed>
