<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
    <title>tiago.rio.br</title>
    
    <link rel="alternate" href="/" />
    <link rel="self" href="/feed.xml" type="application/atom+xml" />
    <id>/</id>
    <updated>2024-03-31T16:09:09Z</updated>

    <author>
        <name>Tiago Albineli Motta</name>
        
        <uri>/</uri>
    </author>

    
        <entry>
            <title>Random recommendation due to concurrency</title>
            <link rel="alternate" href="/work/globocom/recommendation/random-recommendation-due-to-concurrency/" type="text/html" />
            <id>/work/globocom/recommendation/random-recommendation-due-to-concurrency/</id>
            <updated>2024-03-31T16:05:00Z</updated>

            <summary type="html">This is the story of how a simple mitigation solution led us to discover the root cause of a difficult-to-replicate bug, originating from a lack of knowledge about a technology.</summary>
            <content type="html">&lt;p&gt;This is the story of how a simple mitigation solution led us to discover the root cause of a difficult-to-replicate bug, originating from a lack of knowledge about a technology.&lt;/p&gt;

&lt;p&gt;When I started in the Recommendation System team at Globo.com in 2014, there was already a system in production that was used to recommend other articles on the side of the article screen. It was initially used on only one of Globo.com&amp;rsquo;s sites and was about to be used on another one as well.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t exactly remember which sites they were, but let&amp;rsquo;s say initially it was G1, a news website, and it was about to be adopted on GE, a sports website. It was at this moment of adoption that we started receiving sporadic complaints about GE articles being recommended on G1 and vice versa. This was a huge problem, there couldn&amp;rsquo;t be cross-recommendation between products at that moment, and the fact that this bug was occurring was a risk to the continuity of the recommendation systems project.&lt;/p&gt;

&lt;p&gt;However, we were unable to consistently replicate the issue. Every now and then, someone on the team would be able to witness it, proving that yes, it was not just a hallucination.&lt;/p&gt;

&lt;p&gt;To quickly mitigate the issue, we thought of a simple solution, but before explaining this solution, it is important to understand how the first version of recommendation on Globo.com worked.&lt;/p&gt;

&lt;p&gt;There was an API written in Ruby, which in the request made by the client of the article, searched in a Mongo DB for the IDs of articles recommended to a user for a specific website. This personalized recommendation was generated in a periodic batch using Mahout on Hadoop. These article IDs were sent to a Solr Handler along with the ID of the context article where the recommendation would be presented. In this Solr handler, a More Like This query was executed to find similar articles to the context article, and these two lists of articles were then combined for the recommendation return. The diagram below demonstrates how each system interacts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/recommendation-old.png&#34; alt=&#34;/assets/images/recommendation-old.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;In other words, it was a mixture of personalized recommendation through collaborative filtering calculated by Mahout and contextual recommendation by similarity calculated by Solr. And the customized Solr Handler combined both lists.&lt;/p&gt;

&lt;p&gt;Looking at the Solr Handler code, there didn&amp;rsquo;t seem to be any issue with the More Like This query that could cause a wrong return of articles from another site. Also, in the client we didn&amp;rsquo;t find any sign of a problem with sending the site ID incorrectly. In the Ruby API, also quite simple, there didn&amp;rsquo;t seem to be any issues with querying the Mongo DB with the site ID or passing this ID to the Solr Handler.&lt;/p&gt;

&lt;p&gt;Then we were left with the calculation of the personalized recommendation, whose code was a little more complex to understand, involving several codes in PIG and some Map Reduces for Mahout use. But it didn&amp;rsquo;t make sense for that to be the problem, as the execution of this batch process took about two hours, and when the sporadic case occurred, we would look at the key related to the user/site on the Mongo DB and not find this inconsistency.&lt;/p&gt;

&lt;p&gt;Without any tricks up our sleeve, we decided to implement the simple solution I mentioned: Strengthening the filtering in the Solr Handler so that after combining the two algorithms, no content from another website would be returned. The idea was to stop the bleeding, and then try to better understand the root of the problem.&lt;/p&gt;

&lt;p&gt;The solution was deployed and during the deploy we paid more attention to the Solr logs and found sporadic errors in the use of the SimpleDateFormat class. This class was used in this Solr Handler to format the final result of the articles. Looking at the code, it was possible to notice a possible source for this specific problem and a clue of what could be happening to cause the original problem.&lt;/p&gt;

&lt;p&gt;SimpleDateFormat is not threadsafe, so it is ideal not to use global instances. In this Solr Handler, SimpleDateFormat was not instantiated statically, which is a good sign. However, it was instantiated as an attribute of the class. This means that each instance of the Handler would have its own SimpleDateFormat. If each request instantiated its handler, we would not have the concurrency issue.&lt;/p&gt;

&lt;p&gt;But we were having. Reading the documentation, it became clear that the Solr Handler was not generating an instance per request. So in practice, the SimpleDateFormat was being reused in several requests causing the sporadic errors we observed in the logs. Another attribute that was inadvertently in the same situation was the list of documents to be returned in the request.&lt;/p&gt;

&lt;p&gt;Gotcha! That&amp;rsquo;s when you have finally caught the source of the problem!&lt;/p&gt;

&lt;p&gt;During the request, when the combination of the return of More Like This occurred with the personalized recommendation sent via parameter, if another request occurred at the same time, the recommendations from different people and sites were combined.&lt;/p&gt;

&lt;p&gt;It was possible to replicate the problem locally by adding sleeps to force slowness and then execute simultaneous requests. The final solution was to remove these instance attributes and only use them locally in the methods.&lt;/p&gt;

&lt;p&gt;This demonstrates the importance of understanding the system in which you are working, especially when working with multi-threaded systems.&lt;/p&gt;

&lt;p&gt;This reminded me of a problem in the past also at Globo.com, but in this case caused by me: I used a HashMap as cache in a multithread service, and this caused several locks in the application, for the same reason: HashMap is not thread safe. The problem was only identified with thread dumps and the solution was to use a ConcurrentHashMap instead.&lt;/p&gt;

&lt;p&gt;Finally, the lesson is: There is no doubt that it is easier to learn by doing, but to put it into production, it is important to carefully read the specifications of the libraries, frameworks, and environments we are using. And in case of a difficult analysis problem, it is worth trying simple and potentially ineffective solutions, but that may lead you to a greater understanding of the real problem.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The rise, peak and downfall of Web Democracy</title>
            <link rel="alternate" href="/work/personal/webdemocracy/the-rise-peak-and-downfall-of-web-democracy/" type="text/html" />
            <id>/work/personal/webdemocracy/the-rise-peak-and-downfall-of-web-democracy/</id>
            <updated>2023-12-26T15:48:00Z</updated>

            <summary type="html">This is the story about the rise, peak, and downfall of a politicians&amp;#39; rating app that I created and maintained for years as a pet project, along with an explanation of why it was shut down.</summary>
            <content type="html">&lt;p&gt;This is the story about the rise, peak, and downfall of a politicians&amp;#39; rating app that I created and maintained for years as a pet project, along with an explanation of why it was shut down.&lt;/p&gt;

&lt;p&gt;It all began in 2008, when Orkut was the most used social network in Brazil and it had started to release an Open Social API for application integration. I then had the idea of creating an app where you could rate and discuss Brazilian politicians.&lt;/p&gt;

&lt;p&gt;The app was simple, a list of elected politicians and candidates with information gathered through web crawlers on the internet itself, a positive and negative evaluation, and a comments section. This would generate lists of the best, worst, and most controversial politicians.&lt;/p&gt;

&lt;p&gt;And with the open social API, it would still be possible to find friends who are most politically similar to you. I actually didn&amp;rsquo;t end up implementing this part because, as mentioned, it was a pet project and I didn&amp;rsquo;t dedicate a considerable amount of time to it.&lt;/p&gt;

&lt;p&gt;At a certain point, Orkut began to decline in Brazil with the growth of Facebook. In this other social network, there was already the possibility of creating applications, and I spent some time adapting Web Democracy to work on both, with Open Social and Facebook. However, the Facebook application API was very unstable, it changed a lot, and it did not bring as much return as the application on Orkut.&lt;/p&gt;

&lt;p&gt;So after changing the application about three times to adapt it to Facebook, I decided to extract Web Democracy from the social networks and turn it into a website. It was a good decision. The number of accesses started to grow again due to the good implemented SEO, and it remained like that for a long time until it was shut down.&lt;/p&gt;

&lt;p&gt;During this period, interesting things happened.&lt;/p&gt;

&lt;p&gt;In a radio station in the countryside of Minas Gerais, a politician was questioned about a low rating on Web Democracy, and his response was an accusation that the application had been bought by his opponents.&lt;/p&gt;

&lt;p&gt;Another politician threatened to sue me through a Facebook message if I didn&amp;rsquo;t take down his profile because it would be defamation. The interesting thing is that this politician was highly rated on the app. I explained this to him and removed him from the platform. Then he asked to come back and I refused.&lt;/p&gt;

&lt;p&gt;There was a period when leaders of a Brazilian liberal movement reached out to me to have the website ally with them. I refused because the concept of Web Democracy was to be a neutral platform without ideologies.&lt;/p&gt;

&lt;p&gt;During the time I was on the air, I was starting a recommendation systems course, so I implemented a Content-Based recommendation for the website that had a good result in increased engagement, and I even released the data to the recommendation systems community: &lt;a href=&#34;http://programandosemcafeina.blogspot.com/2015/11/dados-abertos-do-web-democracia.html&#34;&gt;http://programandosemcafeina.blogspot.com/2015/11/dados-abertos-do-web-democracia.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At some point, I tried to create new features for the application, such as the ability to create petitions, promote them, and evaluate them. However, this tool did not gain much success.&lt;/p&gt;

&lt;p&gt;And then you ask me: With so much fun, why did the Web Democracy fall?&lt;/p&gt;

&lt;p&gt;Web Democracy was developed using Ruby on Rails. Since I didn&amp;rsquo;t work regularly on this project, every time I came back to develop something it was a challenge. This was because it was always necessary to update some Ruby library, whose ecosystem at the time ignored the concept of compatibility between versions.&lt;/p&gt;

&lt;p&gt;I had even implemented a very comprehensive test suite with 100% coverage, however, whenever there was an update of rspec, cucumber, rails, or any other dependency of these libraries, even the test code would fail. This caused me an incredible laziness to work on the project.&lt;/p&gt;

&lt;p&gt;Another technical difficulty was the frequent updating of politicians that needed to be done. With every new pre and post election, a new crawler had to be made because the source websites would change. It always required effort to bypass captchas, session cookies, and created bursts for protection.&lt;/p&gt;

&lt;p&gt;Not to mention that, as I wouldn&amp;rsquo;t want to lose the old assessments and comments or duplicate politicians, I needed to create a match between them. Notice that the sources of information didn&amp;rsquo;t always have the CPF (Brazilian taxpayer ID), so the match wasn&amp;rsquo;t that simple: Names changed, nicknames changed, parties changed, and even states changed.&lt;/p&gt;

&lt;p&gt;I even developed a heuristic that gave me 100% precision in the match, but with low recall. To deal with uncertain cases, I created an annotation interface for the possible politicians that would be a match and manually did it myself. It was quite a tedious task.&lt;/p&gt;

&lt;p&gt;In a certain year, which I can&amp;rsquo;t quite remember, laziness got the best of me and I stopped doing these updates, and as a result, the Web Democracy platform became outdated. Additionally, Google&amp;rsquo;s algorithm changed during this period, further deprioritizing repeated content, thus limiting the app&amp;rsquo;s access even more.&lt;/p&gt;

&lt;p&gt;With very few monthly visits, outdated and only burdening me in hosting, I decided to kill the project for good in 2022. Occasionally, I still visit the comments and suggestions page of Web Democracy with a bittersweet nostalgia for a project that brought so much joy and sadness: &lt;a href=&#34;http://programandosemcafeina.blogspot.com/2008/08/web-democracia-criticas-e-sugestoes.html&#34;&gt;http://programandosemcafeina.blogspot.com/2008/08/web-democracia-criticas-e-sugestoes.html&lt;/a&gt;.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>How I invented Ajax despite it already existing</title>
            <link rel="alternate" href="/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/" type="text/html" />
            <id>/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/</id>
            <updated>2023-11-15T14:47:00Z</updated>

            <summary type="html">This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.</summary>
            <content type="html">&lt;p&gt;This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.&lt;/p&gt;

&lt;p&gt;Between 2003 and 2004, I worked for a very small software company. There were only three developers, and each one worked separately on a group of softwares, as if they were three one-person teams. I had little experience and had recently graduated, and I also did not have the habit of reading books and articles to learn. All my learning until then was based on empirical experimentation, solving real problems from personal projects and the companies I was working for.&lt;/p&gt;

&lt;p&gt;At that time, the majority of websites and web applications would reload the entire pages every time a click or form submission occurred. And for me, that was indeed the only way to exchange information between the browser and the server. My knowledge was limited to that.&lt;/p&gt;

&lt;p&gt;However, one of the projects at Indata involved customizing a GIS project from Mexico. In this project, I noticed that information was loaded on the page without having to reload the entire page. I found that amazing, a revolution within my limited knowledge. I needed to learn that!&lt;/p&gt;

&lt;p&gt;Notice, at that time we didn&amp;rsquo;t have Chrome&amp;rsquo;s Web Developer Tools. We used Internet Explorer to debug websites. JavaScript errors were masked, there was no way to inspect anything. Additionally, the JavaScript code on the client side of this software was minified.&lt;/p&gt;

&lt;p&gt;Reading line by line of that obfuscated code I found the so-called XMLHttpRequest object where the magic was happening. But I didn&amp;rsquo;t understand anything. To me, that was the end of the execution line. There was nothing more after it.&lt;/p&gt;

&lt;p&gt;I couldn&amp;rsquo;t notice the callback that would be executed through it. It wasn&amp;rsquo;t due to a total unawareness of the pattern, as at that time I had already developed some games in Java that used listeners in response to user actions, a similar pattern, but due to my lack of practice, I couldn&amp;rsquo;t associate it.&lt;/p&gt;

&lt;p&gt;Nowadays, I try to understand why I couldn&amp;rsquo;t associate one thing with another, and my only hypothesis is that the abstraction of a callback for user action made sense to me at the time, as it is an external interruption to the code&amp;rsquo;s execution flow, while the callback within the execution flow didn&amp;rsquo;t make sense in my limited brain at that time.&lt;/p&gt;

&lt;p&gt;Also, Google was not very popular in Brazil yet, and all search tools (in Portuguese because at the time I didn&amp;rsquo;t have experience in other languages either) were directories and did not search internally within websites and texts. Neither did my colleagues at the company know that technology. So, for some time, I was stuck without understanding how that GIS software could load data from the server without reloading the page.&lt;/p&gt;

&lt;p&gt;So one day I remembered an HTML tag that I rarely used, but had learned in an online HTML and JavaScript course: the iframe. I noticed that with the iframe, I could communicate with the parent using JavaScript and exchange information. This was the solution I needed.&lt;/p&gt;

&lt;p&gt;With iframe, I would be able to retrieve information from the server and populate it in the parent through a function call, sending the data as a parameter. In fact, it was the same callback pattern used by XMLHttpRequest, but in this case, it made sense to me, as it was an interruption coming from another page being loaded.&lt;/p&gt;

&lt;p&gt;At the time, I was developing Angaturama, a receptive tourism company management system. I started using the &amp;ldquo;Ajax&amp;rdquo; technique through iframes, and as a result, the system became very user-friendly. We even created a shortcut key to display hidden iframes that were hidden by CSS containing the results for debugging purposes.&lt;/p&gt;

&lt;p&gt;A few years after I left the company, Google started to become more popular in Brazil. I learned English, developed the habit of reading books, technical articles, and participating in software discussion groups. During this period, I began to notice that there was a trending term called &amp;ldquo;Ajax&amp;rdquo;. Everywhere I looked, people were talking about it, and the infamous XMLHttpRequest. In 2006, jQuery was born and made everything even easier.&lt;/p&gt;

&lt;p&gt;When I discovered all of this, I felt like the boy from the movie &lt;a href=&#34;https://www.imdb.com/title/tt0461804/&#34;&gt;&amp;ldquo;Mongolian ping pong&amp;rdquo;&lt;/a&gt; who is sent to school and finds a court with several other boys playing ping pong, with that ball he spent months trying to figure out what it was.&lt;/p&gt;

&lt;p&gt;In a later meeting of Indata employees, I remember us laughing about this makeshift job done on the Angaturama and how, due to our lack of knowledge at the time, I became famous among them for inventing &amp;ldquo;Ajax&amp;rdquo; despite its existence already.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Collecting data to identify data collect</title>
            <link rel="alternate" href="/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/" type="text/html" />
            <id>/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/</id>
            <updated>2023-08-19T13:37:00Z</updated>

            <summary type="html">This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!</summary>
            <content type="html">&lt;p&gt;This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!&lt;/p&gt;

&lt;p&gt;It happened in mid-2022. At the time, I had already been working for a year and a half in the Trust &amp;amp; Safety area at OLX Brasil, and we had achieved significant success in reducing fraud attempts. Especially in a type of fraud known as &amp;ldquo;False Payment&amp;rdquo;. In fact, another type of fraud known as &amp;ldquo;Data Collect&amp;rdquo; has since become the highest number of reported cases.&lt;/p&gt;

&lt;p&gt;The problem with this other type of fraud is that the behavior of the fraudsters was very similar to that of another type of malicious user: the spammer. A modeling to identify this type of fraudster, done a few months earlier, failed for this reason. We were unable to separate the spammer from a fraudster of this type of fraud in any way.&lt;/p&gt;

&lt;p&gt;Then you may ask me, why not prevent both characters with the same model?&lt;/p&gt;

&lt;p&gt;The approach we had towards spammers was quite different from the approach towards a fraudster. I cannot go into detail about how these approaches because confidentiality is part of security, and making it public can assist both groups in finding ways to circumvent. But trust me, it was very important to distinguish our actions in dealing with these two groups.&lt;/p&gt;

&lt;p&gt;We were at an impasse, our last model was actually doing a good job at accurately distinguishing between legitimate users and these two groups, but we couldn&amp;rsquo;t put it into production for making automatic decisions. We were missing the ability to identify which micro behaviors set one group apart from the other, and there was only one team in the company that could help us with that: the monitoring team.&lt;/p&gt;

&lt;p&gt;This team was a multidisciplinary team of fraud identification experts in the company. They were responsible for manually reviewing samples from automated models and operational team decisions regarding reports that were not handled automatically.&lt;/p&gt;

&lt;p&gt;We spoke with this team in an attempt to find any new criteria that could help us separate the cases. The conversation proved to be a not-so-effective tool, as the developers and monitors didn&amp;rsquo;t seem to speak the same language. We couldn&amp;rsquo;t objectively map out these determining factors through these unstructured conversation.&lt;/p&gt;

&lt;p&gt;To address this, we created a spreadsheet with a large sample of cases that the not so good model had identified as fraud (and within those cases, there were many spams) and added two columns: one to indicate whether it was fraud or spam, and another for free text where the operator could explain the reasons behind their conclusion. This spreadsheet was sent to the monitoring team for completion.&lt;/p&gt;

&lt;p&gt;The result could not be more satisfactory. In the first lines, the free text column started off more &amp;ldquo;poetic&amp;rdquo; and gradually, the repetitive pattern of the groups caused the monitors to write in a more concise and even repetitive manner, generating a clear pattern on new criteria that we could adopt for the machine learning model.&lt;/p&gt;

&lt;p&gt;Our data collection had worked. Our data collection had been successful.&lt;/p&gt;

&lt;p&gt;These pieces of information generated new feature calculations for a new machine learning model training. We sent new samples of the results from this new model for monitoring evaluation, which could assure us of high accuracy. For a few weeks, we kept the artifact in production without taking any action, only logging, just to monitor if the generated result would reflect the results obtained in the offline evaluation. Everything went smoothly.&lt;/p&gt;

&lt;p&gt;In the weeks after enabling the new model to take actions against the fraudsters, we experienced an average decrease of 33% in &amp;ldquo;Data collect&amp;rdquo; frauds per week. Another type of fraud, known as &amp;ldquo;False Sale&amp;rdquo;, also saw a 25% decrease, as it relies on a more complicated social engineering involving the execution of &amp;ldquo;Data collect&amp;rdquo; fraud at first place to be effective.&lt;/p&gt;

&lt;p&gt;So we data collected to avoid data collect successfully.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The fantastic widget factory</title>
            <link rel="alternate" href="/work/globocom/webmedia/the-fantastic-widget-factory/" type="text/html" />
            <id>/work/globocom/webmedia/the-fantastic-widget-factory/</id>
            <updated>2023-04-29T14:16:00Z</updated>

            <summary type="html">This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.</summary>
            <content type="html">&lt;p&gt;This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.&lt;/p&gt;

&lt;p&gt;The case occurred in mid-2006 at Globo.com. At that time, the portal was divided into several independent sites, such as G1, GloboEsporte, Ego, among others. Generally, these sites displayed only text news, and all video assets were concentrated on another site called GMC, Globo Media Center.&lt;/p&gt;

&lt;p&gt;Our initial goal was to break down the silo in relation to video content, allowing developers from other sites to offer GMC registered videos in a simple and standardized way. At the time, an API was already available (known as WebmediaAPI), but it did not standardize the display of these content offerings, which would require additional effort from these teams for implementation.&lt;/p&gt;

&lt;p&gt;The solution we proposed would be to develop standardized visual widgets. These widgets would bring offers of various types such as recent videos, most viewed videos, best-rated videos, with the possibility of applying various filters and layout options. In this way, it would only be necessary to instantiate the widget into backend code page to offer video content in any Globo.com website.&lt;/p&gt;

&lt;p&gt;Here comes our first issue: At that time, Globo.com websites used a platform called Vignette, which made all its content static during the publication of homepages and articles. In other words, the news articles were static HTML generated after each publication, and the homepages were the same. Any changes to the page needed an editor to publish them for them to be displayed.&lt;/p&gt;

&lt;p&gt;This went against the dynamic nature of video content. How to maintain dynamically ordered content such as &amp;ldquo;most viewed&amp;rdquo; on a static HTML page? One way would be to use server-side includes, since the static files were served by Apache.&lt;/p&gt;

&lt;p&gt;Unfortunately, there were many restrictions to make any changes to the infrastructure at globo.com in 2006. To achieve this, we would need a lot of persuasion, meetings, presentations and one-on-one conversations, with no guarantee of success. We did not have the necessary time or social skills required for this goal. But we had a lot of creativity!&lt;/p&gt;

&lt;p&gt;So we decided to make these widgets on the client side. However, cross-domain requests to obtain information from WebmediaApi would only be possible if we could provide the necessary CORS headers. But to do this, changes to the infrastructure would also be necessary, as the application server (Apache) in front of WebmediaApi was also rigid, beyond our control, and exchanged all headers.&lt;/p&gt;

&lt;p&gt;Our solution was to allow WebmediaApi to return javascript call to a callback function instead of the json content. We then had a JavaScript with the widget code, which included another &amp;ldquo;JavaScript&amp;rdquo; that was the WebmediaAPI call with the name of a dynamic function as a parameter. This function was &amp;ldquo;printed&amp;rdquo; in the API response as a function call to finally render the chosen and customized widget.&lt;/p&gt;

&lt;p&gt;There was even a timeout control in case the callback took too long to respond.&lt;/p&gt;

&lt;p&gt;This solved our problem and after a few sprints, we created a true widget factory. Needs and ideas would come, and we would generate the necessary widgets and filters on WebmediaApi. It was a success. So much so, that the standardized layout of the widgets created the need to update the GMC, the aggregator site for all videos, to use the same layouts standards. Thus, Globo Vídeos was born, the successor to GMC.&lt;/p&gt;

&lt;p&gt;But there was a catch&amp;hellip;&lt;/p&gt;

&lt;p&gt;At that time, Google did not index pages rendered via JavaScript. And we needed at least Globo Videos to remain server-side rendered so that we wouldn&amp;rsquo;t lose relevance in organic searches.&lt;/p&gt;

&lt;p&gt;The solution: we started rendering javascript widgets on the server. Since everything in Vignette was java, we had to render using Rhino (if I&amp;rsquo;m not mistaken, before java 1.5 it was only possible to render javascript in the JVM through this lib).&lt;/p&gt;

&lt;p&gt;To maintain the freshness of the data, the Globo Vídeos homepage began to be rendered on the server every X period of time. I don&amp;rsquo;t remember if this automatic publication was done via cron or some Vignette tool, but the point is that static HTML began to be generated frequently, with the HTML of the widgets. However, the video pages continued to display client-side widgets as it would be impractical to render millions of HTML frequently.&lt;/p&gt;

&lt;p&gt;There was an added complexity to this solution as the process responsible for generating static HTML files on Globo.com was running on a WebLogic server. Although our server-side JavaScript rendering solution worked seamlessly in all other environments, including local, staging, and QA, it failed to function when deployed to production. After extensive debugging efforts, we discovered that the WebLogic version used in production differed from the other environments and contained an older Rhino version that superseded our newer one. To ensure proper widget rendering, we needed to execute the code by instantiating another classloader that loaded the correct Rhino jar version, which we successfully accomplished.&lt;/p&gt;

&lt;p&gt;In the end, we were able to develop a client-side widget platform that empowers websites to offer videos independently and consistently, without having to duplicate code. Furthermore, we devised a server-side rendered video product that can be easily indexed by Google. These achievements were made possible by implementing a series of three engineering workarounds.&lt;/p&gt;
</content>
        </entry>
    
</feed>
