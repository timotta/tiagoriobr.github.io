<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
    <title>tiago.rio.br</title>
    
    <link rel="alternate" href="/" />
    <link rel="self" href="/feed.xml" type="application/atom+xml" />
    <id>/</id>
    <updated>2023-11-15T14:45:53Z</updated>

    <author>
        <name>Tiago Albineli Motta</name>
        
        <uri>/</uri>
    </author>

    
        <entry>
            <title>How I invented Ajax despite it already existing</title>
            <link rel="alternate" href="/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/" type="text/html" />
            <id>/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/</id>
            <updated>2023-11-15T14:45:00Z</updated>

            <summary type="html">This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.</summary>
            <content type="html">&lt;p&gt;This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.&lt;/p&gt;

&lt;p&gt;Between 2003 and 2004, I worked for a very small software company. There were only three developers, and each one worked separately on a group of softwares, as if they were three one-person teams. I had little experience and had recently graduated, and I also did not have the habit of reading books and articles to learn. All my learning until then was based on empirical experimentation, solving real problems from personal projects and the companies I was working for.&lt;/p&gt;

&lt;p&gt;At that time, the majority of websites and web applications would reload the entire pages every time a click or form submission occurred. And for me, that was indeed the only way to exchange information between the browser and the server. My knowledge was limited to that.&lt;/p&gt;

&lt;p&gt;However, one of the projects at Indata involved customizing a GIS project from Mexico. In this project, I noticed that information was loaded on the page without having to reload the entire page. I found that amazing, a revolution within my limited knowledge. I needed to learn that!&lt;/p&gt;

&lt;p&gt;Notice, at that time we didn&amp;rsquo;t have Chrome&amp;rsquo;s Web Developer Tools. We used Internet Explorer to debug websites. JavaScript errors were masked, there was no way to inspect anything. Additionally, the JavaScript code on the client side of this software was minified.&lt;/p&gt;

&lt;p&gt;Reading line by line of that obfuscated code I found the so-called XMLHttpRequest object where the magic was happening. But I didn&amp;rsquo;t understand anything. To me, that was the end of the execution line. There was nothing more after it.&lt;/p&gt;

&lt;p&gt;I couldn&amp;rsquo;t notice the callback that would be executed through it. It wasn&amp;rsquo;t due to a total unawareness of the pattern, as at that time I had already developed some games in Java that used listeners in response to user actions, a similar pattern, but due to my lack of practice, I couldn&amp;rsquo;t associate it.&lt;/p&gt;

&lt;p&gt;Nowadays, I try to understand why I couldn&amp;rsquo;t associate one thing with another, and my only hypothesis is that the abstraction of a callback for user action made sense to me at the time, as it is an external interruption to the code&amp;rsquo;s execution flow, while the callback within the execution flow didn&amp;rsquo;t make sense in my limited brain at that time.&lt;/p&gt;

&lt;p&gt;Also, Google was not very popular in Brazil yet, and all search tools (in Portuguese because at the time I didn&amp;rsquo;t have experience in other languages either) were directories and did not search internally within websites and texts. Neither did my colleagues at the company know that technology. So, for some time, I was stuck without understanding how that GIS software could load data from the server without reloading the page.&lt;/p&gt;

&lt;p&gt;So one day I remembered an HTML tag that I rarely used, but had learned in an online HTML and JavaScript course: the iframe. I noticed that with the iframe, I could communicate with the parent using JavaScript and exchange information. This was the solution I needed.&lt;/p&gt;

&lt;p&gt;With iframe, I would be able to retrieve information from the server and populate it in the parent through a function call, sending the data as a parameter. In fact, it was the same callback pattern used by XMLHttpRequest, but in this case, it made sense to me, as it was an interruption coming from another page being loaded.&lt;/p&gt;

&lt;p&gt;At the time, I was developing Angaturama, a receptive tourism company management system. I started using the &amp;ldquo;Ajax&amp;rdquo; technique through iframes, and as a result, the system became very user-friendly. We even created a shortcut key to display hidden iframes that were hidden by CSS containing the results for debugging purposes.&lt;/p&gt;

&lt;p&gt;A few years after I left the company, Google started to become more popular in Brazil. I learned English, developed the habit of reading books, technical articles, and participating in software discussion groups. During this period, I began to notice that there was a trending term called &amp;ldquo;Ajax&amp;rdquo;. Everywhere I looked, people were talking about it, and the infamous XMLHttpRequest. In 2006, jQuery was born and made everything even easier.&lt;/p&gt;

&lt;p&gt;When I discovered all of this, I felt like the boy from the movie &lt;a href=&#34;https://www.imdb.com/title/tt0461804/&#34;&gt;&amp;ldquo;Mongolian ping pong&amp;rdquo;&lt;/a&gt; who is sent to school and finds a court with several other boys playing ping pong, with that ball he spent months trying to figure out what it was.&lt;/p&gt;

&lt;p&gt;In a later meeting of Indata employees, I remember us laughing about this makeshift job done on the Angaturama and how, due to our lack of knowledge at the time, I became famous among them for inventing &amp;ldquo;Ajax&amp;rdquo; despite its existence already.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Collecting data to identify data collect</title>
            <link rel="alternate" href="/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/" type="text/html" />
            <id>/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/</id>
            <updated>2023-08-19T13:37:00Z</updated>

            <summary type="html">This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!</summary>
            <content type="html">&lt;p&gt;This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!&lt;/p&gt;

&lt;p&gt;It happened in mid-2022. At the time, I had already been working for a year and a half in the Trust &amp;amp; Safety area at OLX Brasil, and we had achieved significant success in reducing fraud attempts. Especially in a type of fraud known as &amp;ldquo;False Payment&amp;rdquo;. In fact, another type of fraud known as &amp;ldquo;Data Collect&amp;rdquo; has since become the highest number of reported cases.&lt;/p&gt;

&lt;p&gt;The problem with this other type of fraud is that the behavior of the fraudsters was very similar to that of another type of malicious user: the spammer. A modeling to identify this type of fraudster, done a few months earlier, failed for this reason. We were unable to separate the spammer from a fraudster of this type of fraud in any way.&lt;/p&gt;

&lt;p&gt;Then you may ask me, why not prevent both characters with the same model?&lt;/p&gt;

&lt;p&gt;The approach we had towards spammers was quite different from the approach towards a fraudster. I cannot go into detail about how these approaches because confidentiality is part of security, and making it public can assist both groups in finding ways to circumvent. But trust me, it was very important to distinguish our actions in dealing with these two groups.&lt;/p&gt;

&lt;p&gt;We were at an impasse, our last model was actually doing a good job at accurately distinguishing between legitimate users and these two groups, but we couldn&amp;rsquo;t put it into production for making automatic decisions. We were missing the ability to identify which micro behaviors set one group apart from the other, and there was only one team in the company that could help us with that: the monitoring team.&lt;/p&gt;

&lt;p&gt;This team was a multidisciplinary team of fraud identification experts in the company. They were responsible for manually reviewing samples from automated models and operational team decisions regarding reports that were not handled automatically.&lt;/p&gt;

&lt;p&gt;We spoke with this team in an attempt to find any new criteria that could help us separate the cases. The conversation proved to be a not-so-effective tool, as the developers and monitors didn&amp;rsquo;t seem to speak the same language. We couldn&amp;rsquo;t objectively map out these determining factors through these unstructured conversation.&lt;/p&gt;

&lt;p&gt;To address this, we created a spreadsheet with a large sample of cases that the not so good model had identified as fraud (and within those cases, there were many spams) and added two columns: one to indicate whether it was fraud or spam, and another for free text where the operator could explain the reasons behind their conclusion. This spreadsheet was sent to the monitoring team for completion.&lt;/p&gt;

&lt;p&gt;The result could not be more satisfactory. In the first lines, the free text column started off more &amp;ldquo;poetic&amp;rdquo; and gradually, the repetitive pattern of the groups caused the monitors to write in a more concise and even repetitive manner, generating a clear pattern on new criteria that we could adopt for the machine learning model.&lt;/p&gt;

&lt;p&gt;Our data collection had worked. Our data collection had been successful.&lt;/p&gt;

&lt;p&gt;These pieces of information generated new feature calculations for a new machine learning model training. We sent new samples of the results from this new model for monitoring evaluation, which could assure us of high accuracy. For a few weeks, we kept the artifact in production without taking any action, only logging, just to monitor if the generated result would reflect the results obtained in the offline evaluation. Everything went smoothly.&lt;/p&gt;

&lt;p&gt;In the weeks after enabling the new model to take actions against the fraudsters, we experienced an average decrease of 33% in &amp;ldquo;Data collect&amp;rdquo; frauds per week. Another type of fraud, known as &amp;ldquo;False Sale&amp;rdquo;, also saw a 25% decrease, as it relies on a more complicated social engineering involving the execution of &amp;ldquo;Data collect&amp;rdquo; fraud at first place to be effective.&lt;/p&gt;

&lt;p&gt;So we data collected to avoid data collect successfully.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The fantastic widget factory</title>
            <link rel="alternate" href="/work/globocom/webmedia/the-fantastic-widget-factory/" type="text/html" />
            <id>/work/globocom/webmedia/the-fantastic-widget-factory/</id>
            <updated>2023-04-29T14:16:00Z</updated>

            <summary type="html">This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.</summary>
            <content type="html">&lt;p&gt;This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.&lt;/p&gt;

&lt;p&gt;The case occurred in mid-2006 at Globo.com. At that time, the portal was divided into several independent sites, such as G1, GloboEsporte, Ego, among others. Generally, these sites displayed only text news, and all video assets were concentrated on another site called GMC, Globo Media Center.&lt;/p&gt;

&lt;p&gt;Our initial goal was to break down the silo in relation to video content, allowing developers from other sites to offer GMC registered videos in a simple and standardized way. At the time, an API was already available (known as WebmediaAPI), but it did not standardize the display of these content offerings, which would require additional effort from these teams for implementation.&lt;/p&gt;

&lt;p&gt;The solution we proposed would be to develop standardized visual widgets. These widgets would bring offers of various types such as recent videos, most viewed videos, best-rated videos, with the possibility of applying various filters and layout options. In this way, it would only be necessary to instantiate the widget into backend code page to offer video content in any Globo.com website.&lt;/p&gt;

&lt;p&gt;Here comes our first issue: At that time, Globo.com websites used a platform called Vignette, which made all its content static during the publication of homepages and articles. In other words, the news articles were static HTML generated after each publication, and the homepages were the same. Any changes to the page needed an editor to publish them for them to be displayed.&lt;/p&gt;

&lt;p&gt;This went against the dynamic nature of video content. How to maintain dynamically ordered content such as &amp;ldquo;most viewed&amp;rdquo; on a static HTML page? One way would be to use server-side includes, since the static files were served by Apache.&lt;/p&gt;

&lt;p&gt;Unfortunately, there were many restrictions to make any changes to the infrastructure at globo.com in 2006. To achieve this, we would need a lot of persuasion, meetings, presentations and one-on-one conversations, with no guarantee of success. We did not have the necessary time or social skills required for this goal. But we had a lot of creativity!&lt;/p&gt;

&lt;p&gt;So we decided to make these widgets on the client side. However, cross-domain requests to obtain information from WebmediaApi would only be possible if we could provide the necessary CORS headers. But to do this, changes to the infrastructure would also be necessary, as the application server (Apache) in front of WebmediaApi was also rigid, beyond our control, and exchanged all headers.&lt;/p&gt;

&lt;p&gt;Our solution was to allow WebmediaApi to return javascript call to a callback function instead of the json content. We then had a JavaScript with the widget code, which included another &amp;ldquo;JavaScript&amp;rdquo; that was the WebmediaAPI call with the name of a dynamic function as a parameter. This function was &amp;ldquo;printed&amp;rdquo; in the API response as a function call to finally render the chosen and customized widget.&lt;/p&gt;

&lt;p&gt;There was even a timeout control in case the callback took too long to respond.&lt;/p&gt;

&lt;p&gt;This solved our problem and after a few sprints, we created a true widget factory. Needs and ideas would come, and we would generate the necessary widgets and filters on WebmediaApi. It was a success. So much so, that the standardized layout of the widgets created the need to update the GMC, the aggregator site for all videos, to use the same layouts standards. Thus, Globo Vídeos was born, the successor to GMC.&lt;/p&gt;

&lt;p&gt;But there was a catch&amp;hellip;&lt;/p&gt;

&lt;p&gt;At that time, Google did not index pages rendered via JavaScript. And we needed at least Globo Videos to remain server-side rendered so that we wouldn&amp;rsquo;t lose relevance in organic searches.&lt;/p&gt;

&lt;p&gt;The solution: we started rendering javascript widgets on the server. Since everything in Vignette was java, we had to render using Rhino (if I&amp;rsquo;m not mistaken, before java 1.5 it was only possible to render javascript in the JVM through this lib).&lt;/p&gt;

&lt;p&gt;To maintain the freshness of the data, the Globo Vídeos homepage began to be rendered on the server every X period of time. I don&amp;rsquo;t remember if this automatic publication was done via cron or some Vignette tool, but the point is that static HTML began to be generated frequently, with the HTML of the widgets. However, the video pages continued to display client-side widgets as it would be impractical to render millions of HTML frequently.&lt;/p&gt;

&lt;p&gt;There was an added complexity to this solution as the process responsible for generating static HTML files on Globo.com was running on a WebLogic server. Although our server-side JavaScript rendering solution worked seamlessly in all other environments, including local, staging, and QA, it failed to function when deployed to production. After extensive debugging efforts, we discovered that the WebLogic version used in production differed from the other environments and contained an older Rhino version that superseded our newer one. To ensure proper widget rendering, we needed to execute the code by instantiating another classloader that loaded the correct Rhino jar version, which we successfully accomplished.&lt;/p&gt;

&lt;p&gt;In the end, we were able to develop a client-side widget platform that empowers websites to offer videos independently and consistently, without having to duplicate code. Furthermore, we devised a server-side rendered video product that can be easily indexed by Google. These achievements were made possible by implementing a series of three engineering workarounds.&lt;/p&gt;
</content>
        </entry>
    
</feed>
