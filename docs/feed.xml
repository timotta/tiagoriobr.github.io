<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
    <title>tiago.rio.br</title>
    
    <link rel="alternate" href="/" />
    <link rel="self" href="/feed.xml" type="application/atom+xml" />
    <id>/</id>
    <updated>2025-06-28T12:58:21Z</updated>

    <author>
        <name>Tiago Albineli Motta</name>
        
        <uri>/</uri>
    </author>

    
        <entry>
            <title>Melhor bolo de chocolate de microondas de 2024 pelo NY</title>
            <link rel="alternate" href="/pessoal/receitas/sobremesas/teste-teste-teste/" type="text/html" />
            <id>/pessoal/receitas/sobremesas/teste-teste-teste/</id>
            <updated>2025-06-28T12:58:00Z</updated>

            
            <content type="html">&lt;h2&gt;Bolo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;1/3 de xícara de manteiga derretida&lt;/li&gt;
&lt;li&gt;&amp;frac14; de xícara de leite&lt;/li&gt;
&lt;li&gt;1 colher de chá de essência de baunilha&lt;/li&gt;
&lt;li&gt;1 colher de sopa de café passado forte&lt;/li&gt;
&lt;li&gt;&amp;frac34; de xícara de açucar mascavo&lt;/li&gt;
&lt;li&gt;1 xícara de farinha&lt;/li&gt;
&lt;li&gt;&amp;frac12; xícara de cacau em pó 50%&lt;/li&gt;
&lt;li&gt;&amp;frac12; colher de chá de sal&lt;/li&gt;
&lt;li&gt;&amp;frac12; colher de sopa de fermento químico em pó&lt;/li&gt;
&lt;li&gt;&amp;frac14; de colher de sopa de bicarbonato&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Calda&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&amp;frac34; de xícara de água quente&lt;/li&gt;
&lt;li&gt;&amp;frac12; xícara de açucar mascavo&lt;/li&gt;
&lt;li&gt;&amp;frac14; de xícara de cacau em pó 50%&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pra fazer o bolo&lt;/h2&gt;

&lt;p&gt;Mistura tudo com uma colher até ficar uma massa homogênea, espalhe sobre um recipiente que possa ir pro microondas mas não coloque o no microondas ainda.&lt;/p&gt;

&lt;h2&gt;Pra fazer a calda&lt;/h2&gt;

&lt;p&gt;Misture o cacau em pó e o açucar mascavo até ficar homogeneo, e cubra o bolo com essa &amp;ldquo;farinha&amp;rdquo; e depois derrame com cuidado a água quente sobre esse o bolo.&lt;/p&gt;

&lt;h2&gt;Finalizando&lt;/h2&gt;

&lt;p&gt;Coloque no microondas por 5 minutos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.instagram.com/reel/DFqojLzyV6z/&#34;&gt;Receita original&lt;/a&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The Journey to Hybrid Search at OLX Brasil</title>
            <link rel="alternate" href="/work/olxbrasil/search/jouney-hybrid-search/" type="text/html" />
            <id>/work/olxbrasil/search/jouney-hybrid-search/</id>
            <updated>2025-06-07T13:01:00Z</updated>

            <summary type="html">This is the story of how we improved the search experience at OLX Brasil in early 2024 by introducing semantic search.</summary>
            <content type="html">&lt;p&gt;This is the story of how we improved the search experience at OLX Brasil in early 2024 by introducing semantic search.&lt;/p&gt;

&lt;p&gt;We’ve always known that search is at the core of the user experience. When someone types something like “European football team shirt,” they’re not just entering keywords — they’re expressing intent. Our traditional lexical search engine, &lt;a href=&#34;/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/&#34;&gt;Podium&lt;/a&gt;, handled direct matches well, but often failed when the query was more conceptual or abstract.&lt;/p&gt;

&lt;p&gt;That’s where it all started — with a hackday.&lt;/p&gt;

&lt;h2&gt;From Hackday to Proof of Value&lt;/h2&gt;

&lt;p&gt;During an internal experiment started by &lt;a href=&#34;https://www.linkedin.com/in/daniel-correa-araujo/&#34;&gt;Daniel Araújo&lt;/a&gt;, he tested the use of the OpenAI embeddings API to improve our ability to understand and respond to semantic queries. The idea was simple: what if search could understand concepts, not just strings?&lt;/p&gt;

&lt;p&gt;The first tests showed something powerful: it was able to return relevant results for many examples of queries like “European football team shirt” — even when listing titles didn’t contain those exact words. This revealed real potential in semantic search.&lt;/p&gt;

&lt;p&gt;But there was skepticism.&lt;/p&gt;

&lt;h2&gt;A PM’s Bet (and a Little Faith)&lt;/h2&gt;

&lt;p&gt;Our initial hypothesis was conservative: this technology wouldn’t increase replies (the number of contacts on listings). At best, we thought it might improve coverage slightly — and at a high operational cost. Still, our Product Manager, &lt;a href=&#34;https://www.linkedin.com/in/claudiabozza/&#34;&gt;Claudia Bozza&lt;/a&gt; believed in it and pushed forward, even when we flagged potential cost-benefit concerns.&lt;/p&gt;

&lt;p&gt;And she was right.&lt;/p&gt;

&lt;h2&gt;First Version: Fast Build, Fast Results&lt;/h2&gt;

&lt;p&gt;We built a quick solution in just two weeks: if a lexical search returned no results, we would trigger a fallback semantic search. Even with this simple approach, the results were impressive. That gave us the confidence to invest in building a robust hybrid search system.&lt;/p&gt;

&lt;h2&gt;A Strategic Shift: Moving Away from OpenAI&lt;/h2&gt;

&lt;p&gt;As the solution evolved, we decided to move away from OpenAI’s API. Response times were too high for real-time search. We also tested Google’s Gemini, and faced similar issues.&lt;/p&gt;

&lt;p&gt;Instead, we adopted local embedding generation using &lt;a href=&#34;https://sbert.net/&#34;&gt;sentence-transformers&lt;/a&gt;, giving us more control and performance. We started generating vectors in-house, allowing the solution to scale more efficiently.&lt;/p&gt;

&lt;h2&gt;So why hybrid, and not purely semantic?&lt;/h2&gt;

&lt;p&gt;For OLX’s search engine, diversity and recency are critical metrics. We had already proven in several past experiments that both correlate strongly with the number of replies and how quickly items are sold.&lt;/p&gt;

&lt;p&gt;When it comes to diversity, we noticed a major drawback in using a purely semantic approach: if we generated vector embeddings using only the ad title, diversity dropped to near zero for simple one- or two-word queries. This happened because the cosine distance between the word &amp;ldquo;iPhone&amp;rdquo; and an ad titled simply &amp;ldquo;iPhone&amp;rdquo; is effectively zero. Since we have a large volume of listings with minimal or generic titles, this led to highly repetitive results.&lt;/p&gt;

&lt;p&gt;On the other hand, the more ad fields we added to the embedding (e.g. description, location), the better the diversity became — but at the cost of precision.&lt;/p&gt;

&lt;p&gt;As for recency, even when we applied time decay functions in the final score computation, purely semantic search often returned very old listings — especially in queries with lots of potential matches (which is most of them). That’s because in vector search, you have to define how many items each shard should return. Those selected items won’t necessarily include the newest listings. For example, if a user searches for “iPhone” and we have 20,000 iPhone listings per shard, but only retrieve the top 50 per shard, the most recent ads might be buried in the remaining 19,950 items — even if their cosine distance is only slightly worse.&lt;/p&gt;

&lt;p&gt;We could increase the number of items per shard, but that comes at a steep cost in processing time and infrastructure usage.&lt;/p&gt;

&lt;p&gt;After extensive experimentation, we concluded that a hybrid search strategy offered the best of both worlds. The offline results confirmed this: hybrid search provided the strongest balance between ranking precision and overall quality.&lt;/p&gt;

&lt;h2&gt;Engineering the Hybrid Search Algorithm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/hybrid-search.png&#34; alt=&#34;/assets/images/podium-math.gif&#34;&gt;&lt;/p&gt;

&lt;p&gt;Once the concept was validated, we moved into an intensive algorithm analysis phase. We ran over 40 offline experiments, evaluating key relevance metrics such as freshness, diversity and &lt;a href=&#34;https://medium.com/grupoolxtech/uma-nova-m%C3%A9trica-para-calcular-relev%C3%A2ncia-de-busca-65372f154f8f&#34;&gt;R-DCG (ranking-aware precision)&lt;/a&gt;. During this phase, we validated several crucial parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The number of items to retrieve from the vector search&lt;/li&gt;
&lt;li&gt;Which ad fields (title, description, location, etc.) to use for embedding generation&lt;/li&gt;
&lt;li&gt;Which embedding model to use (and how it performed on our dataset)&lt;/li&gt;
&lt;li&gt;How to combine lexical and semantic results effectively&lt;/li&gt;
&lt;li&gt;How to sort results after merging both types of rankings&lt;/li&gt;
&lt;li&gt;Which type of time decay function to apply to maintain result recency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This meticulous tuning allowed us to strike the right balance between semantic understanding and ranking precision. At the end, we could choose the best combination of parameters to get an algorithm that improved:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Freshness: +65%&lt;/li&gt;
&lt;li&gt;Diversity: +9%&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/grupoolxtech/uma-nova-m%C3%A9trica-para-calcular-relev%C3%A2ncia-de-busca-65372f154f8f&#34;&gt;R-DCG&lt;/a&gt;: +0.72%&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Experiments in Production&lt;/h2&gt;

&lt;p&gt;Bringing the solution to production experiments wasn’t straightforward.&lt;/p&gt;

&lt;p&gt;We had a hypothesis: the increased response time of lexical searches — caused by sharing infrastructure with the hybrid search — could negatively impact the control group’s metrics in A/B tests.&lt;/p&gt;

&lt;p&gt;To ensure a fair comparison, we set up a dedicated ElasticSearch infrastructure specifically for the hybrid search experiments. This separation ensured that the computational overhead of vector search wouldn’t degrade the performance of pure lexical queries.&lt;/p&gt;

&lt;p&gt;As a result, all experiments were run in an almost fully isolated infrastructure setup.&lt;/p&gt;

&lt;h2&gt;The Impact: Results that Justify the Investment&lt;/h2&gt;

&lt;p&gt;Despite the increase in response time, the gains in textual searches were undeniable:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;+3.7% in repliers&lt;/li&gt;
&lt;li&gt;+4.3% in replies&lt;/li&gt;
&lt;li&gt;+4.3% in clicks&lt;/li&gt;
&lt;li&gt;And most notably: a 94% reduction in “no result” queries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Encouraged by these results, we decided to roll out the solution to 100% of users. But there was a problem — the cost.&lt;/p&gt;

&lt;h2&gt;Performance: The Millisecond Challenge&lt;/h2&gt;

&lt;p&gt;Initially, the hybrid search response time was a major concern: &lt;strong&gt;P95 &amp;gt; 300ms&lt;/strong&gt;, compared to &lt;strong&gt;~80ms&lt;/strong&gt; for pure lexical search. This made a full rollout unfeasible without quadrupling our infrastructure costs.&lt;/p&gt;

&lt;p&gt;To tackle this, we launched a series of 24 performance experiments, each focused on shaving off latency. And for each experiment, we monitored key business metrics as guardrails to ensure we didn’t lose the gains we had previously achieved. Some of the key optimizations included:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Vector quantization&lt;/li&gt;
&lt;li&gt;Increasing the number of shards&lt;/li&gt;
&lt;li&gt;Upgrading ElasticSearch&lt;/li&gt;
&lt;li&gt;Reducing HNSW candidates&lt;/li&gt;
&lt;li&gt;Merging lexical search fields&lt;/li&gt;
&lt;li&gt;Generating embeddings using GPUs&lt;/li&gt;
&lt;li&gt;And more&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There was no silver bullet, but together these improvements brought &lt;strong&gt;P95 down to ~120ms&lt;/strong&gt; — even faster than other companies in our group that had also implemented hybrid search.&lt;/p&gt;

&lt;h2&gt;Conclusion: Progress, Perception&lt;/h2&gt;

&lt;p&gt;The biggest improvements came in conceptual and indirect searches, where vector models truly understood user intent. But new challenges emerged too. Queries that previously returned nothing started returning irrelevant results, which led to complaints and a perception of low precision.&lt;/p&gt;

&lt;p&gt;This perception didn’t exist in the pure Podium model — if a result didn’t match, it returned nothing. In the hybrid version, we had to deal with the qualitative nuance of returning something that might not feel useful to the user.&lt;/p&gt;

&lt;p&gt;That was the focus of our work during my final months there — work I didn’t get to finish: tuning problematic cases and exploring ways to visually communicate that certain results might be &amp;ldquo;off&amp;rdquo;, while still being the best semantic match available.&lt;/p&gt;

&lt;p&gt;What stood out during this journey was the company’s willingness to invest — not only in research, but also financially — to improve user experience. Even with performance improvements, hybrid search required more computing power: more ElasticSearch resources and ongoing vector processing.&lt;/p&gt;

&lt;h2&gt;The Economics Behind the Rollback&lt;/h2&gt;

&lt;p&gt;Recently, some friends who still work at OLX shared that hybrid search was eventually sunset, and OLX reverted back to &lt;a href=&#34;/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/&#34;&gt;Podium’s&lt;/a&gt; pure lexical model due to infrastructure costs.&lt;/p&gt;

&lt;p&gt;As an active user of the platform, I had already noticed a drop in views and contacts on my OLX listings. I even tested the search myself to understand what was happening — and the absence of semantic understanding was immediately noticeable. &lt;a href=&#34;https://www.linkedin.com/in/raphael-pinheiro-b6530a107/&#34;&gt;Raphael Pinheiro&lt;/a&gt;, who is not only a developer but also sells products made with his 3D printer, also noticed a sharp decline in views on his listings. He even mentioned it to me — without knowing that the hybrid search had been rolled back.&lt;/p&gt;

&lt;p&gt;Of course, these are anecdotal examples, and the overall impact was probably small enough to justify the decision — especially in core categories like real estate, vehicles, appliances, and furniture. But for smaller, more niche categories, the effect may have been significant. A 50% drop in views and replies in those segments might represent less than 1% of the total, but for that specific seller, it’s a huge impact.&lt;/p&gt;

&lt;p&gt;It’s unfortunate, because the progress was substantial and there was still room for optimization. All major players in search are investing in hybrid approaches, because they’re not only proven to deliver better results, but also offers better scalability for product development by avoiding manual tuning of lemmatization, synonyms, and other linguistic rules.&lt;/p&gt;

&lt;p&gt;Hybrid search may be gone from production, but the knowledge, progress, and collaboration built along the way remain one of the most impactful chapters of that journey.&lt;/p&gt;

&lt;h2&gt;The Minds Behind the Magic&lt;/h2&gt;

&lt;p&gt;Just like &lt;a href=&#34;/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/&#34;&gt;Podium&lt;/a&gt;, the hybrid search was build collaborativelly by &lt;a href=&#34;https://www.linkedin.com/in/timotta/&#34;&gt;me&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/daniel-correa-araujo/&#34;&gt;Daniel Araújo&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/raphael-pinheiro-b6530a107/&#34;&gt;Raphael Pinheiro&lt;/a&gt;. The engineering manager at the time was &lt;a href=&#34;https://www.linkedin.com/in/paulohesilva/&#34;&gt;Paulo Silva&lt;/a&gt; and the product manager &lt;a href=&#34;https://www.linkedin.com/in/claudiabozza/&#34;&gt;Claudia Bozza&lt;/a&gt;. A special thanks to &lt;a href=&#34;https://www.linkedin.com/in/josehisse/&#34;&gt;José Hisse&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/denilsonjunior/&#34;&gt;Denilson Limoeiro&lt;/a&gt; who helped us putting the vector generation running under GPU. Please accept my sincere apologies if I inadvertently omitted anyone.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Patching scikit-learn to improve API performance</title>
            <link rel="alternate" href="/work/willbank/account/patching-scikit-learn-improve-api-performance/" type="text/html" />
            <id>/work/willbank/account/patching-scikit-learn-improve-api-performance/</id>
            <updated>2025-02-27T10:56:00Z</updated>

            <summary type="html">This week at Will Bank, we deployed a new machine learning model to compete with the one we introduced into production late last year. Offline evaluations showed promising results, with a 35% improvement in the target metric and a cumulative gain of 60% compared to the solution that preceded last year’s model.</summary>
            <content type="html">&lt;p&gt;This week at Will Bank, we deployed a new machine learning model to compete with the one we introduced into production late last year. Offline evaluations showed promising results, with a 35% improvement in the target metric and a cumulative gain of 60% compared to the solution that preceded last year’s model.&lt;/p&gt;

&lt;p&gt;However, during the initial deployment of this new model—where we logged the results without impacting end users—we observed a significantly higher computational cost compared to the previous model. The API response time at the 99th percentile was 260% higher than that of the previous model.&lt;/p&gt;

&lt;p&gt;Indeed, the new model used more features, and the selected hyperparameters of the &lt;a href=&#34;https://xgboost.readthedocs.io/en/stable/&#34;&gt;XGBoost&lt;/a&gt; classifier indicated the use of more estimators and greater depth in the trees. Furthermore, the size of the model was significantly larger than the previous model. We had moved from a model of approximately 150kb to one of about 4mb in pickle format.&lt;/p&gt;

&lt;p&gt;This larger size made complete sense as we had added some categorical features with a greater diversity of values. Since we used &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html&#34;&gt;TargetEncoder&lt;/a&gt; and &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html&#34;&gt;OneHotEncoder&lt;/a&gt; to handle these features, and these preprocessing steps are part of the persisted &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html&#34;&gt;Pipeline&lt;/a&gt;, all possible categories would be saved in the final pickle, resulting in a much larger file.&lt;/p&gt;

&lt;p&gt;We did not expect this to be the cause of the slowdown, as a category lookup in the TargetEncoder or OneHotEncoder should be O(1), likely using a dictionary to store the valur for each category. As a result, we initially ruled out model size as a contributing factor to the issue.&lt;/p&gt;

&lt;p&gt;We then proceeded to evaluate whether the cause of the slowness was the deeper hyperparameters of XGBoost.&lt;/p&gt;

&lt;p&gt;I then retrained the model with softer parameters to speed up the results with a smaller amount of data. The result of this retraining was a much more performant model. I could conclude here that I had found the cause of the problem: the hyperparameters. But there was one more difference in the experiment setup: the amount of data.&lt;/p&gt;

&lt;p&gt;With this in hands I decided to put it to the test and repeated the hyperparameters of the problematic model, training with a small amount of data, and the model performed just as well as the one with smoother hyperparameters. In other words, the hyperparameters were not the cause of the slowness.&lt;/p&gt;

&lt;p&gt;I changed the approach to this analysis and decided to break down the problematic model by testing the execution time of each step in the Pipeline. Here, we benefit from the flexibility and standardization of the Pipelines and steps in &lt;a href=&#34;https://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt;. As a result, it was possible to observe that it was indeed the TargetEncoder of a single column that was causing 95% of the computational cost of the model. And it was precisely for the feature that had the highest variety of categories.&lt;/p&gt;

&lt;p&gt;This didn&amp;rsquo;t make sense to me; the number of different categories shouldn&amp;rsquo;t be causing performance issues. In my mind, looking up the calculated value would be done in dictionary O(1) time. So, I decided to investigate the code of the TargetEncoder from scikit-learn.&lt;/p&gt;

&lt;p&gt;As soon as I began reading the code, I realized that there was no category-value map as I had assumed. Instead, the implementation used two separate lists—one for categories and another for values. It appears that this &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html&#34;&gt;transformer&lt;/a&gt; is optimized for transforming large datasets, where the number of rows significantly exceeds the number of categories.&lt;/p&gt;

&lt;p&gt;Within the library’s internals, a vector the size of the category set is generated as a mask to determine which category should be applied. This means that each time a transformation is performed, a large mask must be created, applied, and used to select the corresponding value. While this approach might seem inefficient at first, it is highly effective for large datasets. Instead of iterating through every row individually, a much smaller category-based mask is applied across the entire dataset, significantly improving efficiency in such scenarios.&lt;/p&gt;

&lt;p&gt;However, this is not an ideal solution for a web endpoint, where each request processes a single row. In this scenario, the number of categories far exceeds the number of rows transformed per API request, resulting in unnecessary computational overhead.&lt;/p&gt;

&lt;p&gt;To address this issue, I developed the following function to patch the TargetEncoder when used in the API. Notice that I created a category/value cache and used it to define the value of each cell, which would make this algorithm O(NxM). However, since the API processes only one row and one column per request in this TargetEncoder, there is no need to iterate through rows and columns, effectively reducing the algorithm&amp;rsquo;s complexity to O(1).&lt;/p&gt;

&lt;pre&gt;
from sklearn.preprocessing import TargetEncoder
import numpy as np


def patch_target_encoder_improving_performance():
    TargetEncoder.transform_old = TargetEncoder.transform
    TargetEncoder.transform = new_target_encoder_transform


def new_target_encoder_transform(self, *args, **kwargs):
    if &#34;_cache_cat&#34; not in self.__dict__:
        self._cache_cat = {v: i for i, v in enumerate(self.categories_[0])}
    X = args[0]
    X_out = np.empty_like(X, dtype=np.float64)
    for row_i, row in enumerate(X):
        for col_i, category in enumerate(row):
            index = self._cache_cat.get(category)
            if index is None:
                X_out[row_i, col_i] = self.target_mean_
            else:
                X_out[row_i, col_i] = self.encodings_[0][index]
    return X_out
&lt;/pre&gt;

&lt;p&gt;After deploying this improvement, we were able to match the response time of the new model with the previous one as you can see in the graph below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/target-encoder-performance.png&#34; alt=&#34;/assets/images/mab.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;That said, a warning remains: this is not a fully secure solution. A future update to scikit-learn could modify its internal attributes, which this patch relies on, potentially altering its behavior or even causing silent errors. To mitigate this risk, we have implemented tests in the API to verify the expected behavior across various scenarios.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Resources that help me to learn data science</title>
            <link rel="alternate" href="/work/general/articles/resources-help-learn-data-science/" type="text/html" />
            <id>/work/general/articles/resources-help-learn-data-science/</id>
            <updated>2024-11-30T10:56:00Z</updated>

            <summary type="html">In this article, I list and comment on books and courses that helped me learn the discipline of data science as a developer.</summary>
            <content type="html">&lt;p&gt;In this article, I list and comment on books and courses that helped me learn the discipline of data science as a developer.&lt;/p&gt;

&lt;p&gt;About 10 years ago, I started studying Machine Learning casually and purely out of curiosity. Over time, I began working with data science in various roles across different companies. Most of my learning came through hands-on experience in projects that challenged my limited knowledge, forcing me to study further, ask questions to dozens of professionals, read articles, attend conferences, and, most importantly, experiment and learn from mistakes.&lt;/p&gt;

&lt;p&gt;In this journey, I participated in modeling systems for &lt;a href=&#34;/archives/recommendation/&#34;&gt;recommendation&lt;/a&gt;, churn identification, advertising segmentation, &lt;a href=&#34;/archives/trustandsafety/&#34;&gt;fraud prevention&lt;/a&gt;, &lt;a href=&#34;/archives/search/&#34;&gt;search&lt;/a&gt;, judicial decision prediction, and many other auxiliary classifications. This extensive experience, which demanded continuous learning, shaped me into what is now called a hybrid developer—someone who is both a software engineer and a data scientist.&lt;/p&gt;

&lt;p&gt;This brief introduction about myself is meant to provide a clearer response to the developers who often ask me how to become a hybrid developer like me. The answer is not straightforward, as my experience has been quite erratic. There was no clear learning path. As I’ve explained, most of my learning was unstructured and driven by business needs.&lt;/p&gt;

&lt;p&gt;However, there are indeed some courses and books that helped me along the way to hybridizing my knowledge. Without further ado, here are the main courses that helped me:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-introduction&#34;&gt;Machine Learning&lt;/a&gt; by Stanford: This was the first course I took, and it helped me understand the entire theory behind models by implementing algorithms that are now abstracted by various libraries. Without the intuitive knowledge of these algorithms, I believe using these abstractions can lead to false results.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/recommender-systems&#34;&gt;Introduction to Recommender Systems&lt;/a&gt; by the University of Minnesota: My first hybrid role in development and data science was working on the &lt;a href=&#34;/archives/recommendation/&#34;&gt;recommendation system&lt;/a&gt; at &lt;a href=&#34;/archives/globocom/&#34;&gt;Globo.com&lt;/a&gt;. I studied this course to understand this field, and I consider it a must for anyone working with recommendation systems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/game-theory-1&#34;&gt;Game Theory&lt;/a&gt; by Stanford: I took this course purely out of curiosity and don’t regret it. The mathematical knowledge I gained frequently helps me think probabilistically and even design proper incentives and disincentives for features that go beyond data science.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/projects/statistics-data-science&#34;&gt;Statistics for Data Science&lt;/a&gt; by Project Network: I enrolled in this course to deepen my statistical knowledge, which is essential for better understanding model results, research evaluations, and analyses. I constantly apply what I learned here.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/account/accomplishments/specialization/F8SQK79VWVJ2&#34;&gt;Mathematics for Machine Learning&lt;/a&gt; by Imperial College: I took this course to deepen my understanding of the mathematics behind the models I had been using for some time. It was particularly important for understanding the workings of deep learning models and recent LLMs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Equally important, below is a list of the most significant books in my data science journey:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vwang0/recommender_system/blob/master/Recommender%20Systems%20Handbook.pdf&#34;&gt;Recommender System Handbook&lt;/a&gt;: To delve deeper into the concepts of recommendation systems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/relevant-search&#34;&gt;Relevant Search&lt;/a&gt;: A prerequisite for anyone working with search systems.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rj.olx.com.br/rio-de-janeiro-e-regiao/livros-e-revistas/livro-generative-deep-learning-de-david-foster-1018413877?&#34;&gt;Generative Deep Learning&lt;/a&gt;: Essential for understanding the inner workings of new generative algorithm families.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com.br/Small-Data-Indicam-Grandes-Tend%C3%AAncias/dp/8569809719&#34;&gt;Small Data&lt;/a&gt;: Offers an opposing perspective on relying solely on big data for decision-making, showing how anecdotal examples can provide game-changing insights.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com.br/gp/product/B077Q3VZFR/&#34;&gt;Natural Language Processing with TensorFlow&lt;/a&gt;: Although I prefer PyTorch, this book is very interesting as it explains various text processing methods for machine learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope this list of books and courses helps you dive deeper into a new field, whether to work directly in it, as I do, or to simply bridge the communication gap that a lack of knowledge might cause when discussing with specialists.&lt;/p&gt;

&lt;p&gt;But here&amp;rsquo;s a warning: These courses and books were essential, but what truly made me learn was practice and experience. Experiments, mistakes, and exchanges of ideas with the excellent professionals I had the privilege of encountering throughout my life were the real drivers of my knowledge.&lt;/p&gt;

&lt;p&gt;And of course, remember: when starting to study, you may fall prey to the &lt;a href=&#34;https://thedecisionlab.com/biases/dunning-kruger-effect&#34;&gt;Dunning-Kruger Effect&lt;/a&gt;. It happened to me, and it can happen to you too, so be humble.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>No one cared about Multi Armed Bandit’s tech</title>
            <link rel="alternate" href="/work/globocom/recommendation/no-one-cared-about-multi-armed-bandit-tech/" type="text/html" />
            <id>/work/globocom/recommendation/no-one-cared-about-multi-armed-bandit-tech/</id>
            <updated>2024-09-28T10:56:00Z</updated>

            <summary type="html">In mid-2017, inspired by the book &amp;ldquo;Bandit Algorithms for Website Optimization&amp;rdquo; by John Myles White, I decided to implement a service allowing Globo.com products to use Multi Armed Bandit algorithms to optimize their usability.</summary>
            <content type="html">&lt;p&gt;In mid-2017, inspired by the book &amp;ldquo;Bandit Algorithms for Website Optimization&amp;rdquo; by John Myles White, I decided to implement a service allowing Globo.com products to use Multi Armed Bandit algorithms to optimize their usability.&lt;/p&gt;

&lt;p&gt;The algorithms explained in the book are relatively simple. Epsilon-Greedy, Softmax, and UCB are simple statistical calculations that do not require a great processing power or historical storage. So technically it would be possible to aggregate the necessary information in the memory of the service itself, avoiding any IO, enabling the service to have very low latency.&lt;/p&gt;

&lt;p&gt;To keep everything in memory and maintain consistency, a first alternative would be to handle all requests on a single server scaled vertically. We know how dangerous this is in terms of resilience. So, a second alternative would be to have multiple servers behind a load balancer, reducing the consistency of aggregations slightly, and relying on the fact that the site&amp;rsquo;s huge audience will bring the numbers recorded on each server closer together.&lt;/p&gt;

&lt;p&gt;These two possible alternatives would also require that the servers be pre-scaled for the most critical peak moments of the portal. At dawn, for example, when there were very few accesses, we would continue with the same cost as the periods of the most absurd traffic. Something we do not desire.&lt;/p&gt;

&lt;p&gt;The ideal alternative would be to try to use concepts like autoscaling and multiple containers, which are already default decisions in the deployment of new services today. At Globo.com, we already had an internal PaaS called &lt;a href=&#34;https://tsuru.io/&#34;&gt;Tsuru&lt;/a&gt;, which is open source, and I have not seen anything better in any other company to this day.&lt;/p&gt;

&lt;p&gt;However, for a service that is not stateless, the autoscaling process can be painful.&lt;/p&gt;

&lt;p&gt;Imagine the situation where we have 20 pods in the air serving requests, all of them already have their aggregated numbers stabilized and generating the best results for each experiment. Then a peak time starts, new pods are allocated, and we go to 30 pods. At this peak moment, a very important moment for the site, 33% of requests no longer have the most optimized version in terms of results.&lt;/p&gt;

&lt;p&gt;In other words, the regret metric we aim to minimize does not get reduced to the desired extent in the case of a stateful service with autoscaling.&lt;/p&gt;

&lt;p&gt;The solution found was to create a periodic sync of each service instance with a common Redis. So, in addition to storing in the service&amp;rsquo;s memory the total number of events and the total number of successes for each experiment, we also started storing in the service&amp;rsquo;s memory the values found in the common memory represented by Redis. In this sync, the service increment Redis keys with the difference between the current state and what was in the last sync, updating the common memory. After this, the sync gets those keys from Redis updating its internal states. Always using atomic variables.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/mab.png&#34; alt=&#34;/assets/images/mab.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;With this, when a new pod is created due to auto scaling, we start it reading those keys from redis, so it is born with the necessary aggregated data for a more optimal choice that reduces regret for each experiment.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s been over seven years and I know that Multi Armed Bandit is still used at Globo.com because it is possible to observe the service requests when accessing the Portal. However, I believe that the service may have been completely modified by now. It was initially created in single day during a hackday, written using Scala and Finagle, and after that was used for various optimizations at Globo.com, from choosing a photo to selecting the best position to place advertising.&lt;/p&gt;

&lt;p&gt;When I presented it at hackday, I wanted to emphasize this technological solution involving low latency and scalable eventual consistency for its operation as the interesting part of the project. In my mind, the calculations were so simple that they didn&amp;rsquo;t even deserve much attention because it is very simple to replicate them either all in memory, or by spending latency using IO massively.&lt;/p&gt;

&lt;p&gt;However, the technological aspect of this solution did not receive the attention that I thought it deserved. What most sparked interest, obviously, were the expectations of use for improving usability that such a project could bring to users and all its possibilities. In other words, no one cares about the technological solution explicitly, but implicitly, they were crucial for maximizing results, minimizing regret in periods of auto scaling, reducing costs, and reducing latency.&lt;/p&gt;

&lt;p&gt;I like to believe, in the depths of my being, that without this technological solution, the project would not have been used by so many products with as much success as it was. But is that really true?&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Delaying the launch of a model inspired by Turing</title>
            <link rel="alternate" href="/work/olxbrasil/trustandsafety/delaying-launch-model-inspired-by-turing/" type="text/html" />
            <id>/work/olxbrasil/trustandsafety/delaying-launch-model-inspired-by-turing/</id>
            <updated>2024-08-03T10:56:00Z</updated>

            <summary type="html">This is the story of how we built a machine learning model factory, and inspired by the movie &amp;ldquo;The Imitation Game,&amp;rdquo; we decided not to launch it until another change was made. This second change had such a positive outcome that the factory achieved a much lower result than we had originally programmed, but without it, we would not have completed the journey. This case occurred between the first and second quarter of 2023.</summary>
            <content type="html">&lt;p&gt;This is the story of how we built a machine learning model factory, and inspired by the movie &amp;ldquo;The Imitation Game,&amp;rdquo; we decided not to launch it until another change was made. This second change had such a positive outcome that the factory achieved a much lower result than we had originally programmed, but without it, we would not have completed the journey. This case occurred between the first and second quarter of 2023.&lt;/p&gt;

&lt;p&gt;The issue we needed to resolve was the high number of ads for prohibited items being posted on the OLX Brazil platform. Prohibited items are products whose sale is banned, controlled, requires special permission, or are the result of piracy. Examples of prohibited items would be weapons, gas cylinders, wild animals, counterfeit brand clothing, streaming accounts, among others.&lt;/p&gt;

&lt;p&gt;At that time, there was already a structure in place to prevent the entry of such ads through a huge set of regexes that were constantly being incremented. These regexes moved the ads to a manual review queue, and a team of operators analyzed each ad identified by those regexes. The work of these operators was increasing day by day, and we needed to do something to reduce this manual verification.&lt;/p&gt;

&lt;p&gt;The idea was to create a machine learning model to identify a certain percentage of ads with a high degree of certainty to automatically approve or reject it. This idea came from a hackday held by the squad to evaluate if for some group of regexes it would be possible to make a simple model that identified a good percentage with high certainty (&amp;gt;=99% precision).&lt;/p&gt;

&lt;p&gt;A group of regex would be a list of regex that already existed in the old process and that identify a type of prohibited item. We had years of history in the datalake of regexes that matched and whether the ad in question had been approved or rejected by the operators. As the model made on hackday worked, we decided to implement a POC with the evaluated group of regexes.&lt;/p&gt;

&lt;p&gt;In the middle of the implementation, we realized the opportunity, if we left the model code generic enough and with good guardrails for threshold verification to obtain high precision in cross-validation, we could make a &amp;ldquo;factory&amp;rdquo; of models.&lt;/p&gt;

&lt;p&gt;We then created a command line that receive as parameter a group of regexes. The command downloads and prepares the dataset separating it into training and testing, optimizes the model&amp;rsquo;s hyperparameters using cross-validation on the training dataset trying to reach thresholds with high precision &amp;gt;=99% for refusal and for acceptance. Then it is validated on the test dataset if these same thresholds would achieve the same precision. If yes, the model is used, if not, the model is discarded for this group of regexes.&lt;/p&gt;

&lt;p&gt;With this process, we quickly generated 26 models, with an estimate of reducing 5% of the operators&amp;#39; work due to the automatic decision. We put the model into production but without really making the decision, just to observe if the good precision result in offline evaluations would hold up in production, and it was also a success.&lt;/p&gt;

&lt;p&gt;Our drama then began when we realized that if we really let the models make decisions, users would quickly learn how to escape, and the labor savings of operators would not be consummated. This is because the operator&amp;rsquo;s decision tends to be slower, and less deterministic, than that of the model. This prevents the user from testing ways to circumvent. Quick feedback of refusal or approval could cause even more work for the operators.&lt;/p&gt;

&lt;p&gt;In the meetings where we talked about this, with all that pressure to put it into production soon, we cited the movie &amp;ldquo;The Imitation Game,&amp;rdquo; where Turing mentioned a similar strategy. The &amp;ldquo;enemy&amp;rdquo; could not perceive what was happening. A simple idea would be to cause a delay in the model&amp;rsquo;s response. But another idea was hotter and more promising in our minds, something we had been ventilating for a long time: The temporary blocking of the user from posting ads according to the number of &amp;ldquo;fouls&amp;rdquo; he committed, with possible progression.&lt;/p&gt;

&lt;p&gt;The complexity of this change came from the fact that we should create communications explaining what was happening, both for the moment of the event and for the moment of trying to post an ad (or login). Besides the analysis to define the ideal time between one attempt and another, how much ideal blocking time, as well as the natural work of the task, for state storage and expiration of the time window of attempts and interventions in the necessary places to prevent the temporarily blocked user from acting.&lt;/p&gt;

&lt;p&gt;At this moment when we were determined to bet on this solution, inspired by Turing, I was already leaving the squad to a mission in the search squad. I participated only in the debates so that this would be the solution adopted before enabling the prohibited items model, and also the initial architecture discussions. The squad only had top developers, so they quickly put the solution into production and we had an incredible result.&lt;/p&gt;

&lt;p&gt;The solution brought a &lt;strong&gt;48%&lt;/strong&gt; reduction in the index of prohibited items, which greatly reduced the work of the operators for this specific problem, and consequently made the model of prohibited items, later turned on, have a lower result than we initially expected: Still making decisions in &lt;strong&gt;~5%&lt;/strong&gt; of the cases, but from many fewer cases. Interestingly, the precision of the model, one year later, continues at &lt;strong&gt;~99%&lt;/strong&gt;, probably due to the strategy that does not let the user adapt.&lt;/p&gt;

&lt;p&gt;The people envolved in the development of both solutions were: &lt;a href=&#34;https://www.linkedin.com/in/timotta/&#34;&gt;me&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/let%C3%ADciagarcez/&#34;&gt;Leticia Garcez&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/robson-rocha-512a5b16/&#34;&gt;Robson Rocha&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/carvalho-lucas/&#34;&gt;Lucas Carvalho&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/victorseidl/&#34;&gt;Victor Lyra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/leonardopereiracampos/&#34;&gt;Leonardo Campos&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/geusandre/&#34;&gt;Andre de Geus&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nataliakarmierczak&#34;&gt;Natalia Maria Karmierczak&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/daniel-freitas-7487b5136/&#34;&gt;Daniel Freitas&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/debora-algamis-jansen-273187106/&#34;&gt;Debora Jansem&lt;/a&gt; e &lt;a href=&#34;https://www.linkedin.com/in/deborah-cordeiro-568298121/&#34;&gt;Débora Cordeiro&lt;/a&gt;. An amazing group of people.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/turing.png&#34; alt=&#34;/assets/images/turing.png&#34;&gt;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Podium: The algorithm that defeated BM25</title>
            <link rel="alternate" href="/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/" type="text/html" />
            <id>/work/olxbrasil/search/podium-the-algorithm-that-defeated-bm25/</id>
            <updated>2024-06-08T10:56:00Z</updated>

            <summary type="html">This article explains Podium, a search relevance algorithm created by the OLX Brazil Search team to replace the globally renowned BM25. It resulted in increased clicks, contact intentions, and reduced complaints about search quality, and was launched in April 2021.</summary>
            <content type="html">&lt;p&gt;This article explains Podium, a search relevance algorithm created by the OLX Brazil Search team to replace the globally renowned BM25. It resulted in increased clicks, contact intentions, and reduced complaints about search quality, and was launched in April 2021.&lt;/p&gt;

&lt;p&gt;In previous years, our search team at OLX has tried various approaches to optimize search and bring more accurate results. With each change, there was an observed improvement in short-term AB testing, but in the medium term, advertisers adapted to increase impressions of their ads by exploiting algorithm loopholes, consequently worsening the accuracy of the results.&lt;/p&gt;

&lt;p&gt;To make this clearer, I will describe below two examples of changes that were made and caused harmful reactions from advertisers:&lt;/p&gt;

&lt;p&gt;Example 1: At some point, we began to give more importance to the brand and model attributes configured by the advertiser. Therefore, an advertisement for a cellphone marked with the brand &amp;ldquo;Samsung&amp;rdquo; and the model &amp;ldquo;Galaxy&amp;rdquo; would have a better chance of being found in a search for &amp;ldquo;Samsung Galaxy&amp;rdquo; than one that was not configured this way.&lt;/p&gt;

&lt;p&gt;In AB test, this experiment brought gains, but after the rollout, advertisers noticed, and began advertising smartphones with different brands in the most searched brands. So, his ad started to appear in two different searches. In addition, advertisers of services and jobs started placing their ads in other categories that contained the brand and model attributes to appear in searches for the service title as well as in random searches.&lt;/p&gt;

&lt;p&gt;Example 2: In another period, through a precision analysis using Bayesian optimization, we defined automatic weights for each attribute of the ad. It so happened that the title obviously had a higher score. Some months after the AB test, advertisers realized that if they repeated the most important words in the title, it would have a higher chance of being elevated to the top of the search results, because of the BM25.&lt;/p&gt;

&lt;p&gt;In this way ads like &amp;ldquo;freight freight freight&amp;rdquo; and &amp;ldquo;invicta watch invicta watch invicta watch&amp;rdquo; were always at the top of their respective searches, disturbing the lives of buyers and advertisers less focused on SEO at OLX. As a temporary solution, we penalized ads with duplicate terms and standardized the internal title used in the search. However, there was still a loophole for cases of derived words, which internally in Elastic Search were considered the same word due to Stemming, and the search result become boring with a lot of ad with the same title.&lt;/p&gt;

&lt;p&gt;These were just two examples of several of our attempts to optimize the search of OLX Brazil using BM25.&lt;/p&gt;

&lt;p&gt;However, they demonstrate that we would never achieve a good result with BM25, as the mathematical intuition behind this algorithm does not make sense for the C2C marketplace, due to two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the numerator of BM25 there is the quantity of matches of the searched words.&lt;/li&gt;
&lt;li&gt;In the denominator, one of the components is the size of the document.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That is, BM25 always favors smaller documents with the highest number of matches. In this case, if a search for &amp;ldquo;bicycle&amp;rdquo; is done on a C2C marketplace, the ads with the highest match will be those whose title is simply &amp;ldquo;bicycle&amp;rdquo;. And if the ad description is included in the calculation, the highest matches will be those with the fewest words in that description and with the most repetitions of the word. In other words, the least informative, least creative, and poorest ads in terms of information will have the best score.&lt;/p&gt;

&lt;p&gt;During this period when we were studying a way to improve search, we were in love to an &lt;a href=&#34;https://pages.cs.wisc.edu/%7Eanhai/papers/chimera-vldb14.pdf&#34;&gt;Wallmart paper&lt;/a&gt; about product classification, and based on it we created a system called Guanabara (in honor to a famous supermarket in Rio de Janeiro). One of the ideas from the paper is a hybrid system of regexes, machine learning models, and human in the loop to identify products.&lt;/p&gt;

&lt;p&gt;While we were creating the regexes for this system, we noticed that there was a pattern in the title of the ads that we could observe for portuguese language. Ignoring some irrelevant words at the beginning of each title, such as &amp;ldquo;vendo&amp;rdquo;, &amp;ldquo;novo&amp;rdquo;, &amp;ldquo;lindo&amp;rdquo; among others, the order of the words indicated the real relevance of an ad for a certain term, in portuguese.&lt;/p&gt;

&lt;p&gt;Ads like &amp;ldquo;Vendo Samsung Galaxy&amp;rdquo;, &amp;ldquo;Samsung Galaxy s10&amp;rdquo; and &amp;ldquo;Samsung galaxy novo&amp;rdquo;, should have the same relevance to each other, but a higher relevance than &amp;ldquo;Fone de Samsung galaxy&amp;rdquo; in a search for &amp;ldquo;Samsung galaxy&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;And so the idea arose: What if we gave a higher score to matches based on the position of the term in the ad and the position of the term in the searched text?&lt;/p&gt;

&lt;p&gt;Based on this idea, we created the Podium algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/podium-math.gif&#34; alt=&#34;/assets/images/podium-math.gif&#34;&gt;&lt;/p&gt;

&lt;!-- \sum_{k&lt;3}^{k=0}\sum_{i&lt;3}^{i=0} match(P_i,Q_k) * S/2^^{i+k} --&gt;

&lt;p&gt;Where &lt;strong&gt;P&lt;/strong&gt; is the list of the first three relevant words from the ad, and &lt;strong&gt;Q&lt;/strong&gt; is the list of the first three relevant words from the search made, &lt;strong&gt;S&lt;/strong&gt; is an initial higher score (currently at 200k) and &lt;strong&gt;match&lt;/strong&gt; is a function that returns 1 or 0 if the terms are equal.&lt;/p&gt;

&lt;p&gt;This is a base calculation. The other texts from the ad and the query are also used, but with lower scores and not as important for the intuition behind the algorithm. In addition, the terms in the ad and the query are deduplicated through stemming in advance.&lt;/p&gt;

&lt;p&gt;One of the most important things about this algorithm is that it purposely causes ties in ads where the accuracy between the search terms and the terms present are the same. So in the example given, a search for &amp;ldquo;Samsung Galaxy&amp;rdquo;, the ads &amp;ldquo;Vendo Samsung Galaxy&amp;rdquo;, &amp;ldquo;Samsung Galaxy s10&amp;rdquo; and &amp;ldquo;Samsung Galaxy novo&amp;rdquo; all have the same score, allowing us to reorder them by recency, which is another very important factor in a C2C marketplace.&lt;/p&gt;

&lt;p&gt;Before that, with BM25, to achieve good recency in the results, we would apply a score decay function relative to how old the ad was. This decay often caused a loss of precision, especially in cases of products with low inventory. A classic example we used to demonstrate the problem was searching for &amp;ldquo;torno mecânico&amp;rdquo; in a certain region there were only 2 &amp;ldquo;torno mecânico&amp;rdquo; ads, but they were not recent. When searching for it with BM25 plus the decay function, these ads would appear in the last results.&lt;/p&gt;

&lt;p&gt;With Podium forcing ties in similar matches, we achieved a good balance between recency but only in ads with good accuracy, without needing a decay function.&lt;/p&gt;

&lt;p&gt;With that, our qualitative &lt;a href=&#34;https://medium.com/grupoolxtech/uma-nova-m%C3%A9trica-para-calcular-relev%C3%A2ncia-de-busca-65372f154f8f&#34;&gt;R-DCG, a ranking metric we have explained in a previously post,&lt;/a&gt; increased by &lt;strong&gt;3.8%&lt;/strong&gt;. And this result was accompanied by a &lt;strong&gt;3.6%&lt;/strong&gt; increase in contact intentions and a &lt;strong&gt;2.9%&lt;/strong&gt; increase in clicks on the top 5 positions of textual searches.&lt;/p&gt;

&lt;p&gt;And most importantly, after years, advertisers have not been able to find a relevant loophole in Podium to increase their impressions and disturb OLX&amp;rsquo;s search accuracy. It is still possible to do so through tags in ad descriptions, which increases impressions in case of low inventory in specific locations, but since there was already no inventory, it is not a major concern.&lt;/p&gt;

&lt;p&gt;It is worth noting that after Podium went live, several search improvements were implemented, including the inclusion of synonyms, spellers and much more.&lt;/p&gt;

&lt;p&gt;More recently we lauched a new version of Podium, the Podium-N. In this update, we solved the issue of ads that should have the same relevance but didn&amp;rsquo;t with the original Podium. Example: &amp;ldquo;Vendo samsung galaxy&amp;rdquo; and &amp;ldquo;Vendo galaxy da samsung&amp;rdquo; for the search &amp;ldquo;Galaxy&amp;rdquo;. But that&amp;rsquo;s for another post.&lt;/p&gt;

&lt;p&gt;In addition, we are finalizing the integration of semantic search together with Podiun-N, in a hybrid search, which is already showing great results. But that will also be discussed in a future post.&lt;/p&gt;

&lt;p&gt;Podium was a collaborative solution made by &lt;a href=&#34;https://www.linkedin.com/in/timotta/&#34;&gt;me&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/leonardowajnsztok/&#34;&gt;Leonardo Wajnsztok&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/filipecasal/&#34;&gt;Filipe Casal&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/daniel-correa-araujo/&#34;&gt;Daniel Araújo&lt;/a&gt;. The engineering manager at the time was &lt;a href=&#34;https://www.linkedin.com/in/rogeriofrodrigues/&#34;&gt;Rogério Rodrigues&lt;/a&gt; followed by &lt;a href=&#34;https://www.linkedin.com/in/gumaestri/&#34;&gt;Gustavo Maestri&lt;/a&gt; and the product manager and who gave the name for the child was &lt;a href=&#34;https://www.linkedin.com/in/pedrotangari/&#34;&gt;Pedro Tangari&lt;/a&gt;. Please accept my apologies if I inadvertently leave anyone out. It happened three years ago&amp;hellip;&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Random recommendation due to concurrency</title>
            <link rel="alternate" href="/work/globocom/recommendation/random-recommendation-due-to-concurrency/" type="text/html" />
            <id>/work/globocom/recommendation/random-recommendation-due-to-concurrency/</id>
            <updated>2024-03-31T10:56:00Z</updated>

            <summary type="html">This is the story of how a simple mitigation solution led us to discover the root cause of a difficult-to-replicate bug, originating from a lack of knowledge about a technology.</summary>
            <content type="html">&lt;p&gt;This is the story of how a simple mitigation solution led us to discover the root cause of a difficult-to-replicate bug, originating from a lack of knowledge about a technology.&lt;/p&gt;

&lt;p&gt;When I started in the Recommendation System team at Globo.com in 2014, there was already a system in production that was used to recommend other articles on the side of the article screen. It was initially used on only one of Globo.com&amp;rsquo;s sites and was about to be used on another one as well.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t exactly remember which sites they were, but let&amp;rsquo;s say initially it was G1, a news website, and it was about to be adopted on GE, a sports website. It was at this moment of adoption that we started receiving sporadic complaints about GE articles being recommended on G1 and vice versa. This was a huge problem, there couldn&amp;rsquo;t be cross-recommendation between products at that moment, and the fact that this bug was occurring was a risk to the continuity of the recommendation systems project.&lt;/p&gt;

&lt;p&gt;However, we were unable to consistently replicate the issue. Every now and then, someone on the team would be able to witness it, proving that yes, it was not just a hallucination.&lt;/p&gt;

&lt;p&gt;To quickly mitigate the issue, we thought of a simple solution, but before explaining this solution, it is important to understand how the first version of recommendation on Globo.com worked.&lt;/p&gt;

&lt;p&gt;There was an API written in Ruby, which in the request made by the client of the article, searched in a Mongo DB for the IDs of articles recommended to a user for a specific website. This personalized recommendation was generated in a periodic batch using Mahout on Hadoop. These article IDs were sent to a Solr Handler along with the ID of the context article where the recommendation would be presented. In this Solr handler, a More Like This query was executed to find similar articles to the context article, and these two lists of articles were then combined for the recommendation return. The diagram below demonstrates how each system interacts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/assets/images/recommendation-old.png&#34; alt=&#34;/assets/images/recommendation-old.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;In other words, it was a mixture of personalized recommendation through collaborative filtering calculated by Mahout and contextual recommendation by similarity calculated by Solr. And the customized Solr Handler combined both lists.&lt;/p&gt;

&lt;p&gt;Looking at the Solr Handler code, there didn&amp;rsquo;t seem to be any issue with the More Like This query that could cause a wrong return of articles from another site. Also, in the client we didn&amp;rsquo;t find any sign of a problem with sending the site ID incorrectly. In the Ruby API, also quite simple, there didn&amp;rsquo;t seem to be any issues with querying the Mongo DB with the site ID or passing this ID to the Solr Handler.&lt;/p&gt;

&lt;p&gt;Then we were left with the calculation of the personalized recommendation, whose code was a little more complex to understand, involving several codes in PIG and some Map Reduces for Mahout use. But it didn&amp;rsquo;t make sense for that to be the problem, as the execution of this batch process took about two hours, and when the sporadic case occurred, we would look at the key related to the user/site on the Mongo DB and not find this inconsistency.&lt;/p&gt;

&lt;p&gt;Without any tricks up our sleeve, we decided to implement the simple solution I mentioned: Strengthening the filtering in the Solr Handler so that after combining the two algorithms, no content from another website would be returned. The idea was to stop the bleeding, and then try to better understand the root of the problem.&lt;/p&gt;

&lt;p&gt;The solution was deployed and during the deploy we paid more attention to the Solr logs and found sporadic errors in the use of the SimpleDateFormat class. This class was used in this Solr Handler to format the final result of the articles. Looking at the code, it was possible to notice a possible source for this specific problem and a clue of what could be happening to cause the original problem.&lt;/p&gt;

&lt;p&gt;SimpleDateFormat is not threadsafe, so it is ideal not to use global instances. In this Solr Handler, SimpleDateFormat was not instantiated statically, which is a good sign. However, it was instantiated as an attribute of the class. This means that each instance of the Handler would have its own SimpleDateFormat. If each request instantiated its handler, we would not have the concurrency issue.&lt;/p&gt;

&lt;p&gt;But we were having. Reading the documentation, it became clear that the Solr Handler was not generating an instance per request. So in practice, the SimpleDateFormat was being reused in several requests causing the sporadic errors we observed in the logs. Another attribute that was inadvertently in the same situation was the list of documents to be returned in the request.&lt;/p&gt;

&lt;p&gt;Gotcha! That&amp;rsquo;s when you have finally caught the source of the problem!&lt;/p&gt;

&lt;p&gt;During the request, when the combination of the return of More Like This occurred with the personalized recommendation sent via parameter, if another request occurred at the same time, the recommendations from different people and sites were combined.&lt;/p&gt;

&lt;p&gt;It was possible to replicate the problem locally by adding sleeps to force slowness and then execute simultaneous requests. The final solution was to remove these instance attributes and only use them locally in the methods.&lt;/p&gt;

&lt;p&gt;This demonstrates the importance of understanding the system in which you are working, especially when working with multi-threaded systems.&lt;/p&gt;

&lt;p&gt;This reminded me of a problem in the past also at Globo.com, but in this case caused by me: I used a HashMap as cache in a multithread service, and this caused several locks in the application, for the same reason: HashMap is not thread safe. The problem was only identified with thread dumps and the solution was to use a ConcurrentHashMap instead.&lt;/p&gt;

&lt;p&gt;Finally, the lesson is: There is no doubt that it is easier to learn by doing, but to put it into production, it is important to carefully read the specifications of the libraries, frameworks, and environments we are using. And in case of a difficult analysis problem, it is worth trying simple and potentially ineffective solutions, because that may lead you to a greater understanding of the real problem.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The rise, peak and downfall of Web Democracy</title>
            <link rel="alternate" href="/work/personal/webdemocracy/the-rise-peak-and-downfall-of-web-democracy/" type="text/html" />
            <id>/work/personal/webdemocracy/the-rise-peak-and-downfall-of-web-democracy/</id>
            <updated>2023-12-26T10:56:00Z</updated>

            <summary type="html">This is the story about the rise, peak, and downfall of a politicians&amp;#39; rating app that I created and maintained for years as a pet project, along with an explanation of why it was shut down.</summary>
            <content type="html">&lt;p&gt;This is the story about the rise, peak, and downfall of a politicians&amp;#39; rating app that I created and maintained for years as a pet project, along with an explanation of why it was shut down.&lt;/p&gt;

&lt;p&gt;It all began in 2008, when Orkut was the most used social network in Brazil and it had started to release an Open Social API for application integration. I then had the idea of creating an app where you could rate and discuss Brazilian politicians.&lt;/p&gt;

&lt;p&gt;The app was simple, a list of elected politicians and candidates with information gathered through web crawlers on the internet itself, a positive and negative evaluation, and a comments section. This would generate lists of the best, worst, and most controversial politicians.&lt;/p&gt;

&lt;p&gt;And with the open social API, it would still be possible to find friends who are most politically similar to you. I actually didn&amp;rsquo;t end up implementing this part because, as mentioned, it was a pet project and I didn&amp;rsquo;t dedicate a considerable amount of time to it.&lt;/p&gt;

&lt;p&gt;At a certain point, Orkut began to decline in Brazil with the growth of Facebook. In this other social network, there was already the possibility of creating applications, and I spent some time adapting Web Democracy to work on both, with Open Social and Facebook. However, the Facebook application API was very unstable, it changed a lot, and it did not bring as much return as the application on Orkut.&lt;/p&gt;

&lt;p&gt;So after changing the application about three times to adapt it to Facebook, I decided to extract Web Democracy from the social networks and turn it into a website. It was a good decision. The number of accesses started to grow again due to the good implemented SEO, and it remained like that for a long time until it was shut down.&lt;/p&gt;

&lt;p&gt;During this period, interesting things happened.&lt;/p&gt;

&lt;p&gt;In a radio station in the countryside of Minas Gerais, a politician was questioned about a low rating on Web Democracy, and his response was an accusation that the application had been bought by his opponents.&lt;/p&gt;

&lt;p&gt;Another politician threatened to sue me through a Facebook message if I didn&amp;rsquo;t take down his profile because it would be defamation. The interesting thing is that this politician was highly rated on the app. I explained this to him and removed him from the platform. Then he asked to come back and I refused.&lt;/p&gt;

&lt;p&gt;There was a period when leaders of a Brazilian liberal movement reached out to me to have the website ally with them. I refused because the concept of Web Democracy was to be a neutral platform without ideologies.&lt;/p&gt;

&lt;p&gt;During the time I was on the air, I was starting a recommendation systems course, so I implemented a Content-Based recommendation for the website that had a good result in increased engagement, and I even released the data to the recommendation systems community: &lt;a href=&#34;http://programandosemcafeina.blogspot.com/2015/11/dados-abertos-do-web-democracia.html&#34;&gt;http://programandosemcafeina.blogspot.com/2015/11/dados-abertos-do-web-democracia.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At some point, I tried to create new features for the application, such as the ability to create petitions, promote them, and evaluate them. However, this tool did not gain much success.&lt;/p&gt;

&lt;p&gt;And then you ask me: With so much fun, why did the Web Democracy fall?&lt;/p&gt;

&lt;p&gt;Web Democracy was developed using Ruby on Rails. Since I didn&amp;rsquo;t work regularly on this project, every time I came back to develop something it was a challenge. This was because it was always necessary to update some Ruby library, whose ecosystem at the time ignored the concept of compatibility between versions.&lt;/p&gt;

&lt;p&gt;I had even implemented a very comprehensive test suite with 100% coverage, however, whenever there was an update of rspec, cucumber, rails, or any other dependency of these libraries, even the test code would fail. This caused me an incredible laziness to work on the project.&lt;/p&gt;

&lt;p&gt;Another technical difficulty was the frequent updating of politicians that needed to be done. With every new pre and post election, a new crawler had to be made because the source websites would change. It always required effort to bypass captchas, session cookies, and created bursts for protection.&lt;/p&gt;

&lt;p&gt;Not to mention that, as I wouldn&amp;rsquo;t want to lose the old assessments and comments or duplicate politicians, I needed to create a match between them. Notice that the sources of information didn&amp;rsquo;t always have the CPF (Brazilian taxpayer ID), so the match wasn&amp;rsquo;t that simple: Names changed, nicknames changed, parties changed, and even states changed.&lt;/p&gt;

&lt;p&gt;I even developed a heuristic that gave me 100% precision in the match, but with low recall. To deal with uncertain cases, I created an annotation interface for the possible politicians that would be a match and manually did it myself. It was quite a tedious task.&lt;/p&gt;

&lt;p&gt;In a certain year, which I can&amp;rsquo;t quite remember, laziness got the best of me and I stopped doing these updates, and as a result, the Web Democracy platform became outdated. Additionally, Google&amp;rsquo;s algorithm changed during this period, further deprioritizing repeated content, thus limiting the app&amp;rsquo;s access even more.&lt;/p&gt;

&lt;p&gt;With very few monthly visits, outdated and only burdening me in hosting, I decided to kill the project for good in 2022. Occasionally, I still visit the comments and suggestions page of Web Democracy with a bittersweet nostalgia for a project that brought so much joy and sadness: &lt;a href=&#34;http://programandosemcafeina.blogspot.com/2008/08/web-democracia-criticas-e-sugestoes.html&#34;&gt;http://programandosemcafeina.blogspot.com/2008/08/web-democracia-criticas-e-sugestoes.html&lt;/a&gt;.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>How I invented Ajax despite it already existing</title>
            <link rel="alternate" href="/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/" type="text/html" />
            <id>/work/indata/angaturama/how-i-invented-ajax-despite-it-already-existing/</id>
            <updated>2023-11-15T10:56:00Z</updated>

            <summary type="html">This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.</summary>
            <content type="html">&lt;p&gt;This is the story of how I invented Ajax (Asynchronous JavaScript + XML) despite it already existing, and how a junior developer would greatly benefit from mentorship at the beginning of their career.&lt;/p&gt;

&lt;p&gt;Between 2003 and 2004, I worked for a very small software company. There were only three developers, and each one worked separately on a group of softwares, as if they were three one-person teams. I had little experience and had recently graduated, and I also did not have the habit of reading books and articles to learn. All my learning until then was based on empirical experimentation, solving real problems from personal projects and the companies I was working for.&lt;/p&gt;

&lt;p&gt;At that time, the majority of websites and web applications would reload the entire pages every time a click or form submission occurred. And for me, that was indeed the only way to exchange information between the browser and the server. My knowledge was limited to that.&lt;/p&gt;

&lt;p&gt;However, one of the projects at Indata involved customizing a GIS project from Mexico. In this project, I noticed that information was loaded on the page without having to reload the entire page. I found that amazing, a revolution within my limited knowledge. I needed to learn that!&lt;/p&gt;

&lt;p&gt;Notice, at that time we didn&amp;rsquo;t have Chrome&amp;rsquo;s Web Developer Tools. We used Internet Explorer to debug websites. JavaScript errors were masked, there was no way to inspect anything. Additionally, the JavaScript code on the client side of this software was minified.&lt;/p&gt;

&lt;p&gt;Reading line by line of that obfuscated code I found the so-called XMLHttpRequest object where the magic was happening. But I didn&amp;rsquo;t understand anything. To me, that was the end of the execution line. There was nothing more after it.&lt;/p&gt;

&lt;p&gt;I couldn&amp;rsquo;t notice the callback that would be executed through it. It wasn&amp;rsquo;t due to a total unawareness of the pattern, as at that time I had already developed some games in Java that used listeners in response to user actions, a similar pattern, but due to my lack of practice, I couldn&amp;rsquo;t associate it.&lt;/p&gt;

&lt;p&gt;Nowadays, I try to understand why I couldn&amp;rsquo;t associate one thing with another, and my only hypothesis is that the abstraction of a callback for user action made sense to me at the time, as it is an external interruption to the code&amp;rsquo;s execution flow, while the callback within the execution flow didn&amp;rsquo;t make sense in my limited brain at that time.&lt;/p&gt;

&lt;p&gt;Also, Google was not very popular in Brazil yet, and all search tools (in Portuguese because at the time I didn&amp;rsquo;t have experience in other languages either) were directories and did not search internally within websites and texts. Neither did my colleagues at the company know that technology. So, for some time, I was stuck without understanding how that GIS software could load data from the server without reloading the page.&lt;/p&gt;

&lt;p&gt;So one day I remembered an HTML tag that I rarely used, but had learned in an online HTML and JavaScript course: the iframe. I noticed that with the iframe, I could communicate with the parent using JavaScript and exchange information. This was the solution I needed.&lt;/p&gt;

&lt;p&gt;With iframe, I would be able to retrieve information from the server and populate it in the parent through a function call, sending the data as a parameter. In fact, it was the same callback pattern used by XMLHttpRequest, but in this case, it made sense to me, as it was an interruption coming from another page being loaded.&lt;/p&gt;

&lt;p&gt;At the time, I was developing Angaturama, a receptive tourism company management system. I started using the &amp;ldquo;Ajax&amp;rdquo; technique through iframes, and as a result, the system became very user-friendly. We even created a shortcut key to display hidden iframes that were hidden by CSS containing the results for debugging purposes.&lt;/p&gt;

&lt;p&gt;A few years after I left the company, Google started to become more popular in Brazil. I learned English, developed the habit of reading books, technical articles, and participating in software discussion groups. During this period, I began to notice that there was a trending term called &amp;ldquo;Ajax&amp;rdquo;. Everywhere I looked, people were talking about it, and the infamous XMLHttpRequest. In 2006, jQuery was born and made everything even easier.&lt;/p&gt;

&lt;p&gt;When I discovered all of this, I felt like the boy from the movie &lt;a href=&#34;https://www.imdb.com/title/tt0461804/&#34;&gt;&amp;ldquo;Mongolian ping pong&amp;rdquo;&lt;/a&gt; who is sent to school and finds a court with several other boys playing ping pong, with that ball he spent months trying to figure out what it was.&lt;/p&gt;

&lt;p&gt;In a later meeting of Indata employees, I remember us laughing about this makeshift job done on the Angaturama and how, due to our lack of knowledge at the time, I became famous among them for inventing &amp;ldquo;Ajax&amp;rdquo; despite its existence already.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>Collecting data to identify data collect</title>
            <link rel="alternate" href="/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/" type="text/html" />
            <id>/work/olxbrasil/trustandsafety/collecting-data-to-indentify-data-collection/</id>
            <updated>2023-08-19T10:56:00Z</updated>

            <summary type="html">This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!</summary>
            <content type="html">&lt;p&gt;This is the story of how we managed to prevent &amp;ldquo;data collect&amp;rdquo; fraud through a data collection!&lt;/p&gt;

&lt;p&gt;It happened in mid-2022. At the time, I had already been working for a year and a half in the Trust &amp;amp; Safety area at OLX Brasil, and we had achieved significant success in reducing fraud attempts. Especially in a type of fraud known as &amp;ldquo;False Payment&amp;rdquo;. In fact, another type of fraud known as &amp;ldquo;Data Collect&amp;rdquo; has since become the highest number of reported cases.&lt;/p&gt;

&lt;p&gt;The problem with this other type of fraud is that the behavior of the fraudsters was very similar to that of another type of malicious user: the spammer. A modeling to identify this type of fraudster, done a few months earlier, failed for this reason. We were unable to separate the spammer from a fraudster of this type of fraud in any way.&lt;/p&gt;

&lt;p&gt;Then you may ask me, why not prevent both characters with the same model?&lt;/p&gt;

&lt;p&gt;The approach we had towards spammers was quite different from the approach towards a fraudster. I cannot go into detail about how these approaches because confidentiality is part of security, and making it public can assist both groups in finding ways to circumvent. But trust me, it was very important to distinguish our actions in dealing with these two groups.&lt;/p&gt;

&lt;p&gt;We were at an impasse, our last model was actually doing a good job at accurately distinguishing between legitimate users and these two groups, but we couldn&amp;rsquo;t put it into production for making automatic decisions. We were missing the ability to identify which micro behaviors set one group apart from the other, and there was only one team in the company that could help us with that: the monitoring team.&lt;/p&gt;

&lt;p&gt;This team was a multidisciplinary team of fraud identification experts in the company. They were responsible for manually reviewing samples from automated models and operational team decisions regarding reports that were not handled automatically.&lt;/p&gt;

&lt;p&gt;We spoke with this team in an attempt to find any new criteria that could help us separate the cases. The conversation proved to be a not-so-effective tool, as the developers and monitors didn&amp;rsquo;t seem to speak the same language. We couldn&amp;rsquo;t objectively map out these determining factors through these unstructured conversation.&lt;/p&gt;

&lt;p&gt;To address this, we created a spreadsheet with a large sample of cases that the not so good model had identified as fraud (and within those cases, there were many spams) and added two columns: one to indicate whether it was fraud or spam, and another for free text where the operator could explain the reasons behind their conclusion. This spreadsheet was sent to the monitoring team for completion.&lt;/p&gt;

&lt;p&gt;The result could not be more satisfactory. In the first lines, the free text column started off more &amp;ldquo;poetic&amp;rdquo; and gradually, the repetitive pattern of the groups caused the monitors to write in a more concise and even repetitive manner, generating a clear pattern on new criteria that we could adopt for the machine learning model.&lt;/p&gt;

&lt;p&gt;Our data collection had worked. Our data collection had been successful.&lt;/p&gt;

&lt;p&gt;These pieces of information generated new feature calculations for a new machine learning model training. We sent new samples of the results from this new model for monitoring evaluation, which could assure us of high accuracy. For a few weeks, we kept the artifact in production without taking any action, only logging, just to monitor if the generated result would reflect the results obtained in the offline evaluation. Everything went smoothly.&lt;/p&gt;

&lt;p&gt;In the weeks after enabling the new model to take actions against the fraudsters, we experienced an average decrease of 33% in &amp;ldquo;Data collect&amp;rdquo; frauds per week. Another type of fraud, known as &amp;ldquo;False Sale&amp;rdquo;, also saw a 25% decrease, as it relies on a more complicated social engineering involving the execution of &amp;ldquo;Data collect&amp;rdquo; fraud at first place to be effective.&lt;/p&gt;

&lt;p&gt;So we data collected to avoid data collect successfully.&lt;/p&gt;
</content>
        </entry>
    
        <entry>
            <title>The fantastic widget factory</title>
            <link rel="alternate" href="/work/globocom/webmedia/the-fantastic-widget-factory/" type="text/html" />
            <id>/work/globocom/webmedia/the-fantastic-widget-factory/</id>
            <updated>2023-04-29T10:56:00Z</updated>

            <summary type="html">This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.</summary>
            <content type="html">&lt;p&gt;This is the story of how we performed a series of workarounds to overcome the rigidity of an extremely controlled production environment, and successfully deliver a product and platform solution.&lt;/p&gt;

&lt;p&gt;The case occurred in mid-2006 at Globo.com. At that time, the portal was divided into several independent sites, such as G1, GloboEsporte, Ego, among others. Generally, these sites displayed only text news, and all video assets were concentrated on another site called GMC, Globo Media Center.&lt;/p&gt;

&lt;p&gt;Our initial goal was to break down the silo in relation to video content, allowing developers from other sites to offer GMC registered videos in a simple and standardized way. At the time, an API was already available (known as WebmediaAPI), but it did not standardize the display of these content offerings, which would require additional effort from these teams for implementation.&lt;/p&gt;

&lt;p&gt;The solution we proposed would be to develop standardized visual widgets. These widgets would bring offers of various types such as recent videos, most viewed videos, best-rated videos, with the possibility of applying various filters and layout options. In this way, it would only be necessary to instantiate the widget into backend code page to offer video content in any Globo.com website.&lt;/p&gt;

&lt;p&gt;Here comes our first issue: At that time, Globo.com websites used a platform called Vignette, which made all its content static during the publication of homepages and articles. In other words, the news articles were static HTML generated after each publication, and the homepages were the same. Any changes to the page needed an editor to publish them for them to be displayed.&lt;/p&gt;

&lt;p&gt;This went against the dynamic nature of video content. How to maintain dynamically ordered content such as &amp;ldquo;most viewed&amp;rdquo; on a static HTML page? One way would be to use server-side includes, since the static files were served by Apache.&lt;/p&gt;

&lt;p&gt;Unfortunately, there were many restrictions to make any changes to the infrastructure at globo.com in 2006. To achieve this, we would need a lot of persuasion, meetings, presentations and one-on-one conversations, with no guarantee of success. We did not have the necessary time or social skills required for this goal. But we had a lot of creativity!&lt;/p&gt;

&lt;p&gt;So we decided to make these widgets on the client side. However, cross-domain requests to obtain information from WebmediaApi would only be possible if we could provide the necessary CORS headers. But to do this, changes to the infrastructure would also be necessary, as the application server (Apache) in front of WebmediaApi was also rigid, beyond our control, and exchanged all headers.&lt;/p&gt;

&lt;p&gt;Our solution was to allow WebmediaApi to return javascript call to a callback function instead of the json content. We then had a JavaScript with the widget code, which included another &amp;ldquo;JavaScript&amp;rdquo; that was the WebmediaAPI call with the name of a dynamic function as a parameter. This function was &amp;ldquo;printed&amp;rdquo; in the API response as a function call to finally render the chosen and customized widget.&lt;/p&gt;

&lt;p&gt;There was even a timeout control in case the callback took too long to respond.&lt;/p&gt;

&lt;p&gt;This solved our problem and after a few sprints, we created a true widget factory. Needs and ideas would come, and we would generate the necessary widgets and filters on WebmediaApi. It was a success. So much so, that the standardized layout of the widgets created the need to update the GMC, the aggregator site for all videos, to use the same layouts standards. Thus, Globo Vídeos was born, the successor to GMC.&lt;/p&gt;

&lt;p&gt;But there was a catch&amp;hellip;&lt;/p&gt;

&lt;p&gt;At that time, Google did not index pages rendered via JavaScript. And we needed at least Globo Videos to remain server-side rendered so that we wouldn&amp;rsquo;t lose relevance in organic searches.&lt;/p&gt;

&lt;p&gt;The solution: we started rendering javascript widgets on the server. Since everything in Vignette was java, we had to render using Rhino (if I&amp;rsquo;m not mistaken, before java 1.5 it was only possible to render javascript in the JVM through this lib).&lt;/p&gt;

&lt;p&gt;To maintain the freshness of the data, the Globo Vídeos homepage began to be rendered on the server every X period of time. I don&amp;rsquo;t remember if this automatic publication was done via cron or some Vignette tool, but the point is that static HTML began to be generated frequently, with the HTML of the widgets. However, the video pages continued to display client-side widgets as it would be impractical to render millions of HTML frequently.&lt;/p&gt;

&lt;p&gt;There was an added complexity to this solution as the process responsible for generating static HTML files on Globo.com was running on a WebLogic server. Although our server-side JavaScript rendering solution worked seamlessly in all other environments, including local, staging, and QA, it failed to function when deployed to production. After extensive debugging efforts, we discovered that the WebLogic version used in production differed from the other environments and contained an older Rhino version that superseded our newer one. To ensure proper widget rendering, we needed to execute the code by instantiating another classloader that loaded the correct Rhino jar version, which we successfully accomplished.&lt;/p&gt;

&lt;p&gt;In the end, we were able to develop a client-side widget platform that empowers websites to offer videos independently and consistently, without having to duplicate code. Furthermore, we devised a server-side rendered video product that can be easily indexed by Google. These achievements were made possible by implementing a series of three engineering workarounds.&lt;/p&gt;
</content>
        </entry>
    
</feed>
